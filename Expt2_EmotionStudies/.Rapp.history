sample.cluster4
Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample = function(data_file,StartTime,EndTime,StepSize){#
#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				shuf.fnc <- function(x){#
				NewCond <- x$Cond[sample.int(length(x$Cond))]#
				NewStrength <- x$Strength[sample.int(length(x$Strength))]#
				return(data.frame(x,NewCond,NewStrength))#
					}#
				Trials = ddply(Trials, .(Subj), shuf.fnc)#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				contrasts(data_file$Pop)[2] <- 1#
				contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}#
# Cluster finding function and pval finding function#
			# Cluster finding script (written by Jon Brennan )#
			find.clusters <- function(pval.vector, tval.vector, latencies, alpha=.05, cluster.size=5,signed =FALSE, sign = "pos") {#
				binary.stat <- as.numeric(pval.vector < alpha)#
				tval.stat <- rep(0,length(binary.stat))#
				tval.stat[2:length(binary.stat)] = ((tval.vector[2:length(binary.stat)] * tval.vector[1:(length(binary.stat)-1)]) <0)#
				cluster.start <- c()#
				cluster.end <- c()#
				cluster.stat <- c()#
				in.cluster <- 0#
				found.clusters <- 0#
				new.end <- 0#
				new.start <- 0#
				end.cluster <- 0#
				for (n in 1:length(binary.stat)) {#
					if (signed == FALSE){#
						if (in.cluster == 0 && binary.stat[n] == 1 ) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
					if (sign == "pos"){#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] >= 0) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] <= 0) {#
							new.start = n#
							in.cluster = 1#
								}#
					if (in.cluster == 1 && binary.stat[n] == 0) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}#
					if (in.cluster == 1 && tval.stat[n] == 1) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}				#
					# in case we reach the end and we are still "in" a cluster...#
					if (in.cluster == 1 && binary.stat[n] == 1 && n == length(binary.stat)) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 1#
						}		#
					if (end.cluster) {	#
						if ((new.end - new.start) >= cluster.size) {#
							found.clusters <- found.clusters + 1#
							cluster.start<- c(cluster.start, latencies[new.start])#
							cluster.end <- c(cluster.end, latencies[new.end])#
							cluster.stat <- c(cluster.stat, sum(abs(tval.vector[new.start:(new.end-1)])))#
							}#
						end.cluster = 0#
						}#
					}#
				cluster.out <- data.frame(start = cluster.start, end = cluster.end, cluster.stat = cluster.stat)#
				return(cluster.out)	#
				}#
			# Script for assigning pvals to clusters	#
			pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics = function(data_file,StartTime,EndTime,StepSize){#
			#Load the relevant libraries#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)#
			#Start the multicore machine!#
#
			#Split the data frame into a list for each timepoint#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,]$Time)#
			#Parallel application of lmer and coefficient extraction, outputs another list as above#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = x, family = "binomial")))[2:8,1:3]) -> a.co#
			# Evaluate the list to see whether t value meets a threshold (hard coded)#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			# plyr that list into a dataframe#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","PopControl","CondU:Strengthweak","CondU:PopControl","Strengthweak:PopControl", "CondU:Strengthweak:PopControl"))#
			#Split that dataframe into a list based on the regression coefficients#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics.nopop = function(data_file,StartTime,EndTime,StepSize){#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)			#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= 0 & data_file$Time <= 1500,]$Time)#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength +(1+Cond+Strength|Subj) + (1+Cond+Strength|Trial), data = x)))[2:4,1:3]) -> a.co#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","CondU:Strengthweak"))#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
#
# Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample.nopop = function(data_file,StartTime,EndTime,StepSize){#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				Trials$Order = NA#
                                Trials$NewCond = Trials$Cond#
                                Trials$NewStrength = Trials$Strength#
                                Trials$NewPop = Trials$Pop#
			for (i in unique(Trials$Subj)){#
				Trials[Trials$Subj == i,]$Order = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewCond = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Cond#
                                #Trials[Trials$Subj == i,]$NewOrder = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewStrength = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Strength                                #
			}#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				#contrasts(data_file$Pop)[2] <- 1#
				#contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics.nopop(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}
Resamples_N = 25 # Number of samples (it will take a long time to do 1000, so set this to 10 your first time to make sure the script works)#
			resample1.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample2.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample3.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample4.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample5.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample6.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample7.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			for (i in 1:Resamples_N){#
				print(i)#
			stats.resample = resample(ET,StartTime,EndTime,StepSize)#
			recluster1 = find.clusters(stats.resample[[1]]$pval,stats.resample[[1]]$Stat,stats.resample[[1]]$Time,cluster.size = 1)#
			recluster2 = find.clusters(stats.resample[[2]]$pval,stats.resample[[2]]$Stat,stats.resample[[2]]$Time,cluster.size = 1)#
			recluster3 = find.clusters(stats.resample[[3]]$pval,stats.resample[[3]]$Stat,stats.resample[[3]]$Time,cluster.size = 1)#
			recluster4 = find.clusters(stats.resample[[4]]$pval,stats.resample[[4]]$Stat,stats.resample[[4]]$Time,cluster.size = 1)#
			recluster5 = find.clusters(stats.resample[[5]]$pval,stats.resample[[5]]$Stat,stats.resample[[5]]$Time,cluster.size = 1)#
			recluster6 = find.clusters(stats.resample[[6]]$pval,stats.resample[[6]]$Stat,stats.resample[[6]]$Time,cluster.size = 1)#
			recluster7 = find.clusters(stats.resample[[7]]$pval,stats.resample[[7]]$Stat,stats.resample[[7]]$Time,cluster.size = 1)#
			resample1.max = ifelse(is.numeric(recluster1$cluster.stat),max(recluster1$cluster.stat),0)#
			resample2.max = ifelse(is.numeric(recluster2$cluster.stat),max(recluster2$cluster.stat),0)#
			resample3.max = ifelse(is.numeric(recluster3$cluster.stat),max(recluster3$cluster.stat),0)#
			resample4.max = ifelse(is.numeric(recluster4$cluster.stat),max(recluster4$cluster.stat),0)#
			resample5.max = ifelse(is.numeric(recluster5$cluster.stat),max(recluster5$cluster.stat),0)#
			resample6.max = ifelse(is.numeric(recluster6$cluster.stat),max(recluster6$cluster.stat),0)#
			resample7.max = ifelse(is.numeric(recluster7$cluster.stat),max(recluster7$cluster.stat),0)#
			 #Find the largest cluster (this is what you will be comparing against - is your cluster > 95% of largest clusters)#
			print(resample1.max)#
			print(resample3.max)#
		#	print(recluster1[recluster1$cluster.stat == max(recluster1$cluster.stat),]$start)#
		#	print(recluster3[recluster3$cluster.stat == max(recluster3$cluster.stat),]$start )#
			resample1.large.cluster[i] = ifelse(resample1.max>0,resample1.max,0)#
			resample2.large.cluster[i] = ifelse(resample2.max>0,resample2.max,0)#
			print(resample3.max)#
			resample3.large.cluster[i] = ifelse(resample3.max>0,resample3.max,0)#
			resample4.large.cluster[i] = ifelse(resample4.max>0,resample3.max,0)#
			resample5.large.cluster[i] = ifelse(resample5.max>0,resample3.max,0)#
			resample6.large.cluster[i] = ifelse(resample6.max>0,resample3.max,0)#
			resample7.large.cluster[i] = ifelse(resample7.max>0,resample3.max,0)#
#
			}#
#
			sample.cluster1 = pval(sample.cluster1,resample1.large.cluster)#
			sample.cluster2 = pval(sample.cluster2,resample2.large.cluster)#
			sample.cluster3 = pval(sample.cluster3,resample3.large.cluster)#
			sample.cluster4 = pval(sample.cluster4,resample4.large.cluster)#
			sample.cluster5 = pval(sample.cluster5,resample5.large.cluster)#
			sample.cluster6 = pval(sample.cluster6,resample6.large.cluster)#
			sample.cluster7 = pval(sample.cluster7,resample7.large.cluster)
sample.cluster1
sample.cluster4
StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
#
			stats.sample = calculate.statistics(ET,StartTime,EndTime,StepSize)#
			sample.cluster1 = find.clusters(stats.sample[[1]]$pval,stats.sample[[1]]$Stat,stats.sample[[1]]$Time,cluster.size = 1)#
			sample.cluster2 = find.clusters(stats.sample[[2]]$pval,stats.sample[[2]]$Stat,stats.sample[[2]]$Time,cluster.size = 1)#
			sample.cluster3 = find.clusters(stats.sample[[3]]$pval,stats.sample[[3]]$Stat,stats.sample[[3]]$Time,cluster.size = 1)#
			sample.cluster4 = find.clusters(stats.sample[[4]]$pval,stats.sample[[4]]$Stat,stats.sample[[4]]$Time,cluster.size = 1)#
			sample.cluster5 = find.clusters(stats.sample[[5]]$pval,stats.sample[[5]]$Stat,stats.sample[[5]]$Time,cluster.size = 1)#
			sample.cluster6 = find.clusters(stats.sample[[6]]$pval,stats.sample[[6]]$Stat,stats.sample[[6]]$Time,cluster.size = 1)#
			sample.cluster7 = find.clusters(stats.sample[[7]]$pval,stats.sample[[7]]$Stat,stats.sample[[7]]$Time,cluster.size = 1)
summary(ET)
ET$Score <- ifelse(ET$Score <= -3.66, 0,1)
StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
#
			stats.sample = calculate.statistics(ET,StartTime,EndTime,StepSize)#
			sample.cluster1 = find.clusters(stats.sample[[1]]$pval,stats.sample[[1]]$Stat,stats.sample[[1]]$Time,cluster.size = 1)#
			sample.cluster2 = find.clusters(stats.sample[[2]]$pval,stats.sample[[2]]$Stat,stats.sample[[2]]$Time,cluster.size = 1)#
			sample.cluster3 = find.clusters(stats.sample[[3]]$pval,stats.sample[[3]]$Stat,stats.sample[[3]]$Time,cluster.size = 1)#
			sample.cluster4 = find.clusters(stats.sample[[4]]$pval,stats.sample[[4]]$Stat,stats.sample[[4]]$Time,cluster.size = 1)#
			sample.cluster5 = find.clusters(stats.sample[[5]]$pval,stats.sample[[5]]$Stat,stats.sample[[5]]$Time,cluster.size = 1)#
			sample.cluster6 = find.clusters(stats.sample[[6]]$pval,stats.sample[[6]]$Stat,stats.sample[[6]]$Time,cluster.size = 1)#
			sample.cluster7 = find.clusters(stats.sample[[7]]$pval,stats.sample[[7]]$Stat,stats.sample[[7]]$Time,cluster.size = 1)
warnings()
summary(ET)
summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = subset(ET, Time == 900), family = "binomial"))
summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = subset(ET, Time == 100), family = "binomial"))
summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = subset(ET, Time == 200), family = "binomial"))
summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = subset(ET, Time == 1400), family = "binomial"))
summary(glmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = subset(ET, Time == 1400), family = "binomial"))
Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample = function(data_file,StartTime,EndTime,StepSize){#
#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				shuf.fnc <- function(x){#
				NewCond <- x$Cond[sample.int(length(x$Cond))]#
				NewStrength <- x$Strength[sample.int(length(x$Strength))]#
				return(data.frame(x,NewCond,NewStrength))#
					}#
				Trials = ddply(Trials, .(Subj), shuf.fnc)#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				contrasts(data_file$Pop)[2] <- 1#
				contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}#
# Cluster finding function and pval finding function#
			# Cluster finding script (written by Jon Brennan )#
			find.clusters <- function(pval.vector, tval.vector, latencies, alpha=.05, cluster.size=5,signed =FALSE, sign = "pos") {#
				binary.stat <- as.numeric(pval.vector < alpha)#
				tval.stat <- rep(0,length(binary.stat))#
				tval.stat[2:length(binary.stat)] = ((tval.vector[2:length(binary.stat)] * tval.vector[1:(length(binary.stat)-1)]) <0)#
				cluster.start <- c()#
				cluster.end <- c()#
				cluster.stat <- c()#
				in.cluster <- 0#
				found.clusters <- 0#
				new.end <- 0#
				new.start <- 0#
				end.cluster <- 0#
				for (n in 1:length(binary.stat)) {#
					if (signed == FALSE){#
						if (in.cluster == 0 && binary.stat[n] == 1 ) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
					if (sign == "pos"){#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] >= 0) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] <= 0) {#
							new.start = n#
							in.cluster = 1#
								}#
					if (in.cluster == 1 && binary.stat[n] == 0) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}#
					if (in.cluster == 1 && tval.stat[n] == 1) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}				#
					# in case we reach the end and we are still "in" a cluster...#
					if (in.cluster == 1 && binary.stat[n] == 1 && n == length(binary.stat)) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 1#
						}		#
					if (end.cluster) {	#
						if ((new.end - new.start) >= cluster.size) {#
							found.clusters <- found.clusters + 1#
							cluster.start<- c(cluster.start, latencies[new.start])#
							cluster.end <- c(cluster.end, latencies[new.end])#
							cluster.stat <- c(cluster.stat, sum(abs(tval.vector[new.start:(new.end-1)])))#
							}#
						end.cluster = 0#
						}#
					}#
				cluster.out <- data.frame(start = cluster.start, end = cluster.end, cluster.stat = cluster.stat)#
				return(cluster.out)	#
				}#
			# Script for assigning pvals to clusters	#
			pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics = function(data_file,StartTime,EndTime,StepSize){#
			#Load the relevant libraries#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)#
			#Start the multicore machine!#
#
			#Split the data frame into a list for each timepoint#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,]$Time)#
			#Parallel application of lmer and coefficient extraction, outputs another list as above#
			mclapply(a,function(x) coef(summary(glmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = x, family = "binomial")))[2:8,1:3]) -> a.co#
			# Evaluate the list to see whether t value meets a threshold (hard coded)#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			# plyr that list into a dataframe#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","PopControl","CondU:Strengthweak","CondU:PopControl","Strengthweak:PopControl", "CondU:Strengthweak:PopControl"))#
			#Split that dataframe into a list based on the regression coefficients#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics.nopop = function(data_file,StartTime,EndTime,StepSize){#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)			#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= 0 & data_file$Time <= 1500,]$Time)#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength +(1+Cond+Strength|Subj) + (1+Cond+Strength|Trial), data = x)))[2:4,1:3]) -> a.co#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","CondU:Strengthweak"))#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
#
# Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample.nopop = function(data_file,StartTime,EndTime,StepSize){#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				Trials$Order = NA#
                                Trials$NewCond = Trials$Cond#
                                Trials$NewStrength = Trials$Strength#
                                Trials$NewPop = Trials$Pop#
			for (i in unique(Trials$Subj)){#
				Trials[Trials$Subj == i,]$Order = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewCond = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Cond#
                                #Trials[Trials$Subj == i,]$NewOrder = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewStrength = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Strength                                #
			}#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				#contrasts(data_file$Pop)[2] <- 1#
				#contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics.nopop(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}
StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
#
			stats.sample = calculate.statistics(ET,StartTime,EndTime,StepSize)#
			sample.cluster1 = find.clusters(stats.sample[[1]]$pval,stats.sample[[1]]$Stat,stats.sample[[1]]$Time,cluster.size = 1)#
			sample.cluster2 = find.clusters(stats.sample[[2]]$pval,stats.sample[[2]]$Stat,stats.sample[[2]]$Time,cluster.size = 1)#
			sample.cluster3 = find.clusters(stats.sample[[3]]$pval,stats.sample[[3]]$Stat,stats.sample[[3]]$Time,cluster.size = 1)#
			sample.cluster4 = find.clusters(stats.sample[[4]]$pval,stats.sample[[4]]$Stat,stats.sample[[4]]$Time,cluster.size = 1)#
			sample.cluster5 = find.clusters(stats.sample[[5]]$pval,stats.sample[[5]]$Stat,stats.sample[[5]]$Time,cluster.size = 1)#
			sample.cluster6 = find.clusters(stats.sample[[6]]$pval,stats.sample[[6]]$Stat,stats.sample[[6]]$Time,cluster.size = 1)#
			sample.cluster7 = find.clusters(stats.sample[[7]]$pval,stats.sample[[7]]$Stat,stats.sample[[7]]$Time,cluster.size = 1)
sample.cluster1 = pval(sample.cluster1,resample1.large.cluster)#
			sample.cluster2 = pval(sample.cluster2,resample2.large.cluster)#
			sample.cluster3 = pval(sample.cluster3,resample3.large.cluster)#
			sample.cluster4 = pval(sample.cluster4,resample4.large.cluster)#
			sample.cluster5 = pval(sample.cluster5,resample5.large.cluster)#
			sample.cluster6 = pval(sample.cluster6,resample6.large.cluster)#
			sample.cluster7 = pval(sample.cluster7,resample7.large.cluster)
sample.cluster1
sample.cluster3
sample.cluster4
Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample = function(data_file,StartTime,EndTime,StepSize){#
#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				shuf.fnc <- function(x){#
				NewCond <- x$Cond[sample.int(length(x$Cond))]#
				NewStrength <- x$Strength[sample.int(length(x$Strength))]#
				return(data.frame(x,NewCond,NewStrength))#
					}#
				Trials = ddply(Trials, .(Subj), shuf.fnc)#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				contrasts(data_file$Pop)[2] <- 1#
				contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}#
# Cluster finding function and pval finding function#
			# Cluster finding script (written by Jon Brennan )#
			find.clusters <- function(pval.vector, tval.vector, latencies, alpha=.05, cluster.size=5,signed =FALSE, sign = "pos") {#
				binary.stat <- as.numeric(pval.vector < alpha)#
				tval.stat <- rep(0,length(binary.stat))#
				tval.stat[2:length(binary.stat)] = ((tval.vector[2:length(binary.stat)] * tval.vector[1:(length(binary.stat)-1)]) <0)#
				cluster.start <- c()#
				cluster.end <- c()#
				cluster.stat <- c()#
				in.cluster <- 0#
				found.clusters <- 0#
				new.end <- 0#
				new.start <- 0#
				end.cluster <- 0#
				for (n in 1:length(binary.stat)) {#
					if (signed == FALSE){#
						if (in.cluster == 0 && binary.stat[n] == 1 ) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
					if (sign == "pos"){#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] >= 0) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] <= 0) {#
							new.start = n#
							in.cluster = 1#
								}#
					if (in.cluster == 1 && binary.stat[n] == 0) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}#
					if (in.cluster == 1 && tval.stat[n] == 1) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}				#
					# in case we reach the end and we are still "in" a cluster...#
					if (in.cluster == 1 && binary.stat[n] == 1 && n == length(binary.stat)) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 1#
						}		#
					if (end.cluster) {	#
						if ((new.end - new.start) >= cluster.size) {#
							found.clusters <- found.clusters + 1#
							cluster.start<- c(cluster.start, latencies[new.start])#
							cluster.end <- c(cluster.end, latencies[new.end])#
							cluster.stat <- c(cluster.stat, sum(abs(tval.vector[new.start:(new.end-1)])))#
							}#
						end.cluster = 0#
						}#
					}#
				cluster.out <- data.frame(start = cluster.start, end = cluster.end, cluster.stat = cluster.stat)#
				return(cluster.out)	#
				}#
			# Script for assigning pvals to clusters	#
			pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics = function(data_file,StartTime,EndTime,StepSize){#
			#Load the relevant libraries#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)#
			#Start the multicore machine!#
#
			#Split the data frame into a list for each timepoint#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,]$Time)#
			#Parallel application of lmer and coefficient extraction, outputs another list as above#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = x)))[2:8,1:3]) -> a.co#
			# Evaluate the list to see whether t value meets a threshold (hard coded)#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			# plyr that list into a dataframe#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","PopControl","CondU:Strengthweak","CondU:PopControl","Strengthweak:PopControl", "CondU:Strengthweak:PopControl"))#
			#Split that dataframe into a list based on the regression coefficients#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics.nopop = function(data_file,StartTime,EndTime,StepSize){#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)			#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= 0 & data_file$Time <= 1500,]$Time)#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength +(1+Cond+Strength|Subj) + (1+Cond+Strength|Trial), data = x)))[2:4,1:3]) -> a.co#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","CondU:Strengthweak"))#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
#
# Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample.nopop = function(data_file,StartTime,EndTime,StepSize){#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				Trials$Order = NA#
                                Trials$NewCond = Trials$Cond#
                                Trials$NewStrength = Trials$Strength#
                                Trials$NewPop = Trials$Pop#
			for (i in unique(Trials$Subj)){#
				Trials[Trials$Subj == i,]$Order = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewCond = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Cond#
                                #Trials[Trials$Subj == i,]$NewOrder = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewStrength = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Strength                                #
			}#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				#contrasts(data_file$Pop)[2] <- 1#
				#contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics.nopop(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}
StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
#
			stats.sample = calculate.statistics(ET,StartTime,EndTime,StepSize)#
			sample.cluster1 = find.clusters(stats.sample[[1]]$pval,stats.sample[[1]]$Stat,stats.sample[[1]]$Time,cluster.size = 1)#
			sample.cluster2 = find.clusters(stats.sample[[2]]$pval,stats.sample[[2]]$Stat,stats.sample[[2]]$Time,cluster.size = 1)#
			sample.cluster3 = find.clusters(stats.sample[[3]]$pval,stats.sample[[3]]$Stat,stats.sample[[3]]$Time,cluster.size = 1)#
			sample.cluster4 = find.clusters(stats.sample[[4]]$pval,stats.sample[[4]]$Stat,stats.sample[[4]]$Time,cluster.size = 1)#
			sample.cluster5 = find.clusters(stats.sample[[5]]$pval,stats.sample[[5]]$Stat,stats.sample[[5]]$Time,cluster.size = 1)#
			sample.cluster6 = find.clusters(stats.sample[[6]]$pval,stats.sample[[6]]$Stat,stats.sample[[6]]$Time,cluster.size = 1)#
			sample.cluster7 = find.clusters(stats.sample[[7]]$pval,stats.sample[[7]]$Stat,stats.sample[[7]]$Time,cluster.size = 1)
sample.cluster4
stats.sample[[4]]
stats.sample[[1]]
dat<- data.frame(t=seq(0, 2*pi, by=0.1) )#
 xhrt <- function(t) 16*sin(t)^3#
 yhrt <- function(t) 13*cos(t)-5*cos(2*t)-2*cos(3*t)-cos(4*t)#
 dat$y=yhrt(dat$t)#
 dat$x=xhrt(dat$t)#
 with(dat, plot(x,y, type="l"))
with(dat, polygon(x,y, col="hotpink"))
points(c(10,-10, -15, 15), c(-10, -10, 10, 10), pch=169, font=5)
#values used for longdiff#
# s_n<-24 #how many subjects#
# switch_baseline<-.25 #probability of switching to target at baseline#
# switch_on<-.5 #probability of switching to target when effect is in effect#
# switch_off<-.1 #probability of switching off target when effect is in effect#
# items_n<-5 #how many items per condition? 2-condition experiment. items appear in both conditions, within subject#
# bins_n<-20 #how many bins?#
# first_bin<-7 #on average, what is the first bin to show an effect in condition with early effect?#
# time_diff<-2.5 #how many bins earlier is there an effect in condition 1 than condition 2?#
# noise<-2 #trial noise sd#
# sub_speeds<-rnorm(s_n,0,1) #how much faster or slower than typical is the subject?#
# sub_s_sd<-1 #sd for sub speed error#
# sub_effects<-rnorm(s_n,0,1.5) #degree to which effect size is larger or smaller for that subject#
# sub_e_sd<-1 #sd for sub effect error#
# item_effects<-rnorm(items_n,0,.5) #degree to which effect size is larger or smaller than typical for that item#
# item_sd<-1 #sd for item effect error#
#
#values used for shortdiff#
s_n<-24 #how many subjects#
switch_baseline<-.25 #probability of switching to target at baseline#
switch_on<-.9 #probability of switching to target when effect is in effect#
switch_off<-.2 #probability of switching off target when effect is in effect#
items_n<-5 #how many items per condition? 2-condition experiment. items appear in both conditions, within subject#
bins_n<-20 #how many bins?#
first_bin<-7 #on average, what is the first bin to show an effect in condition with early effect?#
time_diff<-0 #how many bins earlier is there an effect in condition 1 than condition 2?#
noise<-.25 #trial noise sd#
sub_speeds<-rnorm(s_n,0,.25) #how much faster or slower than typical is the subject?#
sub_s_sd<-.5 #sd for sub speed error#
sub_effects<-rnorm(s_n,0,2) #degree to which effect size is larger or smaller for that subject#
sub_e_sd<-.25 #sd for sub effect error#
item_effects<-rnorm(items_n,0,2) #degree to which effect size is larger or smaller than typical for that item#
item_sd<-.25 #sd for item effect error
#what's alpha?#
1-.95^(1/bins_n)#
#
mydata<-data.frame(subject=c(NA),condition=c(NA),item=c(NA),bin=c(NA),fixtarg=c(NA))#
for (sub in c(1:s_n)){#
	for (item in c(1:items_n)){#
		#do a item for each condition simultaneously (saves time)#
		#draw a bin for an effect#
		con1_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
		con2_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
			current_1<-rbinom(1,1,.5) #where am I looking in first bin for condition 1?#
			current_2<-rbinom(1,1,.5) #where am I looking in first bin for condition 2?#
			mydata<-rbind(mydata,c(sub,1,item,1,current_1))#
			mydata<-rbind(mydata,c(sub,2,item,1,current_2))#
			for (bin in c(2:bins_n)){#
				if (bin<con1_bin){#
						if (rbinom(1,1,switch_baseline)){current_1<-(1-current_1)}#
					}else{#
						if (current_1){#
							if (rbinom(1,1,switch_off)){current_1<-(1-current_1)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_1<-(1-current_1)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,1,item,bin,current_1))#
				if (bin<con2_bin){#
						if (rbinom(1,1,switch_baseline)){current_2<-(1-current_2)}#
					}else{#
						if (current_2){#
							if (rbinom(1,1,switch_off)){current_2<-(1-current_2)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_2<-(1-current_2)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,2,item,bin,current_2))#
			}#
	}#
}
summary(mydata)
#what's alpha?#
1-.95^(1/bins_n)#
#
mydata<-data.frame(subject=c(NA),condition=c(NA),item=c(NA),bin=c(NA),fixtarg=c(NA))#
for (sub in c(1:s_n)){#
	for (item in c(1:items_n)){#
		#do a item for each condition simultaneously (saves time)#
		#draw a bin for an effect#
		con1_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
		con2_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
			current_1<-rbinom(1,1,.5) #where am I looking in first bin for condition 1?#
			current_2<-rbinom(1,1,.5) #where am I looking in first bin for condition 2?#
			mydata<-rbind(mydata,c(sub,1,item,1,current_1))#
			mydata<-rbind(mydata,c(sub,2,item,1,current_2))#
			for (bin in c(2:bins_n)){#
				if (bin<con1_bin){#
						if (rbinom(1,1,switch_baseline)){current_1<-(1-current_1)}#
					}else{#
						if (current_1){#
							if (rbinom(1,1,switch_off)){current_1<-(1-current_1)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_1<-(1-current_1)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,1,item,bin,current_1))#
				if (bin<con2_bin){#
						if (rbinom(1,1,switch_baseline)){current_2<-(1-current_2)}#
					}else{#
						if (current_2){#
							if (rbinom(1,1,switch_off)){current_2<-(1-current_2)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_2<-(1-current_2)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,2,item,bin,current_2))#
			}#
	}#
}
summary(mydata)
#what's alpha?#
1-.95^(1/bins_n)#
#
mydata<-data.frame(subject=c(NA),condition=c(NA),item=c(NA),bin=c(NA),fixtarg=c(NA))#
for (sub in c(1:s_n)){#
	for (item in c(1:items_n)){#
		#do a item for each condition simultaneously (saves time)#
		#draw a bin for an effect#
		con1_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
		con2_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
			current_1<-rbinom(1,1,.5) #where am I looking in first bin for condition 1?#
			current_2<-rbinom(1,1,.5) #where am I looking in first bin for condition 2?#
			mydata<-rbind(mydata,c(sub,1,item,1,current_1))#
			mydata<-rbind(mydata,c(sub,2,item,1,current_2))#
			for (bin in c(2:bins_n)){#
				if (bin<con1_bin){#
						if (rbinom(1,1,switch_baseline)){current_1<-(1-current_1)}#
					}else{#
						if (current_1){#
							if (rbinom(1,1,switch_off)){current_1<-(1-current_1)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_1<-(1-current_1)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,1,item,bin,current_1))#
				if (bin<con2_bin){#
						if (rbinom(1,1,switch_baseline)){current_2<-(1-current_2)}#
					}else{#
						if (current_2){#
							if (rbinom(1,1,switch_off)){current_2<-(1-current_2)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_2<-(1-current_2)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,2,item,bin,current_2))#
			}#
	}#
}
createdata <- function(){#
mydata<-data.frame(subject=c(NA),condition=c(NA),item=c(NA),bin=c(NA),fixtarg=c(NA))#
for (sub in c(1:s_n)){#
	for (item in c(1:items_n)){#
		#do a item for each condition simultaneously (saves time)#
		#draw a bin for an effect#
		con1_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
		con2_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)+time_diff+rnorm(1,sub_effects[sub],sub_e_sd)+rnorm(1,item_effects[item],item_sd)#
			current_1<-rbinom(1,1,.5) #where am I looking in first bin for condition 1?#
			current_2<-rbinom(1,1,.5) #where am I looking in first bin for condition 2?#
			mydata<-rbind(mydata,c(sub,1,item,1,current_1))#
			mydata<-rbind(mydata,c(sub,2,item,1,current_2))#
			for (bin in c(2:bins_n)){#
				if (bin<con1_bin){#
						if (rbinom(1,1,switch_baseline)){current_1<-(1-current_1)}#
					}else{#
						if (current_1){#
							if (rbinom(1,1,switch_off)){current_1<-(1-current_1)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_1<-(1-current_1)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,1,item,bin,current_1))#
				if (bin<con2_bin){#
						if (rbinom(1,1,switch_baseline)){current_2<-(1-current_2)}#
					}else{#
						if (current_2){#
							if (rbinom(1,1,switch_off)){current_2<-(1-current_2)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_2<-(1-current_2)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,2,item,bin,current_2))#
			}#
	}#
}#
mydata<-mydata[is.na(mydata$subject)==FALSE,]#
}
createdata <- function(){#
mydata<-data.frame(subject=c(NA),condition=c(NA),item=c(NA),bin=c(NA),fixtarg=c(NA))#
for (sub in c(1:s_n)){#
	for (item in c(1:items_n)){#
		#do a item for each condition simultaneously (saves time)#
		#draw a bin for an effect#
		con1_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
		con2_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)+time_diff+rnorm(1,sub_effects[sub],sub_e_sd)+rnorm(1,item_effects[item],item_sd)#
			current_1<-rbinom(1,1,.5) #where am I looking in first bin for condition 1?#
			current_2<-rbinom(1,1,.5) #where am I looking in first bin for condition 2?#
			mydata<-rbind(mydata,c(sub,1,item,1,current_1))#
			mydata<-rbind(mydata,c(sub,2,item,1,current_2))#
			for (bin in c(2:bins_n)){#
				if (bin<con1_bin){#
						if (rbinom(1,1,switch_baseline)){current_1<-(1-current_1)}#
					}else{#
						if (current_1){#
							if (rbinom(1,1,switch_off)){current_1<-(1-current_1)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_1<-(1-current_1)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,1,item,bin,current_1))#
				if (bin<con2_bin){#
						if (rbinom(1,1,switch_baseline)){current_2<-(1-current_2)}#
					}else{#
						if (current_2){#
							if (rbinom(1,1,switch_off)){current_2<-(1-current_2)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_2<-(1-current_2)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,2,item,bin,current_2))#
			}#
	}#
}#
return(mydata<-mydata[is.na(mydata$subject)==FALSE,])#
}
a<- createdata()
summary(a)
#analyze with mixed effects model#
library(lme4)#
get_stats = function(thedata){#
	#assumes data is in the format of the actual data above. Easy to adjust for other datasets.#
	#returns a dataframe, where each row is c(bin#, z-score, p-value)#
	sigs_me<-data.frame(bin=c(1:bins_n),z=rep(0,bins_n),p=rep(0,bins_n))#
	for (bin in c(1:bins_n)){#
		#since our subjects and items aren't different from one another, no need for maximal random effects#
		m<-glmer(fixtarg~condition+(1|subject)+(1|item),data=thedata[thedata$bin==bin,],family="binomial") #
		sigs_me[sigs_me$bin==bin,]<-c(bin,coef(summary(m))[2,3],coef(summary(m))[2,4])#
	}#
	return(sigs_me)#
}
threshold=1.5 #cutoff (t value) for inclusion in a cluster. Lower thresholds get longer, weaker clusters. Choice doesn't effect on false positive rate ... unless you keep trying different thresholds until something "works"#
#
#define some useful functions#
cluster = function(somedata,cutoff){#
	#somedata is a vector of test statistics (not p-values)#
	#cutoff is a threshold for for those statistics. Anything below threshold won't be considered part of a cluster#
	#returns a vector of the same length, where 0 marks a value that is not part of a cluster, and counting numbers mark clusters. #
	#Every value with same # is in same cluster.#
	somedata<-abs(somedata) #will cause problems if there is a sudden, significant switch in sign. Unlikely in practice for eyetracking data.#
	clusters=c()#
	current_cluster=0#
	in_cluster=FALSE#
	for (i in c(1:length(somedata))){#
		if (somedata[i]>cutoff){#
			if (in_cluster){#
				clusters<-c(clusters,current_cluster)#
			}else{#
				in_cluster=TRUE#
				current_cluster<-current_cluster+1#
				clusters<-c(clusters,current_cluster)#
			}#
		}else{#
			clusters<-c(clusters,0)	#
			in_cluster=FALSE#
		}#
	}#
	return(clusters)#
}#
#
score_clusters = function(somedata,someclusters){#
	#somedata is a vector of test statistics (not p-values)#
	#someclusters is the output of cluster()#
	#returns dataframe with the size (sum of test stats) of each cluster#
	c<-data.frame(stat=somedata,cluster=someclusters)#
	scores<-c()#
	for (cluster in c(1:max(someclusters))){#
		scores<-c(scores,sum(abs(c$stat[c$cluster==cluster])))#
	}#
	return(data.frame(cluster=c(1:length(scores)),score=scores))	#
}#
#
resample_data = function(thedata){#
	#This function assumes data is structured like the data above, but should be easy to adapt.#
	##
	#The design of this study is fully crossed (every subject gets every item in every condition)#
	#So all we need to do is shuffle the condition codes for each item for each subject. #
	#If there were also a between-subjects condition, we would have to shuffle subjects between conditions. Etc.#
	#Note: Do NOT shuffle bins and do NOT shuffle condition codes independently for each bin. #
	#That would assume the data in each bin of a trial is independent of the data in the other bins, which it is not.#
	for (sub in c(1:s_n)){#
		for (item in c(1:items_n)){#
			#random coin flip. If heads, switch condition codes. If tails, don't.#
			#if (rbinom(1,1,.5)){#
				#heads. switch codes.#
				# I'm not sure that your analysis does a random partition, Josh. Rather, all the trials that were previously #
				# in one bin, are now assigned to another.#
				#thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==1]<-3#
				#thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==2]<-1#
				#thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==3]<-2#
				#Making the partition random#
				new_cond <- sample(c(rep(c(1,2))),items_n, replace = TRUE)#
				thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==1] <- 3#
				thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==2] <- 4#
				thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==3]<- new_cond[item]#
				#thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==4]<- new_cond[item+items_n]#
				thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==4] <- ifelse(new_cond[item] ==1,2,1)#
			#}else{#
				#tails. do nothing.#
			#}#
		}#
	}#
	return(thedata)#
}
sample_cluster = function(thedata,threshold){#
	#This function does not assume any particular structure to the data, but it calls resample_data and get_stats which do#
	##
	#thedata = the data that you want to resample#
	#print(thedata[1:3,])#
	print(1)#
	sampled_data<-resample_data(thedata)#
	#print(sampled_data[1:3,])#
	sigs_me<-get_stats(sampled_data)#
	sampled_clusters<-cluster(sigs_me$z,threshold)#
	sampled_cluster_scores<-score_clusters(sigs_me$z,sampled_clusters)#
	return(max(sampled_cluster_scores$score))#
}#
#
sample_clusters = function(thedata,n,multi,threshold){#
	#This function does not assume any particular structure to the data, but it calls resample_data and get_stats (via sample_cluster), which do#
	##
	#thedata = the data that you want to resample#
	#n = number of times to resample#
	#if multi==TRUE, use multiple processors (will need to have loaded the appropriate libraries)#
#
	require(multicore)#
	require(doMC)#
	require(foreach)#
#
	if (multi){#
		registerDoMC(cores=3) #how many processes to spawn?	was written for a computer with 4 processors. This keeps 1 free for other uses.#
		clusters<-foreach(i=iter(c(1:n)),.combine=rbind) %dopar% sample_cluster(thedata,threshold)#
	}else{#
		clusters<-foreach(i=iter(c(1:n)),.combine=rbind) %do% sample_cluster(thedata,threshold)#
	}#
	return(clusters)#
}
# This R script uses JoshH's scripts to create a set of fake datasets, and then analyzes#
# those datasets using three different threshold.#
# j is number of simulations#
#
threshold = c(1.6,2.0,2.4)#
real_score = matrix(NA,1,3)#
pval <- matrix(NA,1000,3)#
#
#cluster the data#
for (j in c(1:1000)){#
print(j)#
mydata <- createdata()#
sigs_me<-get_stats(mydata)#
#
for (k in c(1:length(threshold))){ # k is number of thresholds#
actual_clusters<-cluster(sigs_me$z,threshold[k]) #cluster the actual data#
actual_cluster_scores<-score_clusters(sigs_me$z,actual_clusters) #find the scores for the clusters in the actual data#
real_score[1,k] <- max(actual_cluster_scores$score)#
sampled_maxes<-sample_clusters(mydata,1000,TRUE,threshold[k])#
pval[j,k] = 1 - sum(as.numeric(max(actual_cluster_scores$score) > c(max(actual_cluster_scores$score),sampled_maxes[,1])))/(length(sampled_maxes[,1])+1)#
#
}#
}#
#for (k in c(1:length(threshold))){ # k is number of thresholds#
#	sampled_maxes<-sample_clusters(mydata,1000,TRUE,threshold[k])#
#	pval[j,k] = 1 - sum(as.numeric(max(actual_cluster_scores$score) > c(max(actual_cluster_scores$score),sampled_maxes[,1])))/(length(sampled_maxes[,1])+1)#
#	#
#	}#
#}
logit(0)
library(arm)
logit(0)
invlogit(0)
?setwd
#values used for longdiff#
# s_n<-24 #how many subjects#
# switch_baseline<-.25 #probability of switching to target at baseline#
# switch_on<-.5 #probability of switching to target when effect is in effect#
# switch_off<-.1 #probability of switching off target when effect is in effect#
# items_n<-5 #how many items per condition? 2-condition experiment. items appear in both conditions, within subject#
# bins_n<-20 #how many bins?#
# first_bin<-7 #on average, what is the first bin to show an effect in condition with early effect?#
# time_diff<-2.5 #how many bins earlier is there an effect in condition 1 than condition 2?#
# noise<-2 #trial noise sd#
# sub_speeds<-rnorm(s_n,0,1) #how much faster or slower than typical is the subject?#
# sub_s_sd<-1 #sd for sub speed error#
# sub_effects<-rnorm(s_n,0,1.5) #degree to which effect size is larger or smaller for that subject#
# sub_e_sd<-1 #sd for sub effect error#
# item_effects<-rnorm(items_n,0,.5) #degree to which effect size is larger or smaller than typical for that item#
# item_sd<-1 #sd for item effect error#
#
#values used for shortdiff#
s_n<-24 #how many subjects#
switch_baseline<-.25 #probability of switching to target at baseline#
switch_on<-.9 #probability of switching to target when effect is in effect#
switch_off<-.2 #probability of switching off target when effect is in effect#
items_n<-5 #how many items per condition? 2-condition experiment. items appear in both conditions, within subject#
bins_n<-20 #how many bins?#
first_bin<-7 #on average, what is the first bin to show an effect in condition with early effect?#
time_diff<-0 #how many bins earlier is there an effect in condition 1 than condition 2?#
noise<-.25 #trial noise sd#
sub_speeds<-rnorm(s_n,0,.25) #how much faster or slower than typical is the subject?#
sub_s_sd<-.5 #sd for sub speed error#
sub_effects<-rnorm(s_n,0,2) #degree to which effect size is larger or smaller for that subject#
sub_e_sd<-.25 #sd for sub effect error#
item_effects<-rnorm(items_n,0,2) #degree to which effect size is larger or smaller than typical for that item#
item_sd<-.25 #sd for item effect error#
#what's alpha?#
1-.95^(1/bins_n)#
#
createdata <- function(){#
mydata<-data.frame(subject=c(NA),condition=c(NA),item=c(NA),bin=c(NA),fixtarg=c(NA))#
for (sub in c(1:s_n)){#
	for (item in c(1:items_n)){#
		#do a item for each condition simultaneously (saves time)#
		#draw a bin for an effect#
		con1_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
		con2_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise) #+time_diff+rnorm(1,sub_effects[sub],sub_e_sd)+rnorm(1,item_effects[item],item_sd)#
			current_1<-rbinom(1,1,.5) #where am I looking in first bin for condition 1?#
			current_2<-rbinom(1,1,.5) #where am I looking in first bin for condition 2?#
			mydata<-rbind(mydata,c(sub,1,item,1,current_1))#
			mydata<-rbind(mydata,c(sub,2,item,1,current_2))#
			for (bin in c(2:bins_n)){#
				if (bin<con1_bin){#
						if (rbinom(1,1,switch_baseline)){current_1<-(1-current_1)}#
					}else{#
						if (current_1){#
							if (rbinom(1,1,switch_off)){current_1<-(1-current_1)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_1<-(1-current_1)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,1,item,bin,current_1))#
				if (bin<con2_bin){#
						if (rbinom(1,1,switch_baseline)){current_2<-(1-current_2)}#
					}else{#
						if (current_2){#
							if (rbinom(1,1,switch_off)){current_2<-(1-current_2)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_2<-(1-current_2)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,2,item,bin,current_2))#
			}#
	}#
}#
return(mydata<-mydata[is.na(mydata$subject)==FALSE,])#
}#
mydata <- createdata()#
#
#calculate means#
submeans<-aggregate(mydata$fixtarg,by=list(mydata$subject,mydata$condition,mydata$bin),FUN=mean)#
colnames(submeans)<-c("subject","condition","bin","fixtarg")#
means<-aggregate(submeans$fixtarg,by=list(submeans$condition,submeans$bin),FUN=mean)#
colnames(means)<-c("condition","bin","fixtarg")#
temp<-aggregate(submeans$fixtarg,by=list(submeans$condition,submeans$bin),FUN=sd)#
means$se<-temp[,3]/(s_n^.5)
summary(mydata)
0.05/20
logit(1)
log((1/0))
log((0.95/0.05))
?names
load("/Users/hrabagli/Documents/Studies/Sense Resolution/3_Online_ET/ETAutismData/FullStats_April1.RDATA")
ls()
Full.Clusters
3*12
36+7
43/2
36+8
/2
44/22
?grepl
a = ("aa","bb","ss","ba")
a = c("aa","bb","ss","ba")
a
grep(a,"b")
grep("b",a)
grep("b",a) -1
a[(grep("b",a) -1)]
library(plyr)#
library(lme4)#
library(doBy)#
read_data <- function(path_name){#
list.files(path = path_name,full.names = T, pattern = ".csv") -> file_list#
comp = c()#
for (x in file_list){#
	data <- read.csv(x,header = T)#
	if ("perceptual.rating.reactiontime" %in% colnames(data)){ #
		data <- subset(data, select = -perceptual.rating.reactiontime)#
		}#
		if ("X" %in% colnames(data)){ #
		data <- subset(data, select = -X)#
		data$rt <- as.character(data$rt)#
		}#
	comp <- rbind(comp, data)#
	}#
	return(comp)#
}#
#
sense.pop <- read_data("./data/")#
#
# Make RTs numeric [need to remove timeout "none" responses to 8s]#
sense.pop <- subset(sense.pop, rt != "None")#
sense.pop$rt <- as.numeric(sense.pop$rt)#
sense.pop$Length <- nchar(as.character(sense.pop$prime),allowNA = T)#
sense.pop$Condition <- as.character(sense.pop$prime_semantics)#
sense.pop[sense.pop$prime_semantics %in% c("Sklar_control_A","Sklar_control_B"),]$Condition <- "Sklar_control"#
sense.pop$Condition <- as.factor(sense.pop$Condition)#
# Note that this analysis includes all of the inclusion criteria discussed by Sklar et al. #
#
###########################################################################################################################
##
# Let's first analyze for the Sklar trials#
sense.pop.sklar <- subset(sense.pop, Condition %in% c("Sklar_control", "Sklar_violation")) #
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < 3sd above group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.sklar), keep.names = T)#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.sklar <- subset(sense.pop.sklar, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(Condition), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
#
# Remove RTs < 200ms#
sense.pop.sklar <- subset(sense.pop.sklar, rt > 0.2)#
#
# T test (Sklar style)#
sense.pop.sklar.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.sklar.summary, paired = T)#
# lmer (Rabag style)#
print(summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")))))#
print(summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  perceptual.rating == 0 & Condition %in% c("Sklar_violation", "Sklar_control")))))#
print(summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  perceptual.rating != 0 & Condition %in% c("Sklar_violation", "Sklar_control")))))#
#
print(summary(lmer(log(rt) ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")))))#
print(summary(lmer(log(rt) ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  perceptual.rating == 0 & Condition %in% c("Sklar_violation", "Sklar_control")))))#
print(summary(lmer(log(rt) ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  perceptual.rating != 0 & Condition %in% c("Sklar_violation", "Sklar_control")))))#
###########################################################################################################################
##
# Let's now analyze for the new trials#
sense.pop.new <- subset(sense.pop, Condition %in% c("Sensible", "Non-sensible")) #
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.new), keep.names = T)#
sense.pop.new <- subset(sense.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
sense.pop.new <- subset(sense.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.new <- subset(sense.pop.new, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.new <- ddply(sense.pop.new, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.new <- ddply(sense.pop.new, .(Condition), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# Remove RTs < 200ms#
sense.pop.new <- subset(sense.pop.new, rt > 0.2)#
#
# T test (Sklar style)#
sense.pop.new.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.new.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.new.summary, paired = T)#
#
# lmer (rabag style)#
print(summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible")))))#
print(summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.new,  perceptual.rating == 0 & Condition %in% c("Non-sensible","Sensible")))))#
print(summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.new,  perceptual.rating != 0 & Condition %in% c("Non-sensible","Sensible")))))#
#
print(summary(lmer(log(rt) ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible")))))#
print(summary(lmer(log(rt) ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.new,  perceptual.rating == 0 & Condition %in% c("Non-sensible","Sensible")))))#
print(summary(lmer(log(rt) ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.new,  perceptual.rating != 0 & Condition %in% c("Non-sensible","Sensible")))))#
#
############################################################################################################################
##
# Finally -- a quick test if longer stims are perceived faster than shorter, #
#
sense.pop.length <- sense.pop#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.length), keep.names = T)#
sense.pop.length <- subset(sense.pop.length, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
sense.pop.length <- subset(sense.pop.length, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.length <- subset(sense.pop.length, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.length <- ddply(sense.pop.length, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# Remove RTs < 200ms#
sense.pop.length <- subset(sense.pop.length, rt > 0.2)#
#
# Standardize lenth#
sense.pop.length$Length <- (sense.pop.length$Length - mean(sense.pop.length$Length, na.rm = T))/sd(sense.pop.length$Length, na.rm = T)#
summary(lmer(rt ~ Length + (1+Length|SubjNo), data = sense.pop.length))#
#
############################################################################################################################
##
# Graphs#
#
sense.sklar.graph <- summaryBy(rt ~ Condition + SubjNo, data = sense.pop.sklar, keep.names = T)#
sense.sklar.graph <- summaryBy(rt ~ Condition, data = sense.sklar.graph, FUN = c(mean,sd))#
sense.sklar.graph$SE <- sense.sklar.graph$rt.sd/sqrt(length(unique(sense.pop.sklar$SubjNo)))#
sense.sklar.graph$Experiment <- "Experiment 1a"#
#
sense.new.graph <- summaryBy(rt ~ Condition + SubjNo, data = sense.pop.new, keep.names = T)#
sense.new.graph <- summaryBy(rt ~ Condition, data = sense.new.graph, FUN = c(mean,sd))#
sense.new.graph$SE <- sense.new.graph$rt.sd/sqrt(length(unique(sense.pop.sklar$SubjNo)))#
sense.new.graph$Experiment <- "Experiment 1b"#
#
sense.graph <- rbind(sense.sklar.graph,sense.new.graph)#
sense.graph$Cond_Graph <- "Violation"#
sense.graph[sense.graph$Condition %in% c("Sklar_control", "Sensible"),]$Cond_Graph <- "Control"#
sense.graph$Cond_Graph <- ordered(sense.graph$Cond_Graph, levels = c("Violation", "Control")) #
sense.graph$rt.mean <- sense.graph$rt.mean * 1000#
sense.graph$SE <- sense.graph$SE * 1000#
#
dodge <- position_dodge(width=0.9)#
qplot(sense.graph$Experiment, sense.graph$rt.mean, geom = "bar", stat = "identity", fill = sense.graph$Cond_Graph, ylab = "Reaction Time (ms)", xlab = "", position = dodge, ylim = c(0,2000)) +  geom_errorbar(aes(ymax = sense.graph$rt.mean + sense.graph$SE, ymin = sense.graph$rt.mean - sense.graph$SE), width=0.25, position = dodge) + labs(fill = "Sentence Type") + theme(axis.text.x = element_text(colour = "black", size = 12))
library(ggplot2)
sense.sklar.graph <- summaryBy(rt ~ Condition + SubjNo, data = sense.pop.sklar, keep.names = T)#
sense.sklar.graph <- summaryBy(rt ~ Condition, data = sense.sklar.graph, FUN = c(mean,sd))#
sense.sklar.graph$SE <- sense.sklar.graph$rt.sd/sqrt(length(unique(sense.pop.sklar$SubjNo)))#
sense.sklar.graph$Experiment <- "Experiment 1a"#
#
sense.new.graph <- summaryBy(rt ~ Condition + SubjNo, data = sense.pop.new, keep.names = T)#
sense.new.graph <- summaryBy(rt ~ Condition, data = sense.new.graph, FUN = c(mean,sd))#
sense.new.graph$SE <- sense.new.graph$rt.sd/sqrt(length(unique(sense.pop.sklar$SubjNo)))#
sense.new.graph$Experiment <- "Experiment 1b"#
#
sense.graph <- rbind(sense.sklar.graph,sense.new.graph)#
sense.graph$Cond_Graph <- "Violation"#
sense.graph[sense.graph$Condition %in% c("Sklar_control", "Sensible"),]$Cond_Graph <- "Control"#
sense.graph$Cond_Graph <- ordered(sense.graph$Cond_Graph, levels = c("Violation", "Control")) #
sense.graph$rt.mean <- sense.graph$rt.mean * 1000#
sense.graph$SE <- sense.graph$SE * 1000#
#
dodge <- position_dodge(width=0.9)#
qplot(sense.graph$Experiment, sense.graph$rt.mean, geom = "bar", stat = "identity", fill = sense.graph$Cond_Graph, ylab = "Reaction Time (ms)", xlab = "", position = dodge, ylim = c(0,2000)) +  geom_errorbar(aes(ymax = sense.graph$rt.mean + sense.graph$SE, ymin = sense.graph$rt.mean - sense.graph$SE), width=0.25, position = dodge) + labs(fill = "Sentence Type") + theme(axis.text.x = element_text(colour = "black", size = 12))
length(unique(sense.pop.sklar$Item))
length(unique(sense.pop.sklar$Prime))
length(unique(sense.pop.sklar$prime))
length(unique(sense.pop.new$prime))
34/150
150/34
300/70
length(unique(sense.pop.new$SubjNo))
length(unique(sense.pop.Sklar$SubjNo))
length(unique(sense.pop.sklar$SubjNo))
library(plyr)#
library(lme4)#
library(doBy)#
#
emo.pop <- read.csv("all_data_with_pairIDs.csv")#
#
# Make RTs numeric#
emo.pop <- subset(emo.pop, rt != "None")#
#
emo.pop$rt <- as.numeric(as.character(emo.pop$rt))#
emo.pop$Length <- nchar(as.character(emo.pop$prime),allowNA = T)#
#
# First standardize the MeanAffectivity score#
emo.pop$MeanAffectivity <- (emo.pop$MeanAffectivity - mean(emo.pop$MeanAffectivity, na.rm = T))/sd(emo.pop$MeanAffectivity, na.rm = T)#
#
###########################################################################################################################
##
# Sklar Experiment first#
emo.pop.sklar <- subset(emo.pop, prime_semantics %in% c("Negative phrase","Neutral phrase"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.sklar), keep.names = T)#
emo.pop.sklar <- subset(emo.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.sklar <- subset(emo.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.sklar <- subset(emo.pop.sklar, match. == 1)#
#
emo.pop.sklar <- ddply(emo.pop.sklar, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.sklar <- subset(emo.pop.sklar, rt > 0.2)#
#
# First standardize the MeanAffectivity score#
emo.pop.sklar$MeanAffectivity <- (emo.pop.sklar$MeanAffectivity - mean(emo.pop.sklar$MeanAffectivity, na.rm = T))/sd(emo.pop.sklar$MeanAffectivity, na.rm = T)#
#
#Lin Reg (sklar style)#
emo.pop.sklar.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.sklar, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.sklar.sum))#
#
# note that you can calculate BF by estimating the sample SE from Sklar's #
# regression coefficient, his t stat, and his sample size (46) #
#
# lmer (rabag style)#
print(summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase")))))#
###########################################################################################################################
##
# New reversed sentences Experiment next #
emo.pop.new <- subset(emo.pop, prime_semantics %in% c("Negative sentence","Neutral sentence"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.new), keep.names = T)#
emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.new <- subset(emo.pop.new, match. == 1)#
#
emo.pop.new <- ddply(emo.pop.new, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.new <- subset(emo.pop.new, rt > 0.2)#
#
#  standardize the MeanAffectivity score#
emo.pop.new$MeanAffectivity <- (emo.pop.new$MeanAffectivity - mean(emo.pop.new$MeanAffectivity, na.rm = T))/sd(emo.pop.new$MeanAffectivity, na.rm = T)#
#
emo.pop.new.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.new, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.new.sum))#
#
# lmer (rabag style)#
print(summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence")))))#
#
# lmer t-test equivalent (rabag style)#
print(summary(lmer(rt ~ prime_semantics + (1+prime_semantics|SubjNo)+ (1+prime_semantics|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence")))))#
############################################################################################################################
##
# Finally, Hebrew Experiment#
emo.pop.hebr <- subset(emo.pop, prime_semantics %in% c("Hebrew"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.hebr), keep.names = T)#
emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.hebr <- subset(emo.pop.hebr, match. == 1)#
#
emo.pop.hebr <- ddply(emo.pop.hebr, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.hebr <- subset(emo.pop.hebr, rt > 0.2)#
#
# Can't do a by-items analysis until Hebrew is properly recoded.#
#  standardize the MeanAffectivity score#
emo.pop.hebr$MeanAffectivity <- (emo.pop.hebr$MeanAffectivity - mean(emo.pop.hebr$MeanAffectivity, na.rm = T))/sd(emo.pop.hebr$MeanAffectivity, na.rm = T)#
#
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.hebr, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.hebr.sum))#
# lmer (rabag style)#
print(summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew")))))#
print(summary(lmer(rt ~ as.factor(Contrast) + (1+as.factor(Contrast)|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew")))))#
#
###########################################################################################################################
# We can also check if this depends on perceptual rating#
# Finally -- a quick test if English is perceived faster than Hebrew (following Jiang et al 07, #
# tho note that there are length confounds here#
emo.pop$Lang <- "English"#
emo.pop[emo.pop$prime_semantics %in% c("Hebrew"),]$Lang <- "Hebrew"#
summary(lmer(rt ~ Lang + (1+Lang|SubjNo), data = subset(emo.pop, Contrast == 50)))
ggplot(dat, aes(x=emo.pop.hebr.sum$MeanAffectivity, y=emo.pop.hebr.sum$rt)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE)
ggplot(emo.pop.hebr.sum, aes(x=MeanAffectivity, y=rt)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE)
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
graph <- cbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c")
emo.pop.sklar.sum
emo.pop.sklar.sum
emo.pop.sklar.sum <- na.rm(emo.pop.sklar.sum)#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c")
)
summary(graph)
emo.pop.new.sum
graph
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
graph <- na.omit(graph)
summary(graph)
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c")
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))
summary(graph)
ggplot(graph, aes(x=MeanAffectivity, y=rt)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment)
ggplot(graph, aes(x=MeanAffectivity, y=rt), ylab = "Reaction Time (ms)", xlab = "Standardized Valence Rating") +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment)
ggplot(graph, aes(x=MeanAffectivity, y=rt, ylab = "Reaction Time (ms)", xlab = "Standardized Valence Rating")) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment)
ggplot(graph, aes(x=MeanAffectivity, y=rt)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
summary(emo.pop)
ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt, color = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
graph$Contrast <- ordered(graph$Contrast, levels = c("50%","80%"))
ggplot(graph, aes(x=MeanAffectivity, y=rt, color = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
?ordered
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt, fill = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt, color = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt, color = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(alpha = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(alpha = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt, alpha = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(alpha = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point()
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = Contrast), pgh = 21, color = "black") +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point()
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = Contrast), pgh = 21, color = "black") +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = Contrast), pch = 21, color = "black") +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = Contrast), pch = 21, color = "black") +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point()
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = Contrast), pch = 21, color = "black") +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(shape = Contrast)))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = Contrast), pch = 21, color = "black") +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(shape = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = Contrast), pch = 21, color = "black") +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(shape = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = Contrast), pch = 21, color = "black") +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = Contrast), pch = 21, color = "white") +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(shape = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,color = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(shape = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,color = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(size = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,color = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(shape = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,color = Contrast), size = 1) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(shape = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,color = Contrast), size = 1) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,color = Contrast, size = 1)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,color = Contrast, size = 0.1)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,color = Contrast, shape = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,color = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(shape = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(x=MeanAffectivity, y=rt,color = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,color = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(x=MeanAffectivity, y=rt,color = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,color = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(x=MeanAffectivity, y=rt,shape = Contrast))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = Contrast)) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(x=MeanAffectivity, y=rt,shape = Contrast))
ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = Contrast)) +#
    geom_point() +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = Contrast)) +#
    geom_point(color = Contrast) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = Contrast)) +#
    geom_point() +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
library(plyr)#
library(lme4)#
library(doBy)#
#
emo.pop <- read.csv("all_data_with_pairIDs.csv")#
#
# Make RTs numeric#
emo.pop <- subset(emo.pop, rt != "None")#
#
emo.pop$rt <- as.numeric(as.character(emo.pop$rt))#
emo.pop$Length <- nchar(as.character(emo.pop$prime),allowNA = T)#
#
# First standardize the MeanAffectivity score#
emo.pop$MeanAffectivity <- (emo.pop$MeanAffectivity - mean(emo.pop$MeanAffectivity, na.rm = T))/sd(emo.pop$MeanAffectivity, na.rm = T)#
#
###########################################################################################################################
##
# Sklar Experiment first#
emo.pop.sklar <- subset(emo.pop, prime_semantics %in% c("Negative phrase","Neutral phrase"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.sklar), keep.names = T)#
emo.pop.sklar <- subset(emo.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.sklar <- subset(emo.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.sklar <- subset(emo.pop.sklar, match. == 1)#
#
emo.pop.sklar <- ddply(emo.pop.sklar, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.sklar <- subset(emo.pop.sklar, rt > 0.2)#
#
# First standardize the MeanAffectivity score#
emo.pop.sklar$MeanAffectivity <- (emo.pop.sklar$MeanAffectivity - mean(emo.pop.sklar$MeanAffectivity, na.rm = T))/sd(emo.pop.sklar$MeanAffectivity, na.rm = T)#
#
#Lin Reg (sklar style)#
emo.pop.sklar.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.sklar, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.sklar.sum))#
#
# note that you can calculate BF by estimating the sample SE from Sklar's #
# regression coefficient, his t stat, and his sample size (46) #
#
# lmer (rabag style)#
print(summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase")))))#
###########################################################################################################################
##
# New reversed sentences Experiment next #
emo.pop.new <- subset(emo.pop, prime_semantics %in% c("Negative sentence","Neutral sentence"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.new), keep.names = T)#
emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.new <- subset(emo.pop.new, match. == 1)#
#
emo.pop.new <- ddply(emo.pop.new, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.new <- subset(emo.pop.new, rt > 0.2)#
#
#  standardize the MeanAffectivity score#
emo.pop.new$MeanAffectivity <- (emo.pop.new$MeanAffectivity - mean(emo.pop.new$MeanAffectivity, na.rm = T))/sd(emo.pop.new$MeanAffectivity, na.rm = T)#
#
emo.pop.new.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.new, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.new.sum))#
#
# lmer (rabag style)#
print(summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence")))))#
#
# lmer t-test equivalent (rabag style)#
print(summary(lmer(rt ~ prime_semantics + (1+prime_semantics|SubjNo)+ (1+prime_semantics|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence")))))#
############################################################################################################################
##
# Finally, Hebrew Experiment#
emo.pop.hebr <- subset(emo.pop, prime_semantics %in% c("Hebrew"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.hebr), keep.names = T)#
emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.hebr <- subset(emo.pop.hebr, match. == 1)#
#
emo.pop.hebr <- ddply(emo.pop.hebr, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.hebr <- subset(emo.pop.hebr, rt > 0.2)#
#
# Can't do a by-items analysis until Hebrew is properly recoded.#
#  standardize the MeanAffectivity score#
emo.pop.hebr$MeanAffectivity <- (emo.pop.hebr$MeanAffectivity - mean(emo.pop.hebr$MeanAffectivity, na.rm = T))/sd(emo.pop.hebr$MeanAffectivity, na.rm = T)#
#
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.hebr, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.hebr.sum))#
# lmer (rabag style)#
print(summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew")))))#
print(summary(lmer(rt ~ as.factor(Contrast) + (1+as.factor(Contrast)|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew")))))#
#
###########################################################################################################################
# We can also check if this depends on perceptual rating#
# Finally -- a quick test if English is perceived faster than Hebrew (following Jiang et al 07, #
# tho note that there are length confounds here#
emo.pop$Lang <- "English"#
emo.pop[emo.pop$prime_semantics %in% c("Hebrew"),]$Lang <- "Hebrew"#
summary(lmer(rt ~ Lang + (1+Lang|SubjNo), data = subset(emo.pop, Contrast == 50)))#
#
###########################################################################################################################
##
# Graphs#
#
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = Contrast)) +#
    geom_point() +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
#Lin Reg (sklar style)#
emo.pop.sklar.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.sklar, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.sklar.sum))#
summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.sklar.sum))#
#
# note that you can calculate BF by estimating the sample SE from Sklar's #
# regression coefficient, his t stat, and his sample size (46) #
#
# lmer (rabag style)#
print(summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase")))))#
print(summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase")))))
2*pnorm(-abs(1.71))
emo.sklar.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
print(emo.sklar.lmer.raw)
coef(emo.sklar.lmer.raw)
coef(emo.sklar.lmer.raw)$t
coef(emo.sklar.lmer.raw)@t
coef(emo.sklar.lmer.raw)[3,1]
coef(emo.sklar.lmer.raw)[1,3]
coef(emo.sklar.lmer.raw)[2,3]
*pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))
2*pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))
emo.sklar.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
print(emo.sklar.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))#
emo.sklar.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
print(emo.sklar.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.log)[2,3]))
emo.sklar.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
print(emo.sklar.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))))#
emo.sklar.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
print(emo.sklar.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.log)[2,3]))))
emo.sklar.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
print(emo.sklar.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))))#
emo.sklar.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
print(emo.sklar.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.log)[2,3]))))
emo.new.lmer.raw <- summary(lmer(rt ~ prime_semantics + (1+prime_semantics|SubjNo)+ (1+prime_semantics|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
print(emo.new.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.raw)[2,3]))))#
emo.new.lmer.log <- summary(lmer(log(rt) ~ prime_semantics + (1+prime_semantics|SubjNo)+ (1+prime_semantics|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
print(emo.new.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.log)[2,3]))))
# lmer (rabag style)#
emo.new.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
print(emo.new.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.raw)[2,3]))))#
emo.new.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
print(emo.new.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.log)[2,3]))))
# New reversed sentences Experiment next #
emo.pop.new <- subset(emo.pop, prime_semantics %in% c("Negative sentence","Neutral sentence"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.new), keep.names = T)#
emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.new <- subset(emo.pop.new, match. == 1)#
#
emo.pop.new <- ddply(emo.pop.new, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.new <- subset(emo.pop.new, rt > 0.2)#
#
#  standardize the MeanAffectivity score#
emo.pop.new$MeanAffectivity <- (emo.pop.new$MeanAffectivity - mean(emo.pop.new$MeanAffectivity, na.rm = T))/sd(emo.pop.new$MeanAffectivity, na.rm = T)#
#
emo.pop.new.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.new, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.new.sum))#
#
# lmer (rabag style)#
emo.new.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
print(emo.new.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.raw)[2,3]))))#
emo.new.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
print(emo.new.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.log)[2,3]))))
summary(lm(rt ~ MeanAffectivity, data = emo.pop.new.sum))#
summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.new.sum))#
# lmer (rabag style)#
emo.new.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
print(emo.new.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.raw)[2,3]))))#
emo.new.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
print(emo.new.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.log)[2,3]))))
emo.pop.hebr <- subset(emo.pop, prime_semantics %in% c("Hebrew"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.hebr), keep.names = T)#
emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.hebr <- subset(emo.pop.hebr, match. == 1)#
#
emo.pop.hebr <- ddply(emo.pop.hebr, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.hebr <- subset(emo.pop.hebr, rt > 0.2)#
#
# Can't do a by-items analysis until Hebrew is properly recoded.#
#  standardize the MeanAffectivity score#
emo.pop.hebr$MeanAffectivity <- (emo.pop.hebr$MeanAffectivity - mean(emo.pop.hebr$MeanAffectivity, na.rm = T))/sd(emo.pop.hebr$MeanAffectivity, na.rm = T)#
#
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.hebr, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.hebr.sum))#
summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.hebr.sum))#
# lmer (rabag style)#
emo.hebr.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.hebr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.raw)[2,3]))))#
emo.hebr.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.hebr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.log)[2,3]))))#
emo.contr.lmer.raw <- summary(lmer(rt ~ as.factor(Contrast) + (1+as.factor(Contrast)|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.raw)[2,3]))))#
emo.contr.lmer.log <- summary(lmer(rt ~ as.factor(Contrast) + (1+as.factor(Contrast)|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[2,3]))))
emo.hebr.lmer.raw <- summary(lmer(rt ~ MeanAffectivity*Contrast + (1+MeanAffectivity*Contrast|SubjNo)+ (1+Contrast|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.hebr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.raw)[2,3]))))#
emo.hebr.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity*Contrast + (1+MeanAffectivity*Contrast|SubjNo)+ (1+Contrast|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.hebr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.log)[2,3]))))
emo.hebr.lmer.raw <- summary(lmer(rt ~ MeanAffectivity*Contrast + (1+MeanAffectivity*Contrast|SubjNo)+ (1+Contrast|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.hebr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.raw)[2,3]))))#
emo.hebr.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity*Contrast + (1+MeanAffectivity*Contrast|SubjNo)+ (1+Contrast|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.hebr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.log)[2,3]))))as.factor(Contrast)) + (1+as.factor(as.factor(Contrast))|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew")))))#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ as.factor(as.factor(Contrast)) + (1 + as.factor(as.factor(Contrast)) | SubjNo) +      (1 | prime)#
   Data: subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))#
#
REML criterion at convergence: 12105#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-2.4664 -0.4611 -0.1503  0.2224  7.9092 #
#
Random effects:#
 Groups   Name                  Variance Std.Dev. Corr #
 SubjNo   (Intercept)           0.266673 0.51640       #
          as.factor(as.factor(Contrast))80 0.009438 0.09715  -0.36#
 prime    (Intercept)           0.002206 0.04697       #
 Residual                       0.446862 0.66848       #
Number of obs: 5800, groups:  SubjNo, 68; prime, 45#
#
Fixed effects:#
                      Estimate Std. Error t value#
(Intercept)            1.52781    0.06424   23.78#
as.factor(as.factor(Contrast))80 -0.18448    0.02116   -8.72#
#
Correlation of Fixed Effects:#
            (Intr)#
as.fct(C)80 -0.313#
> #
> ###########################################################################################################################
> #
> #
> #
> # We can also check if this depends on perceptual rating#
> #
> #
> # Finally -- a quick test if English is perceived faster than Hebrew (following Jiang et al 07, #
> # tho note that there are length confounds here#
> emo.pop$Lang <- "English"#
> emo.pop[emo.pop$prime_semantics %in% c("Hebrew"),]$Lang <- "Hebrew"#
> summary(lmer(rt ~ Lang + (1+Lang|SubjNo), data = subset(emo.pop, as.factor(Contrast) == 50)))#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ Lang + (1 + Lang | SubjNo)#
   Data: subset(emo.pop, as.factor(Contrast) == 50)#
#
REML criterion at convergence: 27916.8#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-2.9035 -0.4661 -0.1591  0.1598  7.2967 #
#
Random effects:#
 Groups   Name        Variance Std.Dev. Corr #
 SubjNo   (Intercept) 0.38359  0.6194        #
          LangHebrew  0.02607  0.1615   -0.08#
 Residual             0.75151  0.8669        #
Number of obs: 10791, groups:  SubjNo, 73#
#
Fixed effects:#
            Estimate Std. Error t value#
(Intercept)  1.59377    0.07317  21.782#
LangHebrew   0.09718    0.02631   3.694#
#
Correlation of Fixed Effects:#
           (Intr)#
LangHebrew -0.110#
> #
> #
> #
> ggplot(dat, aes(x=emo.pop.hebr.sum$MeanAffectivity, y=emo.pop.hebr.sum$rt)) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) #
Error in ggplot(dat, aes(x = emo.pop.hebr.sum$MeanAffectivity, y = emo.pop.hebr.sum$rt)) : #
  object 'dat' not found#
> ggplot(emo.pop.hebr.sum, aes(x=MeanAffectivity, y=rt)) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE)#
Warning messages:#
1: Removed 45 rows containing missing values (stat_smooth). #
2: Removed 45 rows containing missing values (geom_point). #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> graph <- cbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
Error in data.frame(..., check.names = FALSE) : #
  arguments imply differing number of rows: 100, 112, 90#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c")#
+ emo.pop.sklar.sum#
Error: unexpected symbol in:#
"graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c")#
emo.pop.sklar.sum"#
> #
> emo.pop.sklar.sum#
                  prime MeanAffectivity        rt    Experiment#
1    apartment building    0.8254156367 1.4291358 Experiment 2a#
2    apartment building              NA 0.9797384 Experiment 2a#
3              backpack    0.9385924147 1.4255491 Experiment 2a#
4              backpack              NA 1.7880103 Experiment 2a#
5            bad breath   -0.9408083739 1.5500970 Experiment 2a#
6            bad breath              NA 0.8470525 Experiment 2a#
7             black eye   -0.9035689823 1.5293729 Experiment 2a#
8             black eye              NA 2.0735509 Experiment 2a#
9            black head   -0.4394914567 1.5466056 Experiment 2a#
10           black head              NA 1.8289063 Experiment 2a#
11            body wash    1.4258738164 1.5563579 Experiment 2a#
12            body wash              NA 1.6313469 Experiment 2a#
13        cardboard box    0.6047344253 1.4113357 Experiment 2a#
14        cardboard box              NA 1.3202199 Experiment 2a#
15              carpark    0.6511936277 1.5991092 Experiment 2a#
16              carpark              NA 4.5518490 Experiment 2a#
17       clothes closet    0.8834896394 1.5090789 Experiment 2a#
18       clothes closet              NA 0.8177645 Experiment 2a#
19             cogwheel    0.6677861998 1.6360193 Experiment 2a#
20             cogwheel              NA 2.0972026 Experiment 2a#
21         computer bug   -0.8377671512 1.6334572 Experiment 2a#
22         computer bug              NA 1.2706453 Experiment 2a#
23      computer screen    1.0112524459 1.5233208 Experiment 2a#
24      computer screen              NA 1.4238702 Experiment 2a#
25   concentration camp   -1.6879731900 1.5020871 Experiment 2a#
26   concentration camp              NA 0.9007366 Experiment 2a#
27              cow pat    0.0007647942 1.5690014 Experiment 2a#
28              cow pat              NA 1.8300570 Experiment 2a#
29        dentist drill   -0.7425824437 1.6649823 Experiment 2a#
30        dentist drill              NA 1.1315146 Experiment 2a#
31            desk lamp    0.9531784433 1.5772899 Experiment 2a#
32            desk lamp              NA 2.1865981 Experiment 2a#
33         dining table    1.2238303080 1.5742155 Experiment 2a#
34         dining table              NA 1.9399641 Experiment 2a#
35       electric chair   -1.3908503840 1.5361423 Experiment 2a#
36       electric chair              NA 1.7320906 Experiment 2a#
37          facial hair    0.4885864195 1.3840316 Experiment 2a#
38          facial hair              NA 0.8296614 Experiment 2a#
39          foot fungus   -1.2244616128 1.5372243 Experiment 2a#
40          foot fungus              NA 1.2078900 Experiment 2a#
41           front door    1.0099018878 1.3800098 Experiment 2a#
42           front door              NA 0.9751809 Experiment 2a#
43           frost bite   -1.2482314374 1.5582770 Experiment 2a#
44           frost bite              NA 1.1703937 Experiment 2a#
45           gear shift    0.7246639940 1.3707555 Experiment 2a#
46           gear shift              NA 1.1804720 Experiment 2a#
47            hand rail    0.9067192408 1.4247812 Experiment 2a#
48            hand rail              NA 1.5114807 Experiment 2a#
49       helicopter pad    0.9029376779 1.4112403 Experiment 2a#
50       helicopter pad              NA 2.5041617 Experiment 2a#
51         high ceiling    0.9742471510 1.6132694 Experiment 2a#
52         high ceiling              NA 1.7217713 Experiment 2a#
53    human trafficking   -1.8224756114 1.5068482 Experiment 2a#
54    human trafficking              NA 1.0061660 Experiment 2a#
55         ironed shirt    1.6398022366 1.6249425 Experiment 2a#
56         ironed shirt              NA 1.2486314 Experiment 2a#
57               kidnap   -1.6404335410 1.6013102 Experiment 2a#
58               kidnap              NA 1.3372770 Experiment 2a#
59            letterbox    0.8254156367 1.5520723 Experiment 2a#
60            letterbox              NA 1.2783638 Experiment 2a#
61          lined paper    0.8308358767 1.4068486 Experiment 2a#
62          lined paper              NA 1.6588333 Experiment 2a#
63        missing teeth   -0.8679142455 1.5303623 Experiment 2a#
64        missing teeth              NA 1.1099573 Experiment 2a#
65             overdose   -1.4007544775 1.6460782 Experiment 2a#
66             overdose              NA 1.2194616 Experiment 2a#
67           paper clip    0.7789564343 1.5784409 Experiment 2a#
68           paper clip              NA 1.3673569 Experiment 2a#
69  pedestrian crossing    0.8718748392 1.5085933 Experiment 2a#
70  pedestrian crossing              NA 1.1413281 Experiment 2a#
71           root canal   -1.2360824159 1.6564167 Experiment 2a#
72           root canal              NA 1.2133107 Experiment 2a#
73           runny nose   -0.6302160008 1.3967652 Experiment 2a#
74           runny nose              NA 0.8320130 Experiment 2a#
75             skinhead   -1.1208930921 1.5970269 Experiment 2a#
76             skinhead              NA 1.1506419 Experiment 2a#
77          speed bumps    0.2175744052 1.4672106 Experiment 2a#
78          speed bumps              NA 1.3975387 Experiment 2a#
79          spotty face   -0.3825236255 1.5599323 Experiment 2a#
80          spotty face              NA 1.0118064 Experiment 2a#
81         stomach pump   -0.9748784558 1.4388479 Experiment 2a#
82         stomach pump              NA 1.5295700 Experiment 2a#
83            tile roof    0.8194791829 1.6938697 Experiment 2a#
84            tile roof              NA 2.4781050 Experiment 2a#
85           tin opener    0.8421925710 1.5182643 Experiment 2a#
86           tin opener              NA 1.8380783 Experiment 2a#
87        traffic light    0.7441120326 1.6413041 Experiment 2a#
88        traffic light              NA 1.1545760 Experiment 2a#
89      upside-down car   -0.8935748517 1.4861660 Experiment 2a#
90      upside-down car              NA 2.5949883 Experiment 2a#
91     vegetative state   -1.2125767006 1.5056679 Experiment 2a#
92     vegetative state              NA 0.9636461 Experiment 2a#
93       water boarding   -1.0461879294 1.6217085 Experiment 2a#
94       water boarding              NA 1.1995029 Experiment 2a#
95           weak knees   -0.5945612640 1.7476794 Experiment 2a#
96           weak knees              NA 1.7842463 Experiment 2a#
97           windshield    0.7905712350 1.4938892 Experiment 2a#
98           windshield              NA 2.9642263 Experiment 2a#
99         wooden stool    0.7078511916 1.4997280 Experiment 2a#
100        wooden stool              NA 0.9194078 Experiment 2a#
> #
> #
> emo.pop.sklar.sum <- na.rm(emo.pop.sklar.sum)#
Error: could not find function "na.rm"#
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c")#
+ )#
> summary(graph)#
                 prime     MeanAffectivity          rt                 Experiment #
 Andy cut the paper :  2   Min.   :-1.82248   Min.   :0.7698   Experiment 2a:100  #
 apartment building :  2   1st Qu.:-0.94137   1st Qu.:1.2283   Experiment 2b:112  #
 Baby chased the dog:  2   Median : 0.13858   Median :1.4272   Experiment 2c: 90  #
 backpack           :  2   Mean   : 0.00032   Mean   :1.4506                      #
 bad breath         :  2   3rd Qu.: 0.83254   3rd Qu.:1.5666                      #
 black eye          :  2   Max.   : 2.38362   Max.   :4.5518                      #
 (Other)            :290   NA's   :151                                            #
> emo.pop.new.sum#
                                   prime MeanAffectivity        rt    Experiment#
1                     Andy cut the paper      0.88535686 1.4389739 Experiment 2b#
2                     Andy cut the paper              NA 2.1998320 Experiment 2b#
3                    Baby chased the dog      0.98730441 1.3581484 Experiment 2b#
4                    Baby chased the dog              NA 0.9979614 Experiment 2b#
5                     Bobby ate the boar      0.37621499 1.4757611 Experiment 2b#
6                     Bobby ate the boar              NA 1.3883197 Experiment 2b#
7                 Claire dragged the car      0.17147103 1.4751937 Experiment 2b#
8                 Claire dragged the car              NA 1.4495037 Experiment 2b#
9            Claire went over the bridge      0.35639326 1.3612617 Experiment 2b#
10           Claire went over the bridge              NA 1.9539474 Experiment 2b#
11               Dad slapped the toddler     -1.14375125 1.4519468 Experiment 2b#
12               Dad slapped the toddler              NA 1.1782887 Experiment 2b#
13              Harry caught the octopus      0.88347107 1.3658056 Experiment 2b#
14              Harry caught the octopus              NA 2.0231553 Experiment 2b#
15                  John kicked the baby     -1.21470403 1.3334698 Experiment 2b#
16                  John kicked the baby              NA 1.0707741 Experiment 2b#
17                      Mum hit the baby     -1.06859569 1.5287248 Experiment 2b#
18                      Mum hit the baby              NA 0.8004793 Experiment 2b#
19                  Sally licked the cat      0.33463771 1.3684630 Experiment 2b#
20                  Sally licked the cat              NA 1.1836904 Experiment 2b#
21                      Sam hit the ball      1.66395166 1.3040762 Experiment 2b#
22                      Sam hit the ball              NA 2.3058233 Experiment 2b#
23               Steve jumped on the dog     -0.22834775 1.4150698 Experiment 2b#
24               Steve jumped on the dog              NA 0.8567089 Experiment 2b#
25       The actor shot the photographer     -0.94193001 1.5748203 Experiment 2b#
26       The actor shot the photographer              NA 1.3335610 Experiment 2b#
27  The attacker followed the journalist     -0.99194530 1.4468667 Experiment 2b#
28  The attacker followed the journalist              NA 1.0274718 Experiment 2b#
29                      The baby hit Mum     -0.11033932 1.4177464 Experiment 2b#
30                      The baby hit Mum              NA 1.1492457 Experiment 2b#
31                The baby hit the brick     -0.24517317 1.5001271 Experiment 2b#
32                The baby hit the brick              NA 0.8001571 Experiment 2b#
33                  The baby kicked john      0.02190158 1.3970616 Experiment 2b#
34                  The baby kicked john              NA 0.8370081 Experiment 2b#
35                      The ball hit Sam     -0.37701072 1.3223356 Experiment 2b#
36                      The ball hit Sam              NA 0.8550584 Experiment 2b#
37                    The boar ate Bobby     -1.27527348 1.5598418 Experiment 2b#
38                    The boar ate Bobby              NA 0.9414200 Experiment 2b#
39                The brick hit the baby     -1.09035125 1.4656928 Experiment 2b#
40                The brick hit the baby              NA 0.9776710 Experiment 2b#
41           The bridge went over Claire      0.30571271 1.7315401 Experiment 2b#
42           The bridge went over Claire              NA 1.6054217 Experiment 2b#
43        The bunny outwitted the hunter      1.99844334 1.5996787 Experiment 2b#
44        The bunny outwitted the hunter              NA 0.8668016 Experiment 2b#
45           The car crushed the officer     -1.24086935 1.4832655 Experiment 2b#
46           The car crushed the officer              NA 1.4678054 Experiment 2b#
47                The car dragged Claire     -1.26161302 1.3950835 Experiment 2b#
48                The car dragged Claire              NA 0.9468784 Experiment 2b#
49                  The cat licked Sally      1.80845018 1.2974962 Experiment 2b#
50                  The cat licked Sally              NA 1.0226473 Experiment 2b#
51              The child scared the dog      0.29557660 1.3923421 Experiment 2b#
52              The child scared the dog              NA 1.8304699 Experiment 2b#
53             The child spun the washer      0.77197384 1.4548577 Experiment 2b#
54             The child spun the washer              NA 0.8633173 Experiment 2b#
55    The criminal tackled the policeman     -0.91062069 1.4893719 Experiment 2b#
56    The criminal tackled the policeman              NA 0.8629515 Experiment 2b#
57                   The dog chased baby     -0.46125307 1.4948821 Experiment 2b#
58                   The dog chased baby              NA 1.7676843 Experiment 2b#
59               The dog jumped on Steve      0.39693771 1.2830777 Experiment 2b#
60               The dog jumped on Steve              NA 1.5687600 Experiment 2b#
61              The dog scared the child     -0.45063429 1.4159657 Experiment 2b#
62              The dog scared the child              NA 0.9268649 Experiment 2b#
63       The elephant caught the poacher      1.37000443 1.4484097 Experiment 2b#
64       The elephant caught the poacher              NA 1.3286254 Experiment 2b#
65           The fire killed the fireman     -1.35495953 1.4184200 Experiment 2b#
66           The fire killed the fireman              NA 2.1683981 Experiment 2b#
67           The fireman killed the fire      2.38361559 1.4939368 Experiment 2b#
68           The fireman killed the fire              NA 1.4316603 Experiment 2b#
69               The fish ate the people     -0.95184087 1.5293644 Experiment 2b#
70               The fish ate the people              NA 1.0320196 Experiment 2b#
71        The hunter outwitted the bunny      0.18407937 1.4650770 Experiment 2b#
72        The hunter outwitted the bunny              NA 2.0312103 Experiment 2b#
73  The journalist followed the attacker      0.82265440 1.6568646 Experiment 2b#
74  The journalist followed the attacker              NA 1.1545047 Experiment 2b#
75       The kitten chewed the lawnmower      0.28544048 1.5386711 Experiment 2b#
76       The kitten chewed the lawnmower              NA 2.7542459 Experiment 2b#
77       The lawnmower chewed the kitten     -1.31347219 1.5138815 Experiment 2b#
78       The lawnmower chewed the kitten              NA 1.2563527 Experiment 2b#
79                    The male was a pig     -0.19331400 1.4641862 Experiment 2b#
80                    The male was a pig              NA 1.0509827 Experiment 2b#
81                 The man ate the snake      0.25849326 1.4506554 Experiment 2b#
82                 The man ate the snake              NA 0.7698196 Experiment 2b#
83        The nurse injected the patient      0.57420180 1.5168616 Experiment 2b#
84        The nurse injected the patient              NA 2.2532792 Experiment 2b#
85              The octopus caught Harry     -0.60085123 1.4815235 Experiment 2b#
86              The octopus caught Harry              NA 1.2403451 Experiment 2b#
87           The officer crushed the car     -0.63469320 1.4159685 Experiment 2b#
88           The officer crushed the car              NA 2.1331158 Experiment 2b#
89                    The paper cut Andy     -0.09973176 1.5184082 Experiment 2b#
90                    The paper cut Andy              NA 1.4726383 Experiment 2b#
91        The patient injected the nurse     -0.76376493 1.6097336 Experiment 2b#
92        The patient injected the nurse              NA 2.0349613 Experiment 2b#
93               The people ate the fish      1.42889041 1.5287399 Experiment 2b#
94               The people ate the fish              NA 1.0094527 Experiment 2b#
95       The photographer shot the actor      0.13858473 1.5230063 Experiment 2b#
96       The photographer shot the actor              NA 0.9742126 Experiment 2b#
97                    The pig was a male      0.83423853 1.5971607 Experiment 2b#
98                    The pig was a male              NA 2.0500350 Experiment 2b#
99       The poacher caught the elephant     -1.00332902 1.4594678 Experiment 2b#
100      The poacher caught the elephant              NA 1.1634280 Experiment 2b#
101   The policeman tackled the criminal      1.70449611 1.6196929 Experiment 2b#
102   The policeman tackled the criminal              NA 1.5057720 Experiment 2b#
103                The snake ate the man     -1.20975384 1.4631898 Experiment 2b#
104                The snake ate the man              NA 2.3996156 Experiment 2b#
105 The soldiers captured the terrorists      1.94328404 1.4142682 Experiment 2b#
106 The soldiers captured the terrorists              NA 1.3804228 Experiment 2b#
107 The terrorists captured the soldiers     -1.12298458 1.6242903 Experiment 2b#
108 The terrorists captured the soldiers              NA 1.1572220 Experiment 2b#
109              The toddler slapped dad     -0.35066233 1.5695809 Experiment 2b#
110              The toddler slapped dad              NA 0.8616101 Experiment 2b#
111            The washer spun the child     -0.77489568 1.4976582 Experiment 2b#
112            The washer spun the child              NA 1.1544720 Experiment 2b#
> graph#
                                   prime MeanAffectivity        rt    Experiment#
1                     apartment building    0.8254156367 1.4291358 Experiment 2a#
2                     apartment building              NA 0.9797384 Experiment 2a#
3                               backpack    0.9385924147 1.4255491 Experiment 2a#
4                               backpack              NA 1.7880103 Experiment 2a#
5                             bad breath   -0.9408083739 1.5500970 Experiment 2a#
6                             bad breath              NA 0.8470525 Experiment 2a#
7                              black eye   -0.9035689823 1.5293729 Experiment 2a#
8                              black eye              NA 2.0735509 Experiment 2a#
9                             black head   -0.4394914567 1.5466056 Experiment 2a#
10                            black head              NA 1.8289063 Experiment 2a#
11                             body wash    1.4258738164 1.5563579 Experiment 2a#
12                             body wash              NA 1.6313469 Experiment 2a#
13                         cardboard box    0.6047344253 1.4113357 Experiment 2a#
14                         cardboard box              NA 1.3202199 Experiment 2a#
15                               carpark    0.6511936277 1.5991092 Experiment 2a#
16                               carpark              NA 4.5518490 Experiment 2a#
17                        clothes closet    0.8834896394 1.5090789 Experiment 2a#
18                        clothes closet              NA 0.8177645 Experiment 2a#
19                              cogwheel    0.6677861998 1.6360193 Experiment 2a#
20                              cogwheel              NA 2.0972026 Experiment 2a#
21                          computer bug   -0.8377671512 1.6334572 Experiment 2a#
22                          computer bug              NA 1.2706453 Experiment 2a#
23                       computer screen    1.0112524459 1.5233208 Experiment 2a#
24                       computer screen              NA 1.4238702 Experiment 2a#
25                    concentration camp   -1.6879731900 1.5020871 Experiment 2a#
26                    concentration camp              NA 0.9007366 Experiment 2a#
27                               cow pat    0.0007647942 1.5690014 Experiment 2a#
28                               cow pat              NA 1.8300570 Experiment 2a#
29                         dentist drill   -0.7425824437 1.6649823 Experiment 2a#
30                         dentist drill              NA 1.1315146 Experiment 2a#
31                             desk lamp    0.9531784433 1.5772899 Experiment 2a#
32                             desk lamp              NA 2.1865981 Experiment 2a#
33                          dining table    1.2238303080 1.5742155 Experiment 2a#
34                          dining table              NA 1.9399641 Experiment 2a#
35                        electric chair   -1.3908503840 1.5361423 Experiment 2a#
36                        electric chair              NA 1.7320906 Experiment 2a#
37                           facial hair    0.4885864195 1.3840316 Experiment 2a#
38                           facial hair              NA 0.8296614 Experiment 2a#
39                           foot fungus   -1.2244616128 1.5372243 Experiment 2a#
40                           foot fungus              NA 1.2078900 Experiment 2a#
41                            front door    1.0099018878 1.3800098 Experiment 2a#
42                            front door              NA 0.9751809 Experiment 2a#
43                            frost bite   -1.2482314374 1.5582770 Experiment 2a#
44                            frost bite              NA 1.1703937 Experiment 2a#
45                            gear shift    0.7246639940 1.3707555 Experiment 2a#
46                            gear shift              NA 1.1804720 Experiment 2a#
47                             hand rail    0.9067192408 1.4247812 Experiment 2a#
48                             hand rail              NA 1.5114807 Experiment 2a#
49                        helicopter pad    0.9029376779 1.4112403 Experiment 2a#
50                        helicopter pad              NA 2.5041617 Experiment 2a#
51                          high ceiling    0.9742471510 1.6132694 Experiment 2a#
52                          high ceiling              NA 1.7217713 Experiment 2a#
53                     human trafficking   -1.8224756114 1.5068482 Experiment 2a#
54                     human trafficking              NA 1.0061660 Experiment 2a#
55                          ironed shirt    1.6398022366 1.6249425 Experiment 2a#
56                          ironed shirt              NA 1.2486314 Experiment 2a#
57                                kidnap   -1.6404335410 1.6013102 Experiment 2a#
58                                kidnap              NA 1.3372770 Experiment 2a#
59                             letterbox    0.8254156367 1.5520723 Experiment 2a#
60                             letterbox              NA 1.2783638 Experiment 2a#
61                           lined paper    0.8308358767 1.4068486 Experiment 2a#
62                           lined paper              NA 1.6588333 Experiment 2a#
63                         missing teeth   -0.8679142455 1.5303623 Experiment 2a#
64                         missing teeth              NA 1.1099573 Experiment 2a#
65                              overdose   -1.4007544775 1.6460782 Experiment 2a#
66                              overdose              NA 1.2194616 Experiment 2a#
67                            paper clip    0.7789564343 1.5784409 Experiment 2a#
68                            paper clip              NA 1.3673569 Experiment 2a#
69                   pedestrian crossing    0.8718748392 1.5085933 Experiment 2a#
70                   pedestrian crossing              NA 1.1413281 Experiment 2a#
71                            root canal   -1.2360824159 1.6564167 Experiment 2a#
72                            root canal              NA 1.2133107 Experiment 2a#
73                            runny nose   -0.6302160008 1.3967652 Experiment 2a#
74                            runny nose              NA 0.8320130 Experiment 2a#
75                              skinhead   -1.1208930921 1.5970269 Experiment 2a#
76                              skinhead              NA 1.1506419 Experiment 2a#
77                           speed bumps    0.2175744052 1.4672106 Experiment 2a#
78                           speed bumps              NA 1.3975387 Experiment 2a#
79                           spotty face   -0.3825236255 1.5599323 Experiment 2a#
80                           spotty face              NA 1.0118064 Experiment 2a#
81                          stomach pump   -0.9748784558 1.4388479 Experiment 2a#
82                          stomach pump              NA 1.5295700 Experiment 2a#
83                             tile roof    0.8194791829 1.6938697 Experiment 2a#
84                             tile roof              NA 2.4781050 Experiment 2a#
85                            tin opener    0.8421925710 1.5182643 Experiment 2a#
86                            tin opener              NA 1.8380783 Experiment 2a#
87                         traffic light    0.7441120326 1.6413041 Experiment 2a#
88                         traffic light              NA 1.1545760 Experiment 2a#
89                       upside-down car   -0.8935748517 1.4861660 Experiment 2a#
90                       upside-down car              NA 2.5949883 Experiment 2a#
91                      vegetative state   -1.2125767006 1.5056679 Experiment 2a#
92                      vegetative state              NA 0.9636461 Experiment 2a#
93                        water boarding   -1.0461879294 1.6217085 Experiment 2a#
94                        water boarding              NA 1.1995029 Experiment 2a#
95                            weak knees   -0.5945612640 1.7476794 Experiment 2a#
96                            weak knees              NA 1.7842463 Experiment 2a#
97                            windshield    0.7905712350 1.4938892 Experiment 2a#
98                            windshield              NA 2.9642263 Experiment 2a#
99                          wooden stool    0.7078511916 1.4997280 Experiment 2a#
100                         wooden stool              NA 0.9194078 Experiment 2a#
101                   Andy cut the paper    0.8853568565 1.4389739 Experiment 2b#
102                   Andy cut the paper              NA 2.1998320 Experiment 2b#
103                  Baby chased the dog    0.9873044064 1.3581484 Experiment 2b#
104                  Baby chased the dog              NA 0.9979614 Experiment 2b#
105                   Bobby ate the boar    0.3762149940 1.4757611 Experiment 2b#
106                   Bobby ate the boar              NA 1.3883197 Experiment 2b#
107               Claire dragged the car    0.1714710338 1.4751937 Experiment 2b#
108               Claire dragged the car              NA 1.4495037 Experiment 2b#
109          Claire went over the bridge    0.3563932648 1.3612617 Experiment 2b#
110          Claire went over the bridge              NA 1.9539474 Experiment 2b#
111              Dad slapped the toddler   -1.1437512525 1.4519468 Experiment 2b#
112              Dad slapped the toddler              NA 1.1782887 Experiment 2b#
113             Harry caught the octopus    0.8834710682 1.3658056 Experiment 2b#
114             Harry caught the octopus              NA 2.0231553 Experiment 2b#
115                 John kicked the baby   -1.2147040337 1.3334698 Experiment 2b#
116                 John kicked the baby              NA 1.0707741 Experiment 2b#
117                     Mum hit the baby   -1.0685956931 1.5287248 Experiment 2b#
118                     Mum hit the baby              NA 0.8004793 Experiment 2b#
119                 Sally licked the cat    0.3346377081 1.3684630 Experiment 2b#
120                 Sally licked the cat              NA 1.1836904 Experiment 2b#
121                     Sam hit the ball    1.6639516616 1.3040762 Experiment 2b#
122                     Sam hit the ball              NA 2.3058233 Experiment 2b#
123              Steve jumped on the dog   -0.2283477511 1.4150698 Experiment 2b#
124              Steve jumped on the dog              NA 0.8567089 Experiment 2b#
125      The actor shot the photographer   -0.9419300080 1.5748203 Experiment 2b#
126      The actor shot the photographer              NA 1.3335610 Experiment 2b#
127 The attacker followed the journalist   -0.9919453019 1.4468667 Experiment 2b#
128 The attacker followed the journalist              NA 1.0274718 Experiment 2b#
129                     The baby hit Mum   -0.1103393160 1.4177464 Experiment 2b#
130                     The baby hit Mum              NA 1.1492457 Experiment 2b#
131               The baby hit the brick   -0.2451731725 1.5001271 Experiment 2b#
132               The baby hit the brick              NA 0.8001571 Experiment 2b#
133                 The baby kicked john    0.0219015819 1.3970616 Experiment 2b#
134                 The baby kicked john              NA 0.8370081 Experiment 2b#
135                     The ball hit Sam   -0.3770107212 1.3223356 Experiment 2b#
136                     The ball hit Sam              NA 0.8550584 Experiment 2b#
137                   The boar ate Bobby   -1.2752734808 1.5598418 Experiment 2b#
138                   The boar ate Bobby              NA 0.9414200 Experiment 2b#
139               The brick hit the baby   -1.0903512498 1.4656928 Experiment 2b#
140               The brick hit the baby              NA 0.9776710 Experiment 2b#
141          The bridge went over Claire    0.3057127066 1.7315401 Experiment 2b#
142          The bridge went over Claire              NA 1.6054217 Experiment 2b#
143       The bunny outwitted the hunter    1.9984433445 1.5996787 Experiment 2b#
144       The bunny outwitted the hunter              NA 0.8668016 Experiment 2b#
145          The car crushed the officer   -1.2408693450 1.4832655 Experiment 2b#
146          The car crushed the officer              NA 1.4678054 Experiment 2b#
147               The car dragged Claire   -1.2616130153 1.3950835 Experiment 2b#
148               The car dragged Claire              NA 0.9468784 Experiment 2b#
149                 The cat licked Sally    1.8084501826 1.2974962 Experiment 2b#
150                 The cat licked Sally              NA 1.0226473 Experiment 2b#
151             The child scared the dog    0.2955765954 1.3923421 Experiment 2b#
152             The child scared the dog              NA 1.8304699 Experiment 2b#
153            The child spun the washer    0.7719738406 1.4548577 Experiment 2b#
154            The child spun the washer              NA 0.8633173 Experiment 2b#
155   The criminal tackled the policeman   -0.9106206856 1.4893719 Experiment 2b#
156   The criminal tackled the policeman              NA 0.8629515 Experiment 2b#
157                  The dog chased baby   -0.4612530710 1.4948821 Experiment 2b#
158                  The dog chased baby              NA 1.7676843 Experiment 2b#
159              The dog jumped on Steve    0.3969377113 1.2830777 Experiment 2b#
160              The dog jumped on Steve              NA 1.5687600 Experiment 2b#
161             The dog scared the child   -0.4506342878 1.4159657 Experiment 2b#
162             The dog scared the child              NA 0.9268649 Experiment 2b#
163      The elephant caught the poacher    1.3700044252 1.4484097 Experiment 2b#
164      The elephant caught the poacher              NA 1.3286254 Experiment 2b#
165          The fire killed the fireman   -1.3549595312 1.4184200 Experiment 2b#
166          The fire killed the fireman              NA 2.1683981 Experiment 2b#
167          The fireman killed the fire    2.3836155851 1.4939368 Experiment 2b#
168          The fireman killed the fire              NA 1.4316603 Experiment 2b#
169              The fish ate the people   -0.9518408726 1.5293644 Experiment 2b#
170              The fish ate the people              NA 1.0320196 Experiment 2b#
171       The hunter outwitted the bunny    0.1840793677 1.4650770 Experiment 2b#
172       The hunter outwitted the bunny              NA 2.0312103 Experiment 2b#
173 The journalist followed the attacker    0.8226543983 1.6568646 Experiment 2b#
174 The journalist followed the attacker              NA 1.1545047 Experiment 2b#
175      The kitten chewed the lawnmower    0.2854404836 1.5386711 Experiment 2b#
176      The kitten chewed the lawnmower              NA 2.7542459 Experiment 2b#
177      The lawnmower chewed the kitten   -1.3134721911 1.5138815 Experiment 2b#
178      The lawnmower chewed the kitten              NA 1.2563527 Experiment 2b#
179                   The male was a pig   -0.1933139972 1.4641862 Experiment 2b#
180                   The male was a pig              NA 1.0509827 Experiment 2b#
181                The man ate the snake    0.2584932601 1.4506554 Experiment 2b#
182                The man ate the snake              NA 0.7698196 Experiment 2b#
183       The nurse injected the patient    0.5742018025 1.5168616 Experiment 2b#
184       The nurse injected the patient              NA 2.2532792 Experiment 2b#
185             The octopus caught Harry   -0.6008512259 1.4815235 Experiment 2b#
186             The octopus caught Harry              NA 1.2403451 Experiment 2b#
187          The officer crushed the car   -0.6346932029 1.4159685 Experiment 2b#
188          The officer crushed the car              NA 2.1331158 Experiment 2b#
189                   The paper cut Andy   -0.0997317574 1.5184082 Experiment 2b#
190                   The paper cut Andy              NA 1.4726383 Experiment 2b#
191       The patient injected the nurse   -0.7637649291 1.6097336 Experiment 2b#
192       The patient injected the nurse              NA 2.0349613 Experiment 2b#
193              The people ate the fish    1.4288904069 1.5287399 Experiment 2b#
194              The people ate the fish              NA 1.0094527 Experiment 2b#
195      The photographer shot the actor    0.1385847271 1.5230063 Experiment 2b#
196      The photographer shot the actor              NA 0.9742126 Experiment 2b#
197                   The pig was a male    0.8342385259 1.5971607 Experiment 2b#
198                   The pig was a male              NA 2.0500350 Experiment 2b#
199      The poacher caught the elephant   -1.0033290234 1.4594678 Experiment 2b#
200      The poacher caught the elephant              NA 1.1634280 Experiment 2b#
201   The policeman tackled the criminal    1.7044961081 1.6196929 Experiment 2b#
202   The policeman tackled the criminal              NA 1.5057720 Experiment 2b#
203                The snake ate the man   -1.2097538396 1.4631898 Experiment 2b#
204                The snake ate the man              NA 2.3996156 Experiment 2b#
205 The soldiers captured the terrorists    1.9432840395 1.4142682 Experiment 2b#
206 The soldiers captured the terrorists              NA 1.3804228 Experiment 2b#
207 The terrorists captured the soldiers   -1.1229845848 1.6242903 Experiment 2b#
208 The terrorists captured the soldiers              NA 1.1572220 Experiment 2b#
209              The toddler slapped dad   -0.3506623252 1.5695809 Experiment 2b#
210              The toddler slapped dad              NA 0.8616101 Experiment 2b#
211            The washer spun the child   -0.7748956790 1.4976582 Experiment 2b#
212            The washer spun the child              NA 1.1544720 Experiment 2b#
213                                  1.3033933262 1.5248795 Experiment 2c#
214                                            NA 0.9994167 Experiment 2c#
215                                1.4910045182 1.4018963 Experiment 2c#
216                                          NA 2.0972534 Experiment 2c#
217                              -   -0.4085588007 1.4045359 Experiment 2c#
218                              -              NA 1.6382644 Experiment 2c#
219                               0.6233027552 1.5784784 Experiment 2c#
220                                         NA 1.9469129 Experiment 2c#
221                             -    1.0688793362 1.5271788 Experiment 2c#
222                             -              NA 1.0107867 Experiment 2c#
223                               0.4356915632 1.4268875 Experiment 2c#
224                                         NA 0.9903742 Experiment 2c#
225                                 1.0219765382 1.3396950 Experiment 2c#
226                                           NA 1.1428882 Experiment 2c#
227                                  0.9281709422 1.5372337 Experiment 2c#
228                                            NA 1.8194417 Experiment 2c#
229                              1.4206503212 1.3802096 Experiment 2c#
230                                        NA 1.2714892 Experiment 2c#
231                               -0.7274978271 1.4144477 Experiment 2c#
232                                          NA 1.4128062 Experiment 2c#
233                                0.7171083512 1.3485284 Experiment 2c#
234                                          NA 1.8580654 Experiment 2c#
235                                 -1.4169689577 1.5759109 Experiment 2c#
236                                            NA 1.3238460 Experiment 2c#
237                               -1.2293577657 1.3437848 Experiment 2c#
238                                          NA 0.8230460 Experiment 2c#
239                               -0.9291798585 1.5276060 Experiment 2c#
240                                          NA 0.9738110 Experiment 2c#
241                               -0.7368783867 1.5130639 Experiment 2c#
242                                          NA 1.0768655 Experiment 2c#
243                                -0.4789129977 1.3699853 Experiment 2c#
244                                           NA 1.1838264 Experiment 2c#
245                                0.6233027552 1.3826535 Experiment 2c#
246                                          NA 1.1171809 Experiment 2c#
247                                 0.9750737402 1.4617126 Experiment 2c#
248                                           NA 2.2296943 Experiment 2c#
249                               0.4825943612 1.3921772 Experiment 2c#
250                                         NA 1.2715997 Experiment 2c#
251                               -1.5107745537 1.3541424 Experiment 2c#
252                                          NA 0.9328489 Experiment 2c#
253                             -0.9244895787 1.3349821 Experiment 2c#
254                                        NA 1.2243305 Experiment 2c#
255                               0.6936569522 1.3286454 Experiment 2c#
256                                         NA 0.8906604 Experiment 2c#
257                                  -1.1308618899 1.4185035 Experiment 2c#
258                                             NA 0.8246392 Experiment 2c#
259                                  0.8812681442 1.3801722 Experiment 2c#
260                                            NA 0.9080047 Experiment 2c#
261                                0.8343653462 1.3740320 Experiment 2c#
262                                          NA 1.2191307 Experiment 2c#
263                                  -1.2762605637 1.3693272 Experiment 2c#
264                                             NA 1.6520944 Experiment 2c#
265                              -1.2762605637 1.4048781 Experiment 2c#
266                                         NA 1.0135652 Experiment 2c#
267                                  1.4441017202 1.3312788 Experiment 2c#
268                                            NA 2.7181330 Experiment 2c#
269                                -0.5258157957 1.3233230 Experiment 2c#
270                                           NA 2.5308546 Experiment 2c#
271                                -1.3935175587 1.4604925 Experiment 2c#
272                                           NA 2.7457075 Experiment 2c#
273                          -1.0417465737 1.3589802 Experiment 2c#
274                                     NA 1.2186642 Experiment 2c#
275                               -0.2866115259 1.4952540 Experiment 2c#
276                                          NA 1.5328557 Experiment 2c#
277                                  1.5848101142 1.4058825 Experiment 2c#
278                                            NA 1.0231050 Experiment 2c#
279                                  0.5763999572 1.4254767 Experiment 2c#
280                                            NA 1.2780676 Experiment 2c#
281                                 -1.0886493717 1.4362079 Experiment 2c#
282                                            NA 2.5030891 Experiment 2c#
283                                 -1.1355521697 1.5742292 Experiment 2c#
284                                            NA 1.0266153 Experiment 2c#
285                               -0.6665241897 1.3479210 Experiment 2c#
286                                          NA 0.9009790 Experiment 2c#
287                                -0.4320101997 1.5379088 Experiment 2c#
288                                           NA 1.0439514 Experiment 2c#
289                                0.3887887652 1.3910868 Experiment 2c#
290                                          NA 1.1962825 Experiment 2c#
291                                 0.6936569522 1.4534588 Experiment 2c#
292                                           NA 1.3216723 Experiment 2c#
293                                 0.6702055532 1.3758442 Experiment 2c#
294                                           NA 1.1602166 Experiment 2c#
295                              -1.6749343467 1.4055842 Experiment 2c#
296                                         NA 0.8021575 Experiment 2c#
297                                    0.9281709422 1.6058611 Experiment 2c#
298                                              NA 1.2189221 Experiment 2c#
299                               1.2095877302 1.4667876 Experiment 2c#
300                                         NA 1.5006232 Experiment 2c#
301                                 -0.4789129977 1.4519270 Experiment 2c#
302                                            NA 1.4274185 Experiment 2c#
2016-02-16 11:02:23.652 R[54157:16109823] !!! _NSLayoutTreeSetOutsideDrawsUponLineFragmentAtGlyphIndex invalid glyph index 81365#
2016-02-16 11:02:28.806 R[54157:16109823] !!! _NSLayoutTreeSetOutsideDrawsUponLineFragmentAtGlyphIndex invalid glyph index 80930#
2016-02-16 11:02:28.851 R[54157:16109823] !!! _NSLayoutTreeSetOutsideDrawsUponLineFragmentAtGlyphIndex invalid glyph index 80913#
> #
> #
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> graph <- na.omit(graph)#
> #
> summary(graph)#
                 prime     MeanAffectivity           rt         Experiment       #
 Andy cut the paper :  1   Min.   :-1.822476   Min.   :1.283   Length:151        #
 apartment building :  1   1st Qu.:-0.941369   1st Qu.:1.405   Class :character  #
 Baby chased the dog:  1   Median : 0.138585   Median :1.475   Mode  :character  #
 backpack           :  1   Mean   : 0.000318   Mean   :1.480                     #
 bad breath         :  1   3rd Qu.: 0.832537   3rd Qu.:1.543                     #
 black eye          :  1   Max.   : 2.383616   Max.   :1.748                     #
 (Other)            :145                                                         #
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c")#
+ #
+ graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
Error: unexpected symbol in:#
"#
graph"#
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> summary(graph)#
                 prime     MeanAffectivity           rt                Experiment#
 Andy cut the paper :  1   Min.   :-1.822476   Min.   :1.283   Experiment 2a:50  #
 apartment building :  1   1st Qu.:-0.941369   1st Qu.:1.405   Experiment 2b:56  #
 Baby chased the dog:  1   Median : 0.138585   Median :1.475   Experiment 2c:45  #
 backpack           :  1   Mean   : 0.000318   Mean   :1.480                     #
 bad breath         :  1   3rd Qu.: 0.832537   3rd Qu.:1.543                     #
 black eye          :  1   Max.   : 2.383616   Max.   :1.748                     #
 (Other)            :145                                                         #
> ggplot(graph, aes(x=MeanAffectivity, y=rt)) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment)#
> ggplot(graph, aes(x=MeanAffectivity, y=rt), ylab = "Reaction Time (ms)", xlab = "Standardized Valence Rating") +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment)#
> ggplot(graph, aes(x=MeanAffectivity, y=rt, ylab = "Reaction Time (ms)", xlab = "Standardized Valence Rating")) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment)#
> ggplot(graph, aes(x=MeanAffectivity, y=rt)) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> #
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt)) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> summary(emo.pop)#
       X           Unnamed..0                     Condition       as.factor(Contrast)          List   MeanAffectivity       SubjNo      Unnamed..16    Unnamed..17    correct.response               date               experiment_name    frameRate     given.response     match.      #
 Min.   :    0   Min.   :  0.00   New                  :5174   Min.   :50.00   Min.   :1   Min.   :-1.6462   Min.   : 1.00   Mode:logical   Mode:logical   down:6998        2015_Dec_02_1223:  196   CFS popping time:14027   Min.   :59.98   down:7120      Min.   :0.0000  #
 1st Qu.: 3548   1st Qu.: 49.00   Sklar                :2419   1st Qu.:50.00   1st Qu.:1   1st Qu.:-0.9522   1st Qu.:21.00   NA's:14027     NA's:14027     up  :7029        2015_Dec_02_1300:  196                            1st Qu.:60.01   None:   0      1st Qu.:1.0000  #
 Median : 7086   Median : 98.00   Sklar_Hebrew_negative:3293   Median :50.00   Median :1   Median :-0.0492   Median :39.00                                                  2016_Feb_01_0919:  196                            Median :60.03   up  :6907      Median :1.0000  #
 Mean   : 7120   Mean   : 97.85   Sklar_Hebrew_neutral :3141   Mean   :56.92   Mean   :1   Mean   : 0.0000   Mean   :38.81                                                  2016_Feb_01_0953:  196                            Mean   :68.86                  Mean   :0.9649  #
 3rd Qu.:10676   3rd Qu.:147.00                                3rd Qu.:50.00   3rd Qu.:1   3rd Qu.: 0.8548   3rd Qu.:57.00                                                  2016_Feb_01_1021:  196                            3rd Qu.:85.01                  3rd Qu.:1.0000  #
 Max.   :14307   Max.   :195.00                                Max.   :80.00   Max.   :1   Max.   : 2.2653   Max.   :75.00                                                  2016_Feb_01_1113:  196                            Max.   :85.10                  Max.   :1.0000  #
                                                                                           NA's   :566                                                                      (Other)         :12851                                                                           #
 participant    perceptual.rating         prime                prime_semantics       rt            session      PairID           Length          Lang          #
 Mode:logical   Min.   :0.000       :  145   Hebrew           :6434   Min.   :0.3296   Min.   :1   Min.   :   1.0   Min.   : 6.00   Length:14027      #
 NA's:14027     1st Qu.:1.000         :  145   Negative phrase  :1702   1st Qu.:0.9500   1st Qu.:1   1st Qu.:  15.0   1st Qu.: 9.00   Class :character  #
                Median :2.000       :  145   Negative sentence:2015   Median :1.2337   Median :1   Median :  28.0   Median :11.00   Mode  :character  #
                Mean   :1.749      :  145   Neutral phrase   :1855   Mean   :1.5666   Mean   :1   Mean   : 488.3   Mean   :14.31                     #
                3rd Qu.:2.000      :  145   Neutral sentence :2021   3rd Qu.:1.7379   3rd Qu.:1   3rd Qu.:1023.0   3rd Qu.:18.00                     #
                Max.   :3.000        :  145                            Max.   :7.9982   Max.   :1   Max.   :1051.0   Max.   :36.00                     #
                                  (Other)    :13157                                                         NA's   :6434                                       #
> ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
Error in eval(expr, envir, enclos) : object 'as.factor(Contrast)' not found#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"))#
> #
> #
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"))#
> #
> #
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt, color = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50%","80%"))#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt, color = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
Error in seq.default(h[1], h[2], length.out = n) : #
  'to' cannot be NA, NaN or infinite#
> ?ordered#
starting httpd help server ... done#
> #
> #
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> #
> #
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt, fill = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt, color = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt, color = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(alpha = as.factor(Contrast)))#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt)) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(alpha = as.factor(Contrast)))#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt, alpha = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$as.factor(Contrast) <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$as.factor(Contrast) <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt, alpha = as.factor(Contrast))) +#
    geom_point(shape=1) +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> #
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(alpha = as.factor(Contrast)))#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point()#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = as.factor(Contrast)), pgh = 21, color = "black") +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point()#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = as.factor(Contrast)), pgh = 21, color = "black") +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") #
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = as.factor(Contrast)), pch = 21, color = "black") +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = as.factor(Contrast)), pch = 21, color = "black") +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point()#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = as.factor(Contrast)), pch = 21, color = "black") +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(shape = as.factor(Contrast))))#
Error: unexpected ')' in:#
"    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(shape = as.factor(Contrast))))"#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,alpha = as.factor(Contrast)), pch = 21, color = "black") +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(shape = as.factor(Contrast)))#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = as.factor(Contrast)), pch = 21, color = "black") +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") + geom_point(aes(shape = as.factor(Contrast)))#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = as.factor(Contrast)), pch = 21, color = "black") +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating") #
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = as.factor(Contrast)), pch = 21, color = "white") +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(shape = as.factor(Contrast)))#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(shape = as.factor(Contrast)))#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(size = as.factor(Contrast)))#
> shape#
> #
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(shape = as.factor(Contrast)))#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast)), size = 1) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(shape = as.factor(Contrast)))#
> #
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast)), size = 1) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast), size = 1)) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast), size = 0.1)) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast), shape = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(shape = as.factor(Contrast)))#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast)))#
Error: stat_smooth requires the following missing aesthetics: x, y#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast)))#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,color = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(x=MeanAffectivity, y=rt,shape = as.factor(Contrast)))#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = as.factor(Contrast))) +#
+     geom_point(shape=1) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(x=MeanAffectivity, y=rt,shape = as.factor(Contrast)))#
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$as.factor(Contrast) <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$as.factor(Contrast) <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt,shape = as.factor(Contrast))) +#
        # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")+ geom_point(aes(x=MeanAffectivity, y=rt,shape = as.factor(Contrast)))#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = as.factor(Contrast))) +#
+     geom_point() +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = as.factor(Contrast))) +#
+     geom_point(color = as.factor(Contrast)) +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
Error in do.call("layer", list(mapping = mapping, data = data, stat = stat,  : #
  object 'as.factor(Contrast)' not found#
> ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = as.factor(Contrast))) +#
+     geom_point() +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> library(plyr)#
> library(lme4)#
> library(doBy)#
> #
> emo.pop <- read.csv("all_data_with_pairIDs.csv")#
> #
> # Make RTs numeric#
> emo.pop <- subset(emo.pop, rt != "None")#
> #
> emo.pop$rt <- as.numeric(as.character(emo.pop$rt))#
> emo.pop$Length <- nchar(as.character(emo.pop$prime),allowNA = T)#
> #
> # First standardize the MeanAffectivity score#
> emo.pop$MeanAffectivity <- (emo.pop$MeanAffectivity - mean(emo.pop$MeanAffectivity, na.rm = T))/sd(emo.pop$MeanAffectivity, na.rm = T)#
> #
> ###########################################################################################################################
> ##
> # Sklar Experiment first#
> emo.pop.sklar <- subset(emo.pop, prime_semantics %in% c("Negative phrase","Neutral phrase"))#
> #
> # Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
> Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.sklar), keep.names = T)#
> emo.pop.sklar <- subset(emo.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
> emo.pop.sklar <- subset(emo.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
> #
> # Remove incorrect trials#
> emo.pop.sklar <- subset(emo.pop.sklar, match. == 1)#
> #
> emo.pop.sklar <- ddply(emo.pop.sklar, .(SubjNo), function(d){ #
+ 	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
+ 	d = subset(d, rt > include[1] & rt < include[2])#
+ 	})#
> #
> # Remove RTs < 200ms#
> emo.pop.sklar <- subset(emo.pop.sklar, rt > 0.2)#
> #
> # First standardize the MeanAffectivity score#
> emo.pop.sklar$MeanAffectivity <- (emo.pop.sklar$MeanAffectivity - mean(emo.pop.sklar$MeanAffectivity, na.rm = T))/sd(emo.pop.sklar$MeanAffectivity, na.rm = T)#
> #
> #Lin Reg (sklar style)#
> emo.pop.sklar.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.sklar, keep.names = T)#
> summary(lm(rt ~ MeanAffectivity, data = emo.pop.sklar.sum))#
#
Call:#
lm(formula = rt ~ MeanAffectivity, data = emo.pop.sklar.sum)#
#
Residuals:#
     Min       1Q   Median       3Q      Max #
-0.15028 -0.06408 -0.00141  0.06206  0.20184 #
#
Coefficients:#
                Estimate Std. Error t value Pr(>|t|)    #
(Intercept)      1.53466    0.01241  123.63   <2e-16 ***#
MeanAffectivity -0.01880    0.01246   -1.51    0.138    #
---#
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1#
#
Residual standard error: 0.08778 on 48 degrees of freedom#
  (50 observations deleted due to missingness)#
Multiple R-squared:  0.04533,	Adjusted R-squared:  0.02544 #
F-statistic: 2.279 on 1 and 48 DF,  p-value: 0.1377#
#
> #
> # note that you can calculate BF by estimating the sample SE from Sklar's #
> # regression coefficient, his t stat, and his sample size (46) #
> #
> # lmer (rabag style)#
> print(summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase")))))#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) + (1 |      prime)#
   Data: subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase",      "Neutral phrase"))#
#
REML criterion at convergence: 6959.7#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.0565 -0.4742 -0.1476  0.2166  5.7386 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr#
 SubjNo   (Intercept)     0.3687658 0.60726      #
          MeanAffectivity 0.0003006 0.01734  1.00#
 prime    (Intercept)     0.0000000 0.00000      #
 Residual                 0.4922651 0.70162      #
Number of obs: 3152, groups:  SubjNo, 67; prime, 50#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)      1.55576    0.07524   20.68#
MeanAffectivity -0.02169    0.01268   -1.71#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty 0.165 #
> #
> #
> ###########################################################################################################################
> ##
> # New reversed sentences Experiment next #
> emo.pop.new <- subset(emo.pop, prime_semantics %in% c("Negative sentence","Neutral sentence"))#
> #
> # Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
> Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.new), keep.names = T)#
> emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
> emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
> #
> # Remove incorrect trials#
> emo.pop.new <- subset(emo.pop.new, match. == 1)#
> #
> emo.pop.new <- ddply(emo.pop.new, .(SubjNo), function(d){ #
+ 	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
+ 	d = subset(d, rt > include[1] & rt < include[2])#
+ 	})#
> #
> # Remove RTs < 200ms#
> emo.pop.new <- subset(emo.pop.new, rt > 0.2)#
> #
> #  standardize the MeanAffectivity score#
> emo.pop.new$MeanAffectivity <- (emo.pop.new$MeanAffectivity - mean(emo.pop.new$MeanAffectivity, na.rm = T))/sd(emo.pop.new$MeanAffectivity, na.rm = T)#
> #
> emo.pop.new.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.new, keep.names = T)#
> summary(lm(rt ~ MeanAffectivity, data = emo.pop.new.sum))#
#
Call:#
lm(formula = rt ~ MeanAffectivity, data = emo.pop.new.sum)#
#
Residuals:#
      Min        1Q    Median        3Q       Max #
-0.186134 -0.058908 -0.007124  0.050571  0.261751 #
#
Coefficients:#
                 Estimate Std. Error t value Pr(>|t|)    #
(Intercept)      1.471727   0.012497 117.768   <2e-16 ***#
MeanAffectivity -0.006337   0.012533  -0.506    0.615    #
---#
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1#
#
Residual standard error: 0.09352 on 54 degrees of freedom#
  (56 observations deleted due to missingness)#
Multiple R-squared:  0.004712,	Adjusted R-squared:  -0.01372 #
F-statistic: 0.2556 on 1 and 54 DF,  p-value: 0.6152#
#
> #
> # lmer (rabag style)#
> print(summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence")))))#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) + (1 |      prime)#
   Data: subset(emo.pop.new, prime_semantics %in% c("Negative sentence",      "Neutral sentence"))#
#
REML criterion at convergence: 7380.4#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.1456 -0.4348 -0.1198  0.2207  7.6712 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr #
 SubjNo   (Intercept)     0.3227071 0.56807       #
          MeanAffectivity 0.0004233 0.02057  -1.00#
 prime    (Intercept)     0.0009108 0.03018       #
 Residual                 0.4553041 0.67476       #
Number of obs: 3475, groups:  SubjNo, 65; prime, 56#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)      1.48004    0.07150  20.700#
MeanAffectivity -0.00632    0.01241  -0.509#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty -0.203#
> #
> # lmer t-test equivalent (rabag style)#
> print(summary(lmer(rt ~ prime_semantics + (1+prime_semantics|SubjNo)+ (1+prime_semantics|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence")))))#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ prime_semantics + (1 + prime_semantics | SubjNo) + (1 +      prime_semantics | PairID)#
   Data: subset(emo.pop.new, prime_semantics %in% c("Negative sentence",      "Neutral sentence"))#
#
REML criterion at convergence: 7818.5#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-2.9801 -0.4247 -0.1192  0.2195  7.4547 #
#
Random effects:#
 Groups   Name                            Variance  Std.Dev. Corr#
 SubjNo   (Intercept)                     0.3195960 0.56533      #
          prime_semanticsNeutral sentence 0.0004149 0.02037  1.00#
 PairID   (Intercept)                     0.0003081 0.01755      #
          prime_semanticsNeutral sentence 0.0016776 0.04096  1.00#
 Residual                                 0.4671443 0.68348      #
Number of obs: 3636, groups:  SubjNo, 68; PairID, 28#
#
Fixed effects:#
                                Estimate Std. Error t value#
(Intercept)                      1.46997    0.07048  20.856#
prime_semanticsNeutral sentence  0.01198    0.02409   0.497#
#
Correlation of Fixed Effects:#
            (Intr)#
prm_smntcNs -0.036#
> #
> #
> ############################################################################################################################
> ##
> # Finally, Hebrew Experiment#
> emo.pop.hebr <- subset(emo.pop, prime_semantics %in% c("Hebrew"))#
> #
> # Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
> Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.hebr), keep.names = T)#
> emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
> emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
> #
> # Remove incorrect trials#
> emo.pop.hebr <- subset(emo.pop.hebr, match. == 1)#
> #
> emo.pop.hebr <- ddply(emo.pop.hebr, .(SubjNo), function(d){ #
+ 	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
+ 	d = subset(d, rt > include[1] & rt < include[2])#
+ 	})#
> #
> # Remove RTs < 200ms#
> emo.pop.hebr <- subset(emo.pop.hebr, rt > 0.2)#
> #
> # Can't do a by-items analysis until Hebrew is properly recoded.#
> #  standardize the MeanAffectivity score#
> emo.pop.hebr$MeanAffectivity <- (emo.pop.hebr$MeanAffectivity - mean(emo.pop.hebr$MeanAffectivity, na.rm = T))/sd(emo.pop.hebr$MeanAffectivity, na.rm = T)#
> #
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.hebr, keep.names = T)#
> summary(lm(rt ~ MeanAffectivity, data = emo.pop.hebr.sum))#
#
Call:#
lm(formula = rt ~ MeanAffectivity, data = emo.pop.hebr.sum)#
#
Residuals:#
     Min       1Q   Median       3Q      Max #
-0.10443 -0.05780 -0.02244  0.03802  0.17725 #
#
Coefficients:#
                 Estimate Std. Error t value Pr(>|t|)    #
(Intercept)     1.4280649  0.0117874  121.15   <2e-16 ***#
MeanAffectivity 0.0005854  0.0117883    0.05    0.961    #
---#
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1#
#
Residual standard error: 0.07907 on 43 degrees of freedom#
  (45 observations deleted due to missingness)#
Multiple R-squared:  5.735e-05,	Adjusted R-squared:  -0.0232 #
F-statistic: 0.002466 on 1 and 43 DF,  p-value: 0.9606#
#
> #
> #
> #
> # lmer (rabag style)#
> print(summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew")))))#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) + (1 |      prime)#
   Data: subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))#
#
REML criterion at convergence: 11514#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-2.4822 -0.5082 -0.1552  0.2330  7.6253 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr #
 SubjNo   (Intercept)     0.2344167 0.48417       #
          MeanAffectivity 0.0005944 0.02438  -0.06#
 prime    (Intercept)     0.0020072 0.04480       #
 Residual                 0.4412002 0.66423       #
Number of obs: 5560, groups:  SubjNo, 65; prime, 45#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)     1.434724   0.061078  23.490#
MeanAffectivity 0.002748   0.011541   0.238#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty -0.015#
> print(summary(lmer(rt ~ as.factor(as.factor(Contrast)) + (1+as.factor(as.factor(Contrast))|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew")))))#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ as.factor(as.factor(Contrast)) + (1 + as.factor(as.factor(Contrast)) | SubjNo) +      (1 | prime)#
   Data: subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))#
#
REML criterion at convergence: 12105#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-2.4664 -0.4611 -0.1503  0.2224  7.9092 #
#
Random effects:#
 Groups   Name                  Variance Std.Dev. Corr #
 SubjNo   (Intercept)           0.266673 0.51640       #
          as.factor(as.factor(Contrast))80 0.009438 0.09715  -0.36#
 prime    (Intercept)           0.002206 0.04697       #
 Residual                       0.446862 0.66848       #
Number of obs: 5800, groups:  SubjNo, 68; prime, 45#
#
Fixed effects:#
                      Estimate Std. Error t value#
(Intercept)            1.52781    0.06424   23.78#
as.factor(as.factor(Contrast))80 -0.18448    0.02116   -8.72#
#
Correlation of Fixed Effects:#
            (Intr)#
as.fct(C)80 -0.313#
> #
> ###########################################################################################################################
> #
> #
> #
> # We can also check if this depends on perceptual rating#
> #
> #
> # Finally -- a quick test if English is perceived faster than Hebrew (following Jiang et al 07, #
> # tho note that there are length confounds here#
> emo.pop$Lang <- "English"#
> emo.pop[emo.pop$prime_semantics %in% c("Hebrew"),]$Lang <- "Hebrew"#
> summary(lmer(rt ~ Lang + (1+Lang|SubjNo), data = subset(emo.pop, as.factor(Contrast) == 50)))#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ Lang + (1 + Lang | SubjNo)#
   Data: subset(emo.pop, as.factor(Contrast) == 50)#
#
REML criterion at convergence: 27916.8#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-2.9035 -0.4661 -0.1591  0.1598  7.2967 #
#
Random effects:#
 Groups   Name        Variance Std.Dev. Corr #
 SubjNo   (Intercept) 0.38359  0.6194        #
          LangHebrew  0.02607  0.1615   -0.08#
 Residual             0.75151  0.8669        #
Number of obs: 10791, groups:  SubjNo, 73#
#
Fixed effects:#
            Estimate Std. Error t value#
(Intercept)  1.59377    0.07317  21.782#
LangHebrew   0.09718    0.02631   3.694#
#
Correlation of Fixed Effects:#
           (Intr)#
LangHebrew -0.110#
> #
> ###########################################################################################################################
> ##
> # Graphs#
> #
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + as.factor(Contrast), data = emo.pop.hebr, keep.names = T)#
> emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
> #
> emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
> emo.pop.sklar.sum$as.factor(Contrast) <- 50#
> emo.pop.new.sum$Experiment <- "Experiment 2b"#
> emo.pop.new.sum$as.factor(Contrast) <- 50#
> #
> graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
> #
> graph <- na.omit(graph)#
> graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
> graph$as.factor(Contrast) <- ordered(graph$as.factor(Contrast), levels = c("50","80"), labels = c("50%","80%"))#
> #
> #
> graph$rt <- graph$rt * 1000#
> #
> ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = as.factor(Contrast))) +#
+     geom_point() +    # Use hollow circles#
+     geom_smooth(method=lm,   # Add linear regression line#
+                 se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")#
> , #
> #
> #
> #Lin Reg (sklar style)#
> emo.pop.sklar.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.sklar, keep.names = T)#
> summary(lm(rt ~ MeanAffectivity, data = emo.pop.sklar.sum))#
#
Call:#
lm(formula = rt ~ MeanAffectivity, data = emo.pop.sklar.sum)#
#
Residuals:#
     Min       1Q   Median       3Q      Max #
-0.15028 -0.06408 -0.00141  0.06206  0.20184 #
#
Coefficients:#
                Estimate Std. Error t value Pr(>|t|)    #
(Intercept)      1.53466    0.01241  123.63   <2e-16 ***#
MeanAffectivity -0.01880    0.01246   -1.51    0.138    #
---#
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1#
#
Residual standard error: 0.08778 on 48 degrees of freedom#
  (50 observations deleted due to missingness)#
Multiple R-squared:  0.04533,	Adjusted R-squared:  0.02544 #
F-statistic: 2.279 on 1 and 48 DF,  p-value: 0.1377#
#
> summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.sklar.sum))#
#
Call:#
lm(formula = log(rt) ~ MeanAffectivity, data = emo.pop.sklar.sum)#
#
Residuals:#
      Min        1Q    Median        3Q       Max #
-0.102205 -0.040880  0.000556  0.042093  0.124169 #
#
Coefficients:#
                 Estimate Std. Error t value Pr(>|t|)    #
(Intercept)      0.426660   0.008107  52.630   <2e-16 ***#
MeanAffectivity -0.012548   0.008135  -1.543     0.13    #
---#
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1#
#
Residual standard error: 0.05732 on 48 degrees of freedom#
  (50 observations deleted due to missingness)#
Multiple R-squared:  0.04723,	Adjusted R-squared:  0.02738 #
F-statistic: 2.379 on 1 and 48 DF,  p-value: 0.1295#
#
> #
> # note that you can calculate BF by estimating the sample SE from Sklar's #
> # regression coefficient, his t stat, and his sample size (46) #
> #
> # lmer (rabag style)#
> print(summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase")))))#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) + (1 |      prime)#
   Data: subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase",      "Neutral phrase"))#
#
REML criterion at convergence: 6959.7#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.0565 -0.4742 -0.1476  0.2166  5.7386 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr#
 SubjNo   (Intercept)     0.3687658 0.60726      #
          MeanAffectivity 0.0003006 0.01734  1.00#
 prime    (Intercept)     0.0000000 0.00000      #
 Residual                 0.4922651 0.70162      #
Number of obs: 3152, groups:  SubjNo, 67; prime, 50#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)      1.55576    0.07524   20.68#
MeanAffectivity -0.02169    0.01268   -1.71#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty 0.165 #
> print(summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase")))))#
Linear mixed model fit by REML ['lmerMod']#
Formula: log(rt) ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) +      (1 | prime)#
   Data: subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase",      "Neutral phrase"))#
#
REML criterion at convergence: 2291.1#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.4970 -0.6100 -0.1226  0.4704  4.7420 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr#
 SubjNo   (Intercept)     1.067e-01 0.32658      #
          MeanAffectivity 1.806e-05 0.00425  1.00#
 prime    (Intercept)     2.496e-04 0.01580      #
 Residual                 1.110e-01 0.33322      #
Number of obs: 3152, groups:  SubjNo, 67; prime, 50#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)      0.31604    0.04040   7.822#
MeanAffectivity -0.01159    0.00637  -1.819#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty 0.080 #
> #
> #
> 2*pnorm(-abs(1.71))#
[1] 0.08726587#
> emo.sklar.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
> print(emo.sklar.lmer.raw)#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) + (1 |      prime)#
   Data: subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase",      "Neutral phrase"))#
#
REML criterion at convergence: 6959.7#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.0565 -0.4742 -0.1476  0.2166  5.7386 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr#
 SubjNo   (Intercept)     0.3687658 0.60726      #
          MeanAffectivity 0.0003006 0.01734  1.00#
 prime    (Intercept)     0.0000000 0.00000      #
 Residual                 0.4922651 0.70162      #
Number of obs: 3152, groups:  SubjNo, 67; prime, 50#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)      1.55576    0.07524   20.68#
MeanAffectivity -0.02169    0.01268   -1.71#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty 0.165 #
> #
> coef(emo.sklar.lmer.raw)#
                   Estimate Std. Error   t value#
(Intercept)      1.55575965 0.07524156 20.676865#
MeanAffectivity -0.02169112 0.01268319 -1.710227#
> coef(emo.sklar.lmer.raw)$t#
Error in coef(emo.sklar.lmer.raw)$t : #
  $ operator is invalid for atomic vectors#
> coef(emo.sklar.lmer.raw)@t#
Error: trying to get slot "t" from an object of a basic class ("matrix") with no slots#
> coef(emo.sklar.lmer.raw)[3,1]#
Error in coef(emo.sklar.lmer.raw)[3, 1] : subscript out of bounds#
> coef(emo.sklar.lmer.raw)[1,3]#
[1] 20.67686#
> coef(emo.sklar.lmer.raw)[2,3]#
[1] -1.710227#
> #
> *pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))#
Error: unexpected '*' in "*"#
> 2*pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))#
[1] 0.08722398#
> emo.sklar.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
> print(emo.sklar.lmer.raw)#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) + (1 |      prime)#
   Data: subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase",      "Neutral phrase"))#
#
REML criterion at convergence: 6959.7#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.0565 -0.4742 -0.1476  0.2166  5.7386 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr#
 SubjNo   (Intercept)     0.3687658 0.60726      #
          MeanAffectivity 0.0003006 0.01734  1.00#
 prime    (Intercept)     0.0000000 0.00000      #
 Residual                 0.4922651 0.70162      #
Number of obs: 3152, groups:  SubjNo, 67; prime, 50#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)      1.55576    0.07524   20.68#
MeanAffectivity -0.02169    0.01268   -1.71#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty 0.165 #
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))#
+ emo.sklar.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
Error: unexpected symbol in:#
"print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))#
emo.sklar.lmer.log"#
> print(emo.sklar.lmer.log)#
Error in print(emo.sklar.lmer.log) : #
  error in evaluating the argument 'x' in selecting a method for function 'print': Error: object 'emo.sklar.lmer.log' not found#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.log)[2,3]))#
+ #
+ emo.sklar.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
Error: unexpected symbol in:#
"#
emo.sklar.lmer.raw"#
> print(emo.sklar.lmer.raw)#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) + (1 |      prime)#
   Data: subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase",      "Neutral phrase"))#
#
REML criterion at convergence: 6959.7#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.0565 -0.4742 -0.1476  0.2166  5.7386 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr#
 SubjNo   (Intercept)     0.3687658 0.60726      #
          MeanAffectivity 0.0003006 0.01734  1.00#
 prime    (Intercept)     0.0000000 0.00000      #
 Residual                 0.4922651 0.70162      #
Number of obs: 3152, groups:  SubjNo, 67; prime, 50#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)      1.55576    0.07524   20.68#
MeanAffectivity -0.02169    0.01268   -1.71#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty 0.165 #
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))))#
[1] "p value =  0.0872239847661784"#
> emo.sklar.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
> print(emo.sklar.lmer.log)#
Linear mixed model fit by REML ['lmerMod']#
Formula: log(rt) ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) +      (1 | prime)#
   Data: subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase",      "Neutral phrase"))#
#
REML criterion at convergence: 2291.1#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.4970 -0.6100 -0.1226  0.4704  4.7420 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr#
 SubjNo   (Intercept)     1.067e-01 0.32658      #
          MeanAffectivity 1.806e-05 0.00425  1.00#
 prime    (Intercept)     2.496e-04 0.01580      #
 Residual                 1.110e-01 0.33322      #
Number of obs: 3152, groups:  SubjNo, 67; prime, 50#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)      0.31604    0.04040   7.822#
MeanAffectivity -0.01159    0.00637  -1.819#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty 0.080 #
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.log)[2,3]))))#
[1] "p value =  0.0689080448198917"#
> #
> emo.sklar.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
> print(emo.sklar.lmer.raw)#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) + (1 |      prime)#
   Data: subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase",      "Neutral phrase"))#
#
REML criterion at convergence: 6959.7#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.0565 -0.4742 -0.1476  0.2166  5.7386 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr#
 SubjNo   (Intercept)     0.3687658 0.60726      #
          MeanAffectivity 0.0003006 0.01734  1.00#
 prime    (Intercept)     0.0000000 0.00000      #
 Residual                 0.4922651 0.70162      #
Number of obs: 3152, groups:  SubjNo, 67; prime, 50#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)      1.55576    0.07524   20.68#
MeanAffectivity -0.02169    0.01268   -1.71#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty 0.165 #
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))))#
[1] "p value =  0.0872239847661784"#
> emo.sklar.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
> print(emo.sklar.lmer.log)#
Linear mixed model fit by REML ['lmerMod']#
Formula: log(rt) ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) +      (1 | prime)#
   Data: subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase",      "Neutral phrase"))#
#
REML criterion at convergence: 2291.1#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.4970 -0.6100 -0.1226  0.4704  4.7420 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr#
 SubjNo   (Intercept)     1.067e-01 0.32658      #
          MeanAffectivity 1.806e-05 0.00425  1.00#
 prime    (Intercept)     2.496e-04 0.01580      #
 Residual                 1.110e-01 0.33322      #
Number of obs: 3152, groups:  SubjNo, 67; prime, 50#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)      0.31604    0.04040   7.822#
MeanAffectivity -0.01159    0.00637  -1.819#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty 0.080 #
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.log)[2,3]))))#
[1] "p value =  0.0689080448198917"#
> emo.new.lmer.raw <- summary(lmer(rt ~ prime_semantics + (1+prime_semantics|SubjNo)+ (1+prime_semantics|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
> print(emo.new.lmer.raw)#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ prime_semantics + (1 + prime_semantics | SubjNo) + (1 +      prime_semantics | PairID)#
   Data: subset(emo.pop.new, prime_semantics %in% c("Negative sentence",      "Neutral sentence"))#
#
REML criterion at convergence: 7818.5#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-2.9801 -0.4247 -0.1192  0.2195  7.4547 #
#
Random effects:#
 Groups   Name                            Variance  Std.Dev. Corr#
 SubjNo   (Intercept)                     0.3195960 0.56533      #
          prime_semanticsNeutral sentence 0.0004149 0.02037  1.00#
 PairID   (Intercept)                     0.0003081 0.01755      #
          prime_semanticsNeutral sentence 0.0016776 0.04096  1.00#
 Residual                                 0.4671443 0.68348      #
Number of obs: 3636, groups:  SubjNo, 68; PairID, 28#
#
Fixed effects:#
                                Estimate Std. Error t value#
(Intercept)                      1.46997    0.07048  20.856#
prime_semanticsNeutral sentence  0.01198    0.02409   0.497#
#
Correlation of Fixed Effects:#
            (Intr)#
prm_smntcNs -0.036#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.raw)[2,3]))))#
[1] "p value =  0.619075940654334"#
> emo.new.lmer.log <- summary(lmer(log(rt) ~ prime_semantics + (1+prime_semantics|SubjNo)+ (1+prime_semantics|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
Warning messages:#
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :#
  unable to evaluate scaled gradient#
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :#
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues#
> print(emo.new.lmer.log)#
Linear mixed model fit by REML ['lmerMod']#
Formula: log(rt) ~ prime_semantics + (1 + prime_semantics | SubjNo) +      (1 + prime_semantics | PairID)#
   Data: subset(emo.pop.new, prime_semantics %in% c("Negative sentence",      "Neutral sentence"))#
#
REML criterion at convergence: 2391#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-4.2441 -0.5959 -0.1039  0.4744  5.3332 #
#
Random effects:#
 Groups   Name                            Variance  Std.Dev.  Corr#
 SubjNo   (Intercept)                     1.082e-01 0.3290016     #
          prime_semanticsNeutral sentence 7.554e-08 0.0002748 1.00#
 PairID   (Intercept)                     0.000e+00 0.0000000     #
          prime_semanticsNeutral sentence 8.237e-04 0.0286995  NaN#
 Residual                                 1.042e-01 0.3227893     #
Number of obs: 3636, groups:  SubjNo, 68; PairID, 28#
#
Fixed effects:#
                                Estimate Std. Error t value#
(Intercept)                     0.264984   0.040609   6.525#
prime_semanticsNeutral sentence 0.001413   0.012006   0.118#
#
Correlation of Fixed Effects:#
            (Intr)#
prm_smntcNs -0.115#
convergence code: 0#
unable to evaluate scaled gradient#
Model failed to converge: degenerate  Hessian with 1 negative eigenvalues#
#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.log)[2,3]))))#
[1] "p value =  0.906307374650146"#
> #
> # lmer (rabag style)#
> emo.new.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
> print(emo.new.lmer.raw)#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) + (1 +      MeanAffectivity | PairID)#
   Data: subset(emo.pop.new, prime_semantics %in% c("Negative sentence",      "Neutral sentence"))#
#
REML criterion at convergence: 7377#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.1381 -0.4392 -0.1206  0.2233  7.6842 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr #
 SubjNo   (Intercept)     0.3227255 0.56809       #
          MeanAffectivity 0.0004225 0.02055  -1.00#
 PairID   (Intercept)     0.0009774 0.03126       #
          MeanAffectivity 0.0007617 0.02760  1.00 #
 Residual                 0.4544224 0.67411       #
Number of obs: 3475, groups:  SubjNo, 65; PairID, 28#
#
Fixed effects:#
                 Estimate Std. Error t value#
(Intercept)      1.480888   0.071673  20.662#
MeanAffectivity -0.006573   0.013840  -0.475#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty -0.139#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.raw)[2,3]))))#
[1] "p value =  0.634838298475791"#
> emo.new.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
Warning messages:#
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :#
  unable to evaluate scaled gradient#
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :#
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues#
> print(emo.new.lmer.log)#
Linear mixed model fit by REML ['lmerMod']#
Formula: log(rt) ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) +      (1 + MeanAffectivity | PairID)#
   Data: subset(emo.pop.new, prime_semantics %in% c("Negative sentence",      "Neutral sentence"))#
#
REML criterion at convergence: 2237#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-4.3467 -0.5956 -0.1054  0.4775  5.3131 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr #
 SubjNo   (Intercept)     1.045e-01 0.323323      #
          MeanAffectivity 9.232e-05 0.009608 -1.00#
 PairID   (Intercept)     0.000e+00 0.000000      #
          MeanAffectivity 9.863e-06 0.003140  NaN #
 Residual                 1.031e-01 0.321024      #
Number of obs: 3475, groups:  SubjNo, 65; PairID, 28#
#
Fixed effects:#
                 Estimate Std. Error t value#
(Intercept)      0.271853   0.040473   6.717#
MeanAffectivity -0.001287   0.005628  -0.229#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty -0.210#
convergence code: 0#
unable to evaluate scaled gradient#
Model failed to converge: degenerate  Hessian with 1 negative eigenvalues#
#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.log)[2,3]))))#
[1] "p value =  0.819092159855253"#
> # New reversed sentences Experiment next #
> emo.pop.new <- subset(emo.pop, prime_semantics %in% c("Negative sentence","Neutral sentence"))#
> #
> # Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
> Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.new), keep.names = T)#
> emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
> emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
> #
> # Remove incorrect trials#
> emo.pop.new <- subset(emo.pop.new, match. == 1)#
> #
> emo.pop.new <- ddply(emo.pop.new, .(SubjNo), function(d){ #
+ 	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
+ 	d = subset(d, rt > include[1] & rt < include[2])#
+ 	})#
> #
> # Remove RTs < 200ms#
> emo.pop.new <- subset(emo.pop.new, rt > 0.2)#
> #
> #  standardize the MeanAffectivity score#
> emo.pop.new$MeanAffectivity <- (emo.pop.new$MeanAffectivity - mean(emo.pop.new$MeanAffectivity, na.rm = T))/sd(emo.pop.new$MeanAffectivity, na.rm = T)#
> #
> emo.pop.new.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.new, keep.names = T)#
> summary(lm(rt ~ MeanAffectivity, data = emo.pop.new.sum))#
#
Call:#
lm(formula = rt ~ MeanAffectivity, data = emo.pop.new.sum)#
#
Residuals:#
      Min        1Q    Median        3Q       Max #
-0.186134 -0.058908 -0.007124  0.050571  0.261751 #
#
Coefficients:#
                 Estimate Std. Error t value Pr(>|t|)    #
(Intercept)      1.471727   0.012497 117.768   <2e-16 ***#
MeanAffectivity -0.006337   0.012533  -0.506    0.615    #
---#
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1#
#
Residual standard error: 0.09352 on 54 degrees of freedom#
  (56 observations deleted due to missingness)#
Multiple R-squared:  0.004712,	Adjusted R-squared:  -0.01372 #
F-statistic: 0.2556 on 1 and 54 DF,  p-value: 0.6152#
#
> #
> # lmer (rabag style)#
> emo.new.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
> print(emo.new.lmer.raw)#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) + (1 +      MeanAffectivity | PairID)#
   Data: subset(emo.pop.new, prime_semantics %in% c("Negative sentence",      "Neutral sentence"))#
#
REML criterion at convergence: 7377#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.1381 -0.4392 -0.1206  0.2233  7.6842 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr #
 SubjNo   (Intercept)     0.3227255 0.56809       #
          MeanAffectivity 0.0004225 0.02055  -1.00#
 PairID   (Intercept)     0.0009774 0.03126       #
          MeanAffectivity 0.0007617 0.02760  1.00 #
 Residual                 0.4544224 0.67411       #
Number of obs: 3475, groups:  SubjNo, 65; PairID, 28#
#
Fixed effects:#
                 Estimate Std. Error t value#
(Intercept)      1.480888   0.071673  20.662#
MeanAffectivity -0.006573   0.013840  -0.475#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty -0.139#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.raw)[2,3]))))#
[1] "p value =  0.634838298475791"#
> emo.new.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
Warning messages:#
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :#
  unable to evaluate scaled gradient#
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :#
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues#
> print(emo.new.lmer.log)#
Linear mixed model fit by REML ['lmerMod']#
Formula: log(rt) ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) +      (1 + MeanAffectivity | PairID)#
   Data: subset(emo.pop.new, prime_semantics %in% c("Negative sentence",      "Neutral sentence"))#
#
REML criterion at convergence: 2237#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-4.3467 -0.5956 -0.1054  0.4775  5.3131 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr #
 SubjNo   (Intercept)     1.045e-01 0.323323      #
          MeanAffectivity 9.232e-05 0.009608 -1.00#
 PairID   (Intercept)     0.000e+00 0.000000      #
          MeanAffectivity 9.863e-06 0.003140  NaN #
 Residual                 1.031e-01 0.321024      #
Number of obs: 3475, groups:  SubjNo, 65; PairID, 28#
#
Fixed effects:#
                 Estimate Std. Error t value#
(Intercept)      0.271853   0.040473   6.717#
MeanAffectivity -0.001287   0.005628  -0.229#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty -0.210#
convergence code: 0#
unable to evaluate scaled gradient#
Model failed to converge: degenerate  Hessian with 1 negative eigenvalues#
#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.log)[2,3]))))#
[1] "p value =  0.819092159855253"#
> summary(lm(rt ~ MeanAffectivity, data = emo.pop.new.sum))#
#
Call:#
lm(formula = rt ~ MeanAffectivity, data = emo.pop.new.sum)#
#
Residuals:#
      Min        1Q    Median        3Q       Max #
-0.186134 -0.058908 -0.007124  0.050571  0.261751 #
#
Coefficients:#
                 Estimate Std. Error t value Pr(>|t|)    #
(Intercept)      1.471727   0.012497 117.768   <2e-16 ***#
MeanAffectivity -0.006337   0.012533  -0.506    0.615    #
---#
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1#
#
Residual standard error: 0.09352 on 54 degrees of freedom#
  (56 observations deleted due to missingness)#
Multiple R-squared:  0.004712,	Adjusted R-squared:  -0.01372 #
F-statistic: 0.2556 on 1 and 54 DF,  p-value: 0.6152#
#
> summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.new.sum))#
#
Call:#
lm(formula = log(rt) ~ MeanAffectivity, data = emo.pop.new.sum)#
#
Residuals:#
      Min        1Q    Median        3Q       Max #
-0.133221 -0.039183 -0.002907  0.035182  0.166068 #
#
Coefficients:#
                 Estimate Std. Error t value Pr(>|t|)    #
(Intercept)      0.384487   0.008458  45.461   <2e-16 ***#
MeanAffectivity -0.005050   0.008482  -0.595    0.554    #
---#
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1#
#
Residual standard error: 0.06329 on 54 degrees of freedom#
  (56 observations deleted due to missingness)#
Multiple R-squared:  0.006521,	Adjusted R-squared:  -0.01188 #
F-statistic: 0.3544 on 1 and 54 DF,  p-value: 0.5541#
#
> #
> #
> # lmer (rabag style)#
> emo.new.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
> print(emo.new.lmer.raw)#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) + (1 +      MeanAffectivity | PairID)#
   Data: subset(emo.pop.new, prime_semantics %in% c("Negative sentence",      "Neutral sentence"))#
#
REML criterion at convergence: 7377#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.1381 -0.4392 -0.1206  0.2233  7.6842 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr #
 SubjNo   (Intercept)     0.3227255 0.56809       #
          MeanAffectivity 0.0004225 0.02055  -1.00#
 PairID   (Intercept)     0.0009774 0.03126       #
          MeanAffectivity 0.0007617 0.02760  1.00 #
 Residual                 0.4544224 0.67411       #
Number of obs: 3475, groups:  SubjNo, 65; PairID, 28#
#
Fixed effects:#
                 Estimate Std. Error t value#
(Intercept)      1.480888   0.071673  20.662#
MeanAffectivity -0.006573   0.013840  -0.475#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty -0.139#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.raw)[2,3]))))#
[1] "p value =  0.634838298475791"#
> emo.new.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
Warning messages:#
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :#
  unable to evaluate scaled gradient#
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :#
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues#
> print(emo.new.lmer.log)#
Linear mixed model fit by REML ['lmerMod']#
Formula: log(rt) ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) +      (1 + MeanAffectivity | PairID)#
   Data: subset(emo.pop.new, prime_semantics %in% c("Negative sentence",      "Neutral sentence"))#
#
REML criterion at convergence: 2237#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-4.3467 -0.5956 -0.1054  0.4775  5.3131 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr #
 SubjNo   (Intercept)     1.045e-01 0.323323      #
          MeanAffectivity 9.232e-05 0.009608 -1.00#
 PairID   (Intercept)     0.000e+00 0.000000      #
          MeanAffectivity 9.863e-06 0.003140  NaN #
 Residual                 1.031e-01 0.321024      #
Number of obs: 3475, groups:  SubjNo, 65; PairID, 28#
#
Fixed effects:#
                 Estimate Std. Error t value#
(Intercept)      0.271853   0.040473   6.717#
MeanAffectivity -0.001287   0.005628  -0.229#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty -0.210#
convergence code: 0#
unable to evaluate scaled gradient#
Model failed to converge: degenerate  Hessian with 1 negative eigenvalues#
#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.log)[2,3]))))#
[1] "p value =  0.819092159855253"#
> #
> #
> emo.pop.hebr <- subset(emo.pop, prime_semantics %in% c("Hebrew"))#
> #
> # Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
> Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.hebr), keep.names = T)#
> emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
> emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
> #
> # Remove incorrect trials#
> emo.pop.hebr <- subset(emo.pop.hebr, match. == 1)#
> #
> emo.pop.hebr <- ddply(emo.pop.hebr, .(SubjNo), function(d){ #
+ 	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
+ 	d = subset(d, rt > include[1] & rt < include[2])#
+ 	})#
> #
> # Remove RTs < 200ms#
> emo.pop.hebr <- subset(emo.pop.hebr, rt > 0.2)#
> #
> # Can't do a by-items analysis until Hebrew is properly recoded.#
> #  standardize the MeanAffectivity score#
> emo.pop.hebr$MeanAffectivity <- (emo.pop.hebr$MeanAffectivity - mean(emo.pop.hebr$MeanAffectivity, na.rm = T))/sd(emo.pop.hebr$MeanAffectivity, na.rm = T)#
> #
> emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.hebr, keep.names = T)#
> summary(lm(rt ~ MeanAffectivity, data = emo.pop.hebr.sum))#
#
Call:#
lm(formula = rt ~ MeanAffectivity, data = emo.pop.hebr.sum)#
#
Residuals:#
     Min       1Q   Median       3Q      Max #
-0.10443 -0.05780 -0.02244  0.03802  0.17725 #
#
Coefficients:#
                 Estimate Std. Error t value Pr(>|t|)    #
(Intercept)     1.4280649  0.0117874  121.15   <2e-16 ***#
MeanAffectivity 0.0005854  0.0117883    0.05    0.961    #
---#
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1#
#
Residual standard error: 0.07907 on 43 degrees of freedom#
  (45 observations deleted due to missingness)#
Multiple R-squared:  5.735e-05,	Adjusted R-squared:  -0.0232 #
F-statistic: 0.002466 on 1 and 43 DF,  p-value: 0.9606#
#
> summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.hebr.sum))#
#
Call:#
lm(formula = log(rt) ~ MeanAffectivity, data = emo.pop.hebr.sum)#
#
Residuals:#
     Min       1Q   Median       3Q      Max #
-0.07452 -0.03989 -0.01441  0.02769  0.11839 #
#
Coefficients:#
                 Estimate Std. Error t value Pr(>|t|)    #
(Intercept)     0.3548860  0.0081255  43.676   <2e-16 ***#
MeanAffectivity 0.0004126  0.0081261   0.051     0.96    #
---#
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1#
#
Residual standard error: 0.05451 on 43 degrees of freedom#
  (45 observations deleted due to missingness)#
Multiple R-squared:  5.995e-05,	Adjusted R-squared:  -0.02319 #
F-statistic: 0.002578 on 1 and 43 DF,  p-value: 0.9597#
#
> #
> #
> #
> # lmer (rabag style)#
> emo.hebr.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
> print(emo.hebr.lmer.raw)#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) + (1 |      prime)#
   Data: subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))#
#
REML criterion at convergence: 11514#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-2.4822 -0.5082 -0.1552  0.2330  7.6253 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr #
 SubjNo   (Intercept)     0.2344167 0.48417       #
          MeanAffectivity 0.0005944 0.02438  -0.06#
 prime    (Intercept)     0.0020072 0.04480       #
 Residual                 0.4412002 0.66423       #
Number of obs: 5560, groups:  SubjNo, 65; prime, 45#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)     1.434724   0.061078  23.490#
MeanAffectivity 0.002748   0.011541   0.238#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty -0.015#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.raw)[2,3]))))#
[1] "p value =  0.811814112360251"#
> emo.hebr.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
> print(emo.hebr.lmer.log)#
Linear mixed model fit by REML ['lmerMod']#
Formula: log(rt) ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) +      (1 | prime)#
   Data: subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))#
#
REML criterion at convergence: 4223.1#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.5075 -0.6554 -0.1164  0.4983  4.5337 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr #
 SubjNo   (Intercept)     0.0870254 0.29500       #
          MeanAffectivity 0.0002955 0.01719  -0.04#
 prime    (Intercept)     0.0009478 0.03079       #
 Residual                 0.1180557 0.34359       #
Number of obs: 5560, groups:  SubjNo, 65; prime, 45#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)     0.244268   0.037165   6.573#
MeanAffectivity 0.000947   0.006847   0.138#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty -0.013#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.log)[2,3]))))#
[1] "p value =  0.889988182812993"#
> #
> #
> emo.contr.lmer.raw <- summary(lmer(rt ~ as.factor(as.factor(Contrast)) + (1+as.factor(as.factor(Contrast))|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
> print(emo.contr.lmer.raw)#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ as.factor(as.factor(Contrast)) + (1 + as.factor(as.factor(Contrast)) | SubjNo) +      (1 | prime)#
   Data: subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))#
#
REML criterion at convergence: 12105#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-2.4664 -0.4611 -0.1503  0.2224  7.9092 #
#
Random effects:#
 Groups   Name                  Variance Std.Dev. Corr #
 SubjNo   (Intercept)           0.266673 0.51640       #
          as.factor(as.factor(Contrast))80 0.009438 0.09715  -0.36#
 prime    (Intercept)           0.002206 0.04697       #
 Residual                       0.446862 0.66848       #
Number of obs: 5800, groups:  SubjNo, 68; prime, 45#
#
Fixed effects:#
                      Estimate Std. Error t value#
(Intercept)            1.52781    0.06424   23.78#
as.factor(as.factor(Contrast))80 -0.18448    0.02116   -8.72#
#
Correlation of Fixed Effects:#
            (Intr)#
as.fct(C)80 -0.313#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.raw)[2,3]))))#
[1] "p value =  2.78962780287397e-18"#
> emo.contr.lmer.log <- summary(lmer(rt ~ as.factor(as.factor(Contrast)) + (1+as.factor(as.factor(Contrast))|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
> print(emo.contr.lmer.log)#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ as.factor(as.factor(Contrast)) + (1 + as.factor(as.factor(Contrast)) | SubjNo) +      (1 | prime)#
   Data: subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))#
#
REML criterion at convergence: 12105#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-2.4664 -0.4611 -0.1503  0.2224  7.9092 #
#
Random effects:#
 Groups   Name                  Variance Std.Dev. Corr #
 SubjNo   (Intercept)           0.266673 0.51640       #
          as.factor(as.factor(Contrast))80 0.009438 0.09715  -0.36#
 prime    (Intercept)           0.002206 0.04697       #
 Residual                       0.446862 0.66848       #
Number of obs: 5800, groups:  SubjNo, 68; prime, 45#
#
Fixed effects:#
                      Estimate Std. Error t value#
(Intercept)            1.52781    0.06424   23.78#
as.factor(as.factor(Contrast))80 -0.18448    0.02116   -8.72#
#
Correlation of Fixed Effects:#
            (Intr)#
as.fct(C)80 -0.313#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[2,3]))))#
[1] "p value =  2.78962780287397e-18"#
> #
> emo.hebr.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.hebr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.raw)[2,3]))))#
emo.hebr.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.hebr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.log)[2,3]))))#
> #
> #
> emo.hebr.lmer.raw <- summary(lmer(rt ~ MeanAffectivity*as.factor(Contrast) + (1+MeanAffectivity*as.factor(Contrast)|SubjNo)+ (1+as.factor(Contrast)|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
Warning messages:#
1: In optwrap(optimizer, devfun, getStart(start, rho$lower, rho$pp),  :#
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded#
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :#
  Model failed to converge with max|grad| = 7.69421 (tol = 0.002, component 1)#
3: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :#
  Model is nearly unidentifiable: very large eigenvalue#
 - Rescale variables?#
> print(emo.hebr.lmer.raw)#
Linear mixed model fit by REML ['lmerMod']#
Formula: rt ~ MeanAffectivity * as.factor(Contrast) + (1 + MeanAffectivity * as.factor(Contrast) |      SubjNo) + (1 + as.factor(Contrast) | prime)#
   Data: subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))#
#
REML criterion at convergence: 11404.4#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-2.6481 -0.4734 -0.1542  0.2285  7.3641 #
#
Random effects:#
 Groups   Name                     Variance  Std.Dev. Corr             #
 SubjNo   (Intercept)              5.228e-01 0.723015                  #
          MeanAffectivity          3.922e-02 0.198043  0.11            #
          as.factor(Contrast)                 1.741e-05 0.004173 -0.77 -0.45      #
          MeanAffectivity:as.factor(Contrast) 1.100e-05 0.003317 -0.08 -0.99  0.34#
 prime    (Intercept)              1.310e-02 0.114470                  #
          as.factor(Contrast)                 1.300e-06 0.001140 -0.97            #
 Residual                          4.247e-01 0.651680                  #
Number of obs: 5560, groups:  SubjNo, 65; prime, 45#
#
Fixed effects:#
                           Estimate Std. Error t value#
(Intercept)               1.8527830  0.0992800  18.662#
MeanAffectivity          -0.0098281  0.0491244  -0.200#
as.factor(Contrast)                 -0.0064144  0.0007982  -8.036#
MeanAffectivity:as.factor(Contrast)  0.0002063  0.0007338   0.281#
#
Correlation of Fixed Effects:#
            (Intr) MnAffc Cntrst#
MenAffctvty  0.052              #
as.factor(Contrast)    -0.769 -0.147       #
MnAffctvt:C -0.040 -0.970  0.126#
convergence code: 1#
Model failed to converge with max|grad| = 7.69421 (tol = 0.002, component 1)#
Model is nearly unidentifiable: very large eigenvalue#
 - Rescale variables?#
#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.raw)[2,3]))))#
[1] "p value =  0.84142962609243"#
> emo.hebr.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity*as.factor(Contrast) + (1+MeanAffectivity*as.factor(Contrast)|SubjNo)+ (1+as.factor(Contrast)|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
Error in summary(lmer(log(rt) ~ MeanAffectivity * as.factor(Contrast) + (1 + MeanAffectivity *  : #
  error in evaluating the argument 'object' in selecting a method for function 'summary': Error in print(args(+as.factor(Contrast))) : #
  error in evaluating the argument 'x' in selecting a method for function 'print': Error in args(+as.factor(Contrast)) : object 'as.factor(Contrast)' not found#
#
> print(emo.hebr.lmer.log)#
Linear mixed model fit by REML ['lmerMod']#
Formula: log(rt) ~ MeanAffectivity + (1 + MeanAffectivity | SubjNo) +      (1 | prime)#
   Data: subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))#
#
REML criterion at convergence: 4223.1#
#
Scaled residuals: #
    Min      1Q  Median      3Q     Max #
-3.5075 -0.6554 -0.1164  0.4983  4.5337 #
#
Random effects:#
 Groups   Name            Variance  Std.Dev. Corr #
 SubjNo   (Intercept)     0.0870254 0.29500       #
          MeanAffectivity 0.0002955 0.01719  -0.04#
 prime    (Intercept)     0.0009478 0.03079       #
 Residual                 0.1180557 0.34359       #
Number of obs: 5560, groups:  SubjNo, 65; prime, 45#
#
Fixed effects:#
                Estimate Std. Error t value#
(Intercept)     0.244268   0.037165   6.573#
MeanAffectivity 0.000947   0.006847   0.138#
#
Correlation of Fixed Effects:#
            (Intr)#
MenAffctvty -0.013#
> print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.log)[2,3]))))#
[1] "p value =  0.889988182812993"#
> #
> #
> emo.hebr.lmer.raw <- summary(lmer(rt ~ MeanAffectivity*as.factor(Contrast) + (1+MeanAffectivity*as.factor(Contrast)|SubjNo)+ (1+as.factor(Contrast)|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.hebr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.hebr.lmer.raw)[2,3]))))#
emo.hebr.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity*as.factor(Contrast) + (1+MeanAffectivity*as.factor(Contrast)|SubjNo)+ (1+as.factor(Contrast)
emo.contr.lmer.raw <- summary(lmer(rt ~ as.factor(Contrast)*MeanAffectivity + (1+as.factor(Contrast)*MeanAffectivity|SubjNo)+ (1+as.factor(Contrast|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.raw)[2,3]))))#
emo.contr.lmer.log <- summary(lmer(rt ~ as.factor(Contrast)*MeanAffectivity + (1+as.factor(Contrast)*MeanAffectivity|SubjNo)+ (1+as.factor(Contrast|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[2,3]))))
emo.contr.lmer.raw <- summary(lmer(rt ~ as.factor(Contrast)*MeanAffectivity + (1+as.factor(Contrast)*MeanAffectivity|SubjNo)+ (1+as.factor(Contrast|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.raw)[2,3]))))#
emo.contr.lmer.log <- summary(lmer(rt ~ as.factor(Contrast)*MeanAffectivity + (1+as.factor(Contrast)*MeanAffectivity|SubjNo)+ (1+as.factor(Contrast|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[2,3]))))
emo.contr.lmer.raw <- summary(lmer(rt ~ as.factor(Contrast)*MeanAffectivity + (1+as.factor(Contrast)*MeanAffectivity|SubjNo)+ (1+as.factor(Contrast)|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.raw)[2,3]))))#
emo.contr.lmer.log <- summary(lmer(rt ~ as.factor(Contrast)*MeanAffectivity + (1+as.factor(Contrast)*MeanAffectivity|SubjNo)+ (1+as.factor(Contrast)|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[2,3]))))
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[,3]))))
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.raw)[,3]))))
contrasts(emo.pop.hebr$Contrast)[1] <- -1#
emo.contr.lmer.raw <- summary(lmer(rt ~ Contrast*MeanAffectivity + (1+Contrast*MeanAffectivity|SubjNo)+ (1+Contrast|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.raw)[,3]))))#
emo.contr.lmer.log <- summary(lmer(rt ~ Contrast*MeanAffectivity + (1+Contrast*MeanAffectivity|SubjNo)+ (1+Contrast|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[,3]))))
emo.pop.hebr$Contrast2 <- as.facter(emo.pop.hebr$Contrast)#
contrasts(emo.pop.hebr$Contrast2)[1] <- -1#
emo.contr.lmer.raw <- summary(lmer(rt ~ Contrast2*MeanAffectivity + (1+Contrast2*MeanAffectivity|SubjNo)+ (1+Contrast2|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.raw)[,3]))))#
emo.contr.lmer.log <- summary(lmer(rt ~ Contrast2*MeanAffectivity + (1+Contrast2*MeanAffectivity|SubjNo)+ (1+Contrast2|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[,3]))))
?as.factor
emo.pop.hebr$Contrast2 <- facter(emo.pop.hebr$Contrast)
emo.pop.hebr$Contrast2 <- as.factor(emo.pop.hebr$Contrast)#
contrasts(emo.pop.hebr$Contrast2)[1] <- -1#
emo.contr.lmer.raw <- summary(lmer(rt ~ Contrast2*MeanAffectivity + (1+Contrast2*MeanAffectivity|SubjNo)+ (1+Contrast2|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.raw)[,3]))))#
emo.contr.lmer.log <- summary(lmer(rt ~ Contrast2*MeanAffectivity + (1+Contrast2*MeanAffectivity|SubjNo)+ (1+Contrast2|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[,3]))))
emo.contr.lmer.log <- summary(lmer(log(rt) ~ Contrast2*MeanAffectivity + (1+Contrast2*MeanAffectivity|SubjNo)+ (1+Contrast2|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[,3]))))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Contrast <- as.factor(emo.pop.hebr.sum$Contrast)#
contrasts(emo.pop.hebr.sum$Contrast)[1] <- -1#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.hebr.sum))#
summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.hebr.sum))
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Contrast <- as.factor(emo.pop.hebr.sum$Contrast)#
contrasts(emo.pop.hebr.sum$Contrast)[1] <- -1#
summary(lm(rt ~ MeanAffectivity*Contrast, data = emo.pop.hebr.sum))#
summary(lm(log(rt) ~ MeanAffectivity*Contrast, data = emo.pop.hebr.sum))
# Finally -- a quick test if English is perceived faster than Hebrew (following Jiang et al 07, #
# tho note that there are length confounds here#
emo.pop$Lang <- "English"#
emo.pop[emo.pop$prime_semantics %in% c("Hebrew"),]$Lang <- "Hebrew"#
emo.pop.lang <- (lmer(rt ~ Lang + (1+Lang|SubjNo), data = subset(emo.pop, Contrast == 50))#
print(emo.pop.lang)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[,3]))))
# Finally -- a quick test if English is perceived faster than Hebrew (following Jiang et al 07, #
# tho note that there are length confounds here#
emo.pop$Lang <- "English"#
emo.pop[emo.pop$prime_semantics %in% c("Hebrew"),]$Lang <- "Hebrew"#
emo.pop.lang <- lmer(rt ~ Lang + (1+Lang|SubjNo), data = subset(emo.pop, Contrast == 50))#
print(emo.pop.lang)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[,3]))))
# Finally -- a quick test if English is perceived faster than Hebrew (following Jiang et al 07, #
# tho note that there are length confounds here#
emo.pop$Lang <- "English"#
emo.pop[emo.pop$prime_semantics %in% c("Hebrew"),]$Lang <- "Hebrew"#
emo.pop.lang <- lmer(rt ~ Lang + (1+Lang|SubjNo), data = subset(emo.pop, Contrast == 50))#
print(summary(emo.pop.lang))#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[,3]))))
print(paste("p value = ", 2*pnorm(-abs(coef(emo.pop.lang)[,3]))))
emo.pop.lang <- summary(lmer(rt ~ Lang + (1+Lang|SubjNo), data = subset(emo.pop, Contrast == 50)))#
print(summary(emo.pop.lang))#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.pop.lang)[,3]))))
lang <- summaryBy(rt ~ Lang + SubjNo, data = subset(emo.pop, Contrast == 50),keep.names = T)#
lang <- summaryBy(rt ~ Lang , data = subset(emo.pop, Contrast == 50),keep.names = T, FUN = c(mean,sd))
lang
summary(emo.pop)
summary(lmer(rt ~ Length + (1+Length|SubjNo), data = subset(emo.pop, Contrast == 50)))
2*pnorm(-abs(2.86))
library(plyr)#
library(lme4)#
library(doBy)#
library(ggplot2)#
#
read_data <- function(path_name){#
list.files(path = path_name,full.names = T, pattern = ".csv") -> file_list#
comp = c()#
for (x in file_list){#
	data <- read.csv(x,header = T)#
	if ("perceptual.rating.reactiontime" %in% colnames(data)){ #
		data <- subset(data, select = -perceptual.rating.reactiontime)#
		}#
		if ("X" %in% colnames(data)){ #
		data <- subset(data, select = -X)#
		data$rt <- as.character(data$rt)#
		}#
	comp <- rbind(comp, data)#
	}#
	return(comp)#
}#
#
sense.pop <- read_data("./data/")#
#
# Make RTs numeric [need to remove timeout "none" responses to 8s]#
sense.pop <- subset(sense.pop, rt != "None")#
sense.pop$rt <- as.numeric(sense.pop$rt)#
sense.pop$Length <- nchar(as.character(sense.pop$prime),allowNA = T)#
sense.pop$Condition <- as.character(sense.pop$prime_semantics)#
sense.pop[sense.pop$prime_semantics %in% c("Sklar_control_A","Sklar_control_B"),]$Condition <- "Sklar_control"#
sense.pop$Condition <- as.factor(sense.pop$Condition)#
# Note that this analysis includes all of the inclusion criteria discussed by Sklar et al. #
#
###########################################################################################################################
##
# Let's first analyze for the Sklar trials#
sense.pop.sklar <- subset(sense.pop, Condition %in% c("Sklar_control", "Sklar_violation")) #
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < 3sd above group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.sklar), keep.names = T)#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.sklar <- subset(sense.pop.sklar, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(Condition), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
#
# Remove RTs < 200ms#
sense.pop.sklar <- subset(sense.pop.sklar, rt > 0.2)#
#
# T test (Sklar style)#
sense.pop.sklar.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.sklar.summary, paired = T)#
# lmer (Rabag style)#
sense.pop.sklar.raw <- summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control"))))#
print(sense.pop.sklar.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.sklar.raw)[,3]))))#
#
sense.pop.sklar.log <- summary(lmer(log(rt) ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control"))))#
print(sense.pop.sklar.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.sklar.log)[,3]))))
###########################################################################################################################
##
# Let's now analyze for the new trials#
sense.pop.new <- subset(sense.pop, Condition %in% c("Sensible", "Non-sensible")) #
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.new), keep.names = T)#
sense.pop.new <- subset(sense.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
sense.pop.new <- subset(sense.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.new <- subset(sense.pop.new, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.new <- ddply(sense.pop.new, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.new <- ddply(sense.pop.new, .(Condition), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# Remove RTs < 200ms#
sense.pop.new <- subset(sense.pop.new, rt > 0.2)#
#
# T test (Sklar style)#
sense.pop.new.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.new.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.new.summary, paired = T)#
#
# lmer (rabag style)#
sense.pop.new.raw <-summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible"))))#
print(sense.pop.new.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.new.raw)[,3]))))#
#
sense.pop.new.log <- summary(lmer(log(rt) ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible"))))#
print(sense.pop.new.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.new.log)[,3]))))
# Finally -- a quick test if longer stims are perceived faster than shorter, #
#
sense.pop.length <- sense.pop#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.length), keep.names = T)#
sense.pop.length <- subset(sense.pop.length, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
sense.pop.length <- subset(sense.pop.length, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.length <- subset(sense.pop.length, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.length <- ddply(sense.pop.length, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# Remove RTs < 200ms#
sense.pop.length <- subset(sense.pop.length, rt > 0.2)#
#
# Standardize lenth#
sense.pop.length$Length <- (sense.pop.length$Length - mean(sense.pop.length$Length, na.rm = T))/sd(sense.pop.length$Length, na.rm = T)#
sense.pop.length.raw <- summary(lmer(rt ~ Length + (1+Length|SubjNo), data = sense.pop.length))#
print(sense.pop.length.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.length.raw)[,3]))))
library(plyr)#
library(lme4)#
library(doBy)#
#
emo.pop <- read.csv("all_data_with_string_lengths")#
#
# Make RTs numeric#
emo.pop <- subset(emo.pop, rt != "None")#
#
emo.pop$rt <- as.numeric(as.character(emo.pop$rt))#
emo.pop$Length <- nchar(as.character(emo.pop$prime),allowNA = T)#
#
# First standardize the MeanAffectivity score#
emo.pop$MeanAffectivity <- (emo.pop$MeanAffectivity - mean(emo.pop$MeanAffectivity, na.rm = T))/sd(emo.pop$MeanAffectivity, na.rm = T)#
#
###########################################################################################################################
##
# Sklar Experiment first#
emo.pop.sklar <- subset(emo.pop, prime_semantics %in% c("Negative phrase","Neutral phrase"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.sklar), keep.names = T)#
emo.pop.sklar <- subset(emo.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.sklar <- subset(emo.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.sklar <- subset(emo.pop.sklar, match. == 1)#
#
emo.pop.sklar <- ddply(emo.pop.sklar, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.sklar <- subset(emo.pop.sklar, rt > 0.2)#
#
# First standardize the MeanAffectivity score#
emo.pop.sklar$MeanAffectivity <- (emo.pop.sklar$MeanAffectivity - mean(emo.pop.sklar$MeanAffectivity, na.rm = T))/sd(emo.pop.sklar$MeanAffectivity, na.rm = T)#
#
#Lin Reg (sklar style)#
emo.pop.sklar.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.sklar, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.sklar.sum))#
summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.sklar.sum))#
#
# note that you can calculate BF by estimating the sample SE from Sklar's #
# regression coefficient, his t stat, and his sample size (46) #
#
# lmer (rabag style)#
emo.sklar.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
print(emo.sklar.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))))#
emo.sklar.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
print(emo.sklar.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.log)[2,3]))))#
#
###########################################################################################################################
##
# New reversed sentences Experiment next #
emo.pop.new <- subset(emo.pop, prime_semantics %in% c("Negative sentence","Neutral sentence"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.new), keep.names = T)#
emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.new <- subset(emo.pop.new, match. == 1)#
#
emo.pop.new <- ddply(emo.pop.new, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.new <- subset(emo.pop.new, rt > 0.2)#
#
#  standardize the MeanAffectivity score#
emo.pop.new$MeanAffectivity <- (emo.pop.new$MeanAffectivity - mean(emo.pop.new$MeanAffectivity, na.rm = T))/sd(emo.pop.new$MeanAffectivity, na.rm = T)#
#
emo.pop.new.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.new, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.new.sum))#
summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.new.sum))#
# lmer (rabag style)#
emo.new.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
print(emo.new.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.raw)[2,3]))))#
emo.new.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
print(emo.new.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.log)[2,3]))))#
############################################################################################################################
##
# Finally, Hebrew Experiment#
emo.pop.hebr <- subset(emo.pop, prime_semantics %in% c("Hebrew"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.hebr), keep.names = T)#
emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.hebr <- subset(emo.pop.hebr, match. == 1)#
#
emo.pop.hebr <- ddply(emo.pop.hebr, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.hebr <- subset(emo.pop.hebr, rt > 0.2)#
#
# Can't do a by-items analysis until Hebrew is properly recoded.#
#  standardize the MeanAffectivity score#
emo.pop.hebr$MeanAffectivity <- (emo.pop.hebr$MeanAffectivity - mean(emo.pop.hebr$MeanAffectivity, na.rm = T))/sd(emo.pop.hebr$MeanAffectivity, na.rm = T)#
#
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Contrast <- as.factor(emo.pop.hebr.sum$Contrast)#
contrasts(emo.pop.hebr.sum$Contrast)[1] <- -1#
summary(lm(rt ~ MeanAffectivity*Contrast, data = emo.pop.hebr.sum))#
summary(lm(log(rt) ~ MeanAffectivity*Contrast, data = emo.pop.hebr.sum))#
# lmer (rabag style)#
emo.pop.hebr$Contrast2 <- as.factor(emo.pop.hebr$Contrast)#
contrasts(emo.pop.hebr$Contrast2)[1] <- -1#
emo.contr.lmer.raw <- summary(lmer(rt ~ Contrast2*MeanAffectivity + (1+Contrast2*MeanAffectivity|SubjNo)+ (1+Contrast2|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.raw)[,3]))))#
emo.contr.lmer.log <- summary(lmer(log(rt) ~ Contrast2*MeanAffectivity + (1+Contrast2*MeanAffectivity|SubjNo)+ (1+Contrast2|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[,3]))))#
###########################################################################################################################
# We can also check if this depends on perceptual rating#
# Finally -- a quick test if English is perceived faster than Hebrew (following Jiang et al 07, #
# tho note that there are length confounds here#
emo.pop$Lang <- "English"#
emo.pop[emo.pop$prime_semantics %in% c("Hebrew"),]$Lang <- "Hebrew"#
lang <- summaryBy(rt ~ Lang + SubjNo, data = subset(emo.pop, Contrast == 50),keep.names = T)#
summaryBy(rt ~ Lang , data = subset(emo.pop, Contrast == 50),keep.names = T, FUN = c(mean,sd))#
emo.pop.lang <- summary(lmer(rt ~ Lang + (1+Lang|SubjNo), data = subset(emo.pop, Contrast == 50)))#
print(summary(emo.pop.lang))#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.pop.lang)[,3]))))#
#
###########################################################################################################################
##
# Graphs#
#
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = Contrast)) +#
    geom_point() +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
library(plyr)#
library(lme4)#
library(doBy)#
#
emo.pop <- read.csv("all_data_with_string_lengths.csv")#
#
# Make RTs numeric#
emo.pop <- subset(emo.pop, rt != "None")#
#
emo.pop$rt <- as.numeric(as.character(emo.pop$rt))#
emo.pop$Length <- nchar(as.character(emo.pop$prime),allowNA = T)#
#
# First standardize the MeanAffectivity score#
emo.pop$MeanAffectivity <- (emo.pop$MeanAffectivity - mean(emo.pop$MeanAffectivity, na.rm = T))/sd(emo.pop$MeanAffectivity, na.rm = T)#
#
###########################################################################################################################
##
# Sklar Experiment first#
emo.pop.sklar <- subset(emo.pop, prime_semantics %in% c("Negative phrase","Neutral phrase"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.sklar), keep.names = T)#
emo.pop.sklar <- subset(emo.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.sklar <- subset(emo.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.sklar <- subset(emo.pop.sklar, match. == 1)#
#
emo.pop.sklar <- ddply(emo.pop.sklar, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.sklar <- subset(emo.pop.sklar, rt > 0.2)#
#
# First standardize the MeanAffectivity score#
emo.pop.sklar$MeanAffectivity <- (emo.pop.sklar$MeanAffectivity - mean(emo.pop.sklar$MeanAffectivity, na.rm = T))/sd(emo.pop.sklar$MeanAffectivity, na.rm = T)#
#
#Lin Reg (sklar style)#
emo.pop.sklar.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.sklar, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.sklar.sum))#
summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.sklar.sum))#
#
# note that you can calculate BF by estimating the sample SE from Sklar's #
# regression coefficient, his t stat, and his sample size (46) #
#
# lmer (rabag style)#
emo.sklar.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
print(emo.sklar.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.raw)[2,3]))))#
emo.sklar.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1|prime), data = subset(emo.pop.sklar, prime_semantics %in% c("Negative phrase","Neutral phrase"))))#
print(emo.sklar.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.sklar.lmer.log)[2,3]))))#
#
###########################################################################################################################
##
# New reversed sentences Experiment next #
emo.pop.new <- subset(emo.pop, prime_semantics %in% c("Negative sentence","Neutral sentence"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.new), keep.names = T)#
emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.new <- subset(emo.pop.new, match. == 1)#
#
emo.pop.new <- ddply(emo.pop.new, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.new <- subset(emo.pop.new, rt > 0.2)#
#
#  standardize the MeanAffectivity score#
emo.pop.new$MeanAffectivity <- (emo.pop.new$MeanAffectivity - mean(emo.pop.new$MeanAffectivity, na.rm = T))/sd(emo.pop.new$MeanAffectivity, na.rm = T)#
#
emo.pop.new.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.new, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.new.sum))#
summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.new.sum))#
# lmer (rabag style)#
emo.new.lmer.raw <- summary(lmer(rt ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
print(emo.new.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.raw)[2,3]))))#
emo.new.lmer.log <- summary(lmer(log(rt) ~ MeanAffectivity + (1+MeanAffectivity|SubjNo)+ (1+MeanAffectivity|PairID), data = subset(emo.pop.new, prime_semantics %in% c("Negative sentence","Neutral sentence"))))#
print(emo.new.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.new.lmer.log)[2,3]))))#
############################################################################################################################
##
# Finally, Hebrew Experiment#
emo.pop.hebr <- subset(emo.pop, prime_semantics %in% c("Hebrew"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.hebr), keep.names = T)#
emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.hebr <- subset(emo.pop.hebr, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.hebr <- subset(emo.pop.hebr, match. == 1)#
#
emo.pop.hebr <- ddply(emo.pop.hebr, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.hebr <- subset(emo.pop.hebr, rt > 0.2)#
#
# Can't do a by-items analysis until Hebrew is properly recoded.#
#  standardize the MeanAffectivity score#
emo.pop.hebr$MeanAffectivity <- (emo.pop.hebr$MeanAffectivity - mean(emo.pop.hebr$MeanAffectivity, na.rm = T))/sd(emo.pop.hebr$MeanAffectivity, na.rm = T)#
#
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Contrast <- as.factor(emo.pop.hebr.sum$Contrast)#
contrasts(emo.pop.hebr.sum$Contrast)[1] <- -1#
summary(lm(rt ~ MeanAffectivity*Contrast, data = emo.pop.hebr.sum))#
summary(lm(log(rt) ~ MeanAffectivity*Contrast, data = emo.pop.hebr.sum))#
# lmer (rabag style)#
emo.pop.hebr$Contrast2 <- as.factor(emo.pop.hebr$Contrast)#
contrasts(emo.pop.hebr$Contrast2)[1] <- -1#
emo.contr.lmer.raw <- summary(lmer(rt ~ Contrast2*MeanAffectivity + (1+Contrast2*MeanAffectivity|SubjNo)+ (1+Contrast2|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.raw)[,3]))))#
emo.contr.lmer.log <- summary(lmer(log(rt) ~ Contrast2*MeanAffectivity + (1+Contrast2*MeanAffectivity|SubjNo)+ (1+Contrast2|prime), data = subset(emo.pop.hebr, prime_semantics %in% c("Hebrew"))))#
print(emo.contr.lmer.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.contr.lmer.log)[,3]))))#
###########################################################################################################################
# We can also check if this depends on perceptual rating#
# Finally -- a quick test if English is perceived faster than Hebrew (following Jiang et al 07, #
# tho note that there are length confounds here#
emo.pop$Lang <- "English"#
emo.pop[emo.pop$prime_semantics %in% c("Hebrew"),]$Lang <- "Hebrew"#
lang <- summaryBy(rt ~ Lang + SubjNo, data = subset(emo.pop, Contrast == 50),keep.names = T)#
summaryBy(rt ~ Lang , data = subset(emo.pop, Contrast == 50),keep.names = T, FUN = c(mean,sd))#
emo.pop.lang <- summary(lmer(rt ~ Lang + (1+Lang|SubjNo), data = subset(emo.pop, Contrast == 50)))#
print(summary(emo.pop.lang))#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.pop.lang)[,3]))))#
#
###########################################################################################################################
##
# Graphs#
#
emo.pop.hebr.sum <- summaryBy(rt ~ prime + MeanAffectivity + Contrast, data = emo.pop.hebr, keep.names = T)#
emo.pop.hebr.sum$Experiment <- "Experiment 2c"#
#
emo.pop.sklar.sum$Experiment <- "Experiment 2a"#
emo.pop.sklar.sum$Contrast <- 50#
emo.pop.new.sum$Experiment <- "Experiment 2b"#
emo.pop.new.sum$Contrast <- 50#
#
graph <- rbind(emo.pop.sklar.sum,emo.pop.new.sum,emo.pop.hebr.sum)#
#
graph <- na.omit(graph)#
graph$Experiment <- ordered(graph$Experiment, levels = c("Experiment 2a", "Experiment 2b", "Experiment 2c"))#
graph$Contrast <- ordered(graph$Contrast, levels = c("50","80"), labels = c("50%","80%"))#
graph$rt <- graph$rt * 1000#
#
ggplot(graph, aes(x=MeanAffectivity, y=rt, shape = Contrast)) +#
    geom_point() +    # Use hollow circles#
    geom_smooth(method=lm,   # Add linear regression line#
                se=FALSE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
summary(emo.pop)
emo.pop.lang <- summary(lmer(rt ~ Lang+Length + (1+Lang|SubjNo), data = subset(emo.pop, Contrast == 50)))#
print(summary(emo.pop.lang))
emo.pop.lang <- summary(lmer(rt ~ Lang +Length+ (1+Lang+Length|SubjNo), data = subset(emo.pop, Contrast == 50)))#
print(summary(emo.pop.lang))#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.pop.lang)[,3]))))
emo.pop.lang <- summary(lmer(rt ~ Lang *Length+ (1+Lang*Length|SubjNo), data = subset(emo.pop, Contrast == 50)))#
print(summary(emo.pop.lang))#
print(paste("p value = ", 2*pnorm(-abs(coef(emo.pop.lang)[,3]))))
0.356/2.523
library(plyr)#
library(lme4)#
library(doBy)#
#
emo.pop <- read.csv("all_data_with_pairIDs.csv")#
#
# Make RTs numeric#
emo.pop <- subset(emo.pop, rt != "None")#
#
emo.pop$rt <- as.numeric(as.character(emo.pop$rt))#
emo.pop$Length <- nchar(as.character(emo.pop$prime),allowNA = T)#
#
# First standardize the MeanAffectivity score#
emo.pop$MeanAffectivity <- (emo.pop$MeanAffectivity - mean(emo.pop$MeanAffectivity, na.rm = T))/sd(emo.pop$MeanAffectivity, na.rm = T)#
#
###########################################################################################################################
##
# Sklar Experiment first#
emo.pop.sklar <- subset(emo.pop, prime_semantics %in% c("Negative phrase","Neutral phrase"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.sklar), keep.names = T)#
emo.pop.sklar <- subset(emo.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.sklar <- subset(emo.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.sklar <- subset(emo.pop.sklar, match. == 1)#
#
emo.pop.sklar <- ddply(emo.pop.sklar, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.sklar <- subset(emo.pop.sklar, rt > 0.2)#
#
# First standardize the MeanAffectivity score#
emo.pop.sklar$MeanAffectivity <- (emo.pop.sklar$MeanAffectivity - mean(emo.pop.sklar$MeanAffectivity, na.rm = T))/sd(emo.pop.sklar$MeanAffectivity, na.rm = T)#
#
#Lin Reg (sklar style)#
emo.pop.sklar.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.sklar, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.sklar.sum))#
summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.sklar.sum))#
#
# note that you can calculate BF by estimating the sample SE from Sklar's #
# regression coefficient (0.356) and his t stat 2.523. t = b/se therefore se = b/t = #
# This is done using Z.D.'s BF calculator http://www.lifesci.sussex.ac.uk/home/Zoltan_Dienes/inference/Bayes.htm#
# We know the original coef and the t, and we are testing against a 0 effect. If Sklar is right, we should get data#
# that falls within the normal distribution around his effect.
-0.01880*1000
0.01246*1000
emo.pop.sklar.sum$rt_stand <- (emo.pop.sklar.sum$rt - mean(emo.pop.sklar.sum$rt))/sd(emo.pop.sklar.sum$rt)
summary(lm(rt_stand ~ MeanAffectivity, data = emo.pop.sklar.sum))
summary(emo.pop.sklar.sum)
emo.pop.sklar.sum$rt_stand <- (emo.pop.sklar.sum$rt - mean(emo.pop.sklar.sum$rt, na.rm = T))/sd(emo.pop.sklar.sum$rt, na.rm = T)
summary(emo.pop.sklar.sum)
sd(emo.pop.sklar.sum$rt)
mean(emo.pop.sklar.sum$rt)
summary(lm(rt_stand ~ MeanAffectivity, data = emo.pop.sklar.sum))
summary(lm(rt ~ MeanAffectivity, data = emo.pop.sklar.sum))
-0.03966 * 01000
0.356/2.523
0.14 * sqrt(44)
0.14 * sqrt(46)
0.14 * sqrt(46)
library(bayesFactor)
library(BayesFactor)
BFManual()
library(plyr)#
library(lme4)#
library(doBy)#
library(ggplot2)#
#
read_data <- function(path_name){#
list.files(path = path_name,full.names = T, pattern = ".csv") -> file_list#
comp = c()#
for (x in file_list){#
	data <- read.csv(x,header = T)#
	if ("perceptual.rating.reactiontime" %in% colnames(data)){ #
		data <- subset(data, select = -perceptual.rating.reactiontime)#
		}#
		if ("X" %in% colnames(data)){ #
		data <- subset(data, select = -X)#
		data$rt <- as.character(data$rt)#
		}#
	comp <- rbind(comp, data)#
	}#
	return(comp)#
}#
#
sense.pop <- read_data("./data/")#
#
# Make RTs numeric [need to remove timeout "none" responses to 8s]#
sense.pop <- subset(sense.pop, rt != "None")#
sense.pop$rt <- as.numeric(sense.pop$rt)#
sense.pop$Length <- nchar(as.character(sense.pop$prime),allowNA = T)#
sense.pop$Condition <- as.character(sense.pop$prime_semantics)#
sense.pop[sense.pop$prime_semantics %in% c("Sklar_control_A","Sklar_control_B"),]$Condition <- "Sklar_control"#
sense.pop$Condition <- as.factor(sense.pop$Condition)#
# Note that this analysis includes all of the inclusion criteria discussed by Sklar et al. #
#
###########################################################################################################################
##
# Let's first analyze for the Sklar trials#
sense.pop.sklar <- subset(sense.pop, Condition %in% c("Sklar_control", "Sklar_violation")) #
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < 3sd above group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.sklar), keep.names = T)#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.sklar <- subset(sense.pop.sklar, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(Condition), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
#
# Remove RTs < 200ms#
sense.pop.sklar <- subset(sense.pop.sklar, rt > 0.2)#
#
# T test (Sklar style)#
sense.pop.sklar.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.sklar.summary, paired = T)
-0.03519684/-1.7874
0.03519684/0.01969
958-939
sense.pop.new <- subset(sense.pop, Condition %in% c("Sensible", "Non-sensible")) #
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.new), keep.names = T)#
sense.pop.new <- subset(sense.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
sense.pop.new <- subset(sense.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.new <- subset(sense.pop.new, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.new <- ddply(sense.pop.new, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.new <- ddply(sense.pop.new, .(Condition), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# Remove RTs < 200ms#
sense.pop.new <- subset(sense.pop.new, rt > 0.2)#
#
# T test (Sklar style)#
sense.pop.new.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.new.summary, paired = T)
-0.0002327283/-0.02217
summary(sense.pop.new.summary)
subset(sense.pop.new.summary, Condition == "Non-sensible")$rt
subset(sense.pop.new.summary, Condition == "Non-sensible")$rt - subset(sense.pop.new.summary, Condition == "Sensible")$rt
length(subset(sense.pop.new.summary, Condition == "Non-sensible")$rt - subset(sense.pop.new.summary, Condition == "Sensible")$rt)
length(subset(sense.pop.new.summary, Condition == "Non-sensible")$rt)
sd(subset(sense.pop.new.summary, Condition == "Non-sensible")$rt - subset(sense.pop.new.summary, Condition == "Sensible")$rt)
sd(subset(sense.pop.new.summary, Condition == "Non-sensible")$rt - subset(sense.pop.new.summary, Condition == "Sensible")$rt)/sqrt(48)
20/1000
sd(subset(sense.pop.new.summary, Condition == "Non-sensible")$rt - subset(sense.pop.new.summary, Condition == "Sensible")$rt)/sqrt(60)
1059-1108
# T test (Sklar style)#
sense.pop.sklar.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)
-0.03/-1.7874
1059 - 1,108
1059 - 1108
-0.03519684/-1.7874
t.test(rt ~ Condition, data = sense.pop.new.summary, paired = T)
-0.0002327283/-0.02217
# New reversed sentences Experiment next #
emo.pop.new <- subset(emo.pop, prime_semantics %in% c("Negative sentence","Neutral sentence"))#
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(emo.pop.new), keep.names = T)#
emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
emo.pop.new <- subset(emo.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
emo.pop.new <- subset(emo.pop.new, match. == 1)#
#
emo.pop.new <- ddply(emo.pop.new, .(SubjNo), function(d){ #
	include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt > include[1] & rt < include[2])#
	})#
#
# Remove RTs < 200ms#
emo.pop.new <- subset(emo.pop.new, rt > 0.2)#
#
#  standardize the MeanAffectivity score#
emo.pop.new$MeanAffectivity <- (emo.pop.new$MeanAffectivity - mean(emo.pop.new$MeanAffectivity, na.rm = T))/sd(emo.pop.new$MeanAffectivity, na.rm = T)#
#
emo.pop.new.sum <- summaryBy(rt ~ prime + MeanAffectivity, data = emo.pop.new, keep.names = T)#
summary(lm(rt ~ MeanAffectivity, data = emo.pop.new.sum))#
summary(lm(log(rt) ~ MeanAffectivity, data = emo.pop.new.sum))#
#
# note that you can calculate BF by estimating the sample SE from Sklar's #
# regression coefficient (0.356) and his t stat 2.523. t = b/se therefore se = b/t = 0.1411019. Therefore SD = SE * sqrt(N = 46) = 0.9495262#
# This is done using Z.D.'s BF calculator http://www.lifesci.sussex.ac.uk/home/Zoltan_Dienes/inference/Bayes.htm#
# We know the original coef and the t, and we are testing against a 0 effect. If Sklar is right, we should get data#
# that falls within the normal distribution around his effect [ie don't use uniform option].#
# Note that Sklar's original coef appears to be a Beta, i.e., a standardized coefficient, so first#
emo.pop.new.sum$rt_stand <- (emo.pop.new.sum$rt - mean(emo.pop.new.sum$rt, na.rm = T))/sd(emo.pop.new.sum$rt, na.rm = T)#
summary(lm(rt_stand ~ MeanAffectivity, data = emo.pop.new.sum))#
# Sample estimate is -0.03966 and sample SE is 0.02627 [from regression above], Theory M = 0.356, Theory SD = 0.9495262#
#BF is therefore 0.08
