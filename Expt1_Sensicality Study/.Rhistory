edin.emo$Length <- nchar(as.character(edin.emo$Stim_column))
edin.emo$Length <- (edin.emo$Length - mean(edin.emo$Length))/sd(edin.emo$Length)
edin.emo$rt <- log(edin.emo$rt)
#Mark timeouts as incorrect, for exclusion
edin.emo[edin.emo$Acc < 0,]$Acc <- 0
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)
Acc <- summaryBy(Acc + rt ~ SubjNo, data = edin.emo, keep.names = T)
edin.emo <- edin.emo[edin.emo$SubjNo %in% Acc[Acc$Acc > 0.9,]$SubjNo,]
edin.emo <- edin.emo[edin.emo$SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo,]
#Remove incorrect trials
edin.emo <- edin.emo[edin.emo$Acc ==1,]
# Remove RTs +/-3sd from each subject's mean
edin.emo <- ddply(edin.emo, .(SubjNo), function(d){
include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt > include[1] & rt < include[2])
})
# Remove RTs < 200ms & > 10s
edin.emo <- subset(edin.emo, rt > -1.6)
edin.emo <- subset(edin.emo, rt <= 2.3)
# First standardize the MeanAffectivity score
edin.emo$MeanAffectivity_column <- (edin.emo$MeanAffectivity_column - mean(edin.emo$MeanAffectivity_column, na.rm = T))/sd(edin.emo$MeanAffectivity_column, na.rm = T)
# Lin Reg (edin style)
edin.emo$log.rt <- log(edin.emo$rt)
edin.emo.sum <- summaryBy(rt + log.rt~ Stim_column + MeanAffectivity_column, data = edin.emo, keep.names = T)
summary(lm(rt ~ MeanAffectivity_column, data = edin.emo.sum))
summary(lm(log.rt ~ MeanAffectivity_column, data = edin.emo.sum))
# Lin Reg (sklar style)
edin.emo$log.rt <- log(edin.emo$rt)
edin.emo.sum.factorial <- summaryBy(rt + log.rt ~ SubjNo + Type_column, data = edin.emo, keep.names = T)
t.test(rt ~ Type_column, data = edin.emo.sum.factorial, paired = T)
t.test(log.rt ~ Type_column, data = edin.emo.sum.factorial, paired = T)
# lmer (Rabag style)
edin.emo.sum.raw <- summary(lmer(rt ~ Type_column + (1+Type_column|SubjNo)+ (1|Stim_column), data = edin.emo))
print(edin.emo.sum.raw)
print(paste("p value = ", 2*pnorm(-abs(coef(edin.emo.sum.raw)[,3]))))
edin.emo.sum.log <- summary(lmer(rt ~ Type_column +Length + (1+Type_column|SubjNo)+ (1|Stim_column), data = edin.emo))
print(edin.emo.sum.log)
print(paste("p value = ", 2*pnorm(-abs(coef(edin.emo.sum.log)[,3]))))
sense.graph <- summaryBy(rt ~ Type_column + SubjNo, data = edin.emo, keep.names = T)
sense.graph <- summaryBy(rt ~ Type_column, data = sense.graph, FUN = c(mean,sd))
sense.graph$SE <- sense.graph$rt.sd/sqrt(length(unique(emo.pop.new$SubjNo)))
sense.graph$rt.mean <- sense.graph$rt.mean * 1000
sense.graph$SE <- sense.graph$SE * 1000
dodge <- position_dodge(width=0.9)
ggplot(sense.graph, aes(Type_column,rt.mean, fill = Type_column)) +
geom_bar(stat = "identity",  position = dodge) +
geom_errorbar(aes(ymax = sense.graph$rt.mean +
sense.graph$SE, ymin = sense.graph$rt.mean - sense.graph$SE), width=0.25, position = dodge) +
labs(fill = "Sentence Type") +
theme(axis.text.x = element_text(colour = "black", size = 12)) +
ylab("Reaction Time (ms)") +
xlab("") +
ylim(c(0,2000))
##################
# Emo Graphs
sklar.emo.sum$Experiment <- "Two Word Phrases"
edin.emo.sum$Experiment <- "Reversible Sentences"
graph <- rbind(sklar.emo.sum,edin.emo.sum)
graph <- na.omit(graph)
graph$Experiment <- ordered(graph$Experiment, levels = c("Two Word Phrases", "Reversible Sentences"))
graph$rt <- graph$rt * 1000
ggplot(graph, aes(x=MeanAffectivity_column, y=rt)) +
geom_point() +    # Use hollow circles
geom_smooth(method=lm,   # Add linear regression line
se=TRUE) + facet_grid(.~Experiment) + labs(y = "Reaction Time (ms)", x = "Standardized Valence Rating")
##########
# Sklar Sem
sklar.sem <- Test_Import("./data","sklar_sem")
sklar.sem$rt <- log(sklar.sem$rt)
sklar.sem$Length <- nchar(as.character(sklar.sem$Stim_column))
sklar.sem$Length  <- (sklar.sem$Length  - mean(sklar.sem$Length,na.rm= T))/sd(sklar.sem$Length,na.rm= T)
sklar.sem <- sklar.sem[sklar.sem$Type_column != "Sklar_filler",]
sklar.sem$Condition = "Control"
sklar.sem[sklar.sem$Type_column == "Sklar_violation",]$Condition <- "Violation"
#Mark timeouts as incorrect, for exclusion
sklar.sem[sklar.sem$Acc < 0,]$Acc <- 0
# Rsemve outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we rsemve extra-fast participants)
Acc <- summaryBy(Acc + rt ~ SubjNo, data = sklar.sem, keep.names = T)
sklar.sem <- sklar.sem[sklar.sem$SubjNo %in% Acc[Acc$Acc > 0.9,]$SubjNo,]
# Also exclude
sklar.sem <- sklar.sem[sklar.sem$SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo,]
#Rsemve incorrect trials
sklar.sem <- sklar.sem[sklar.sem$Acc ==1,]
# Rsemve RTs +/-3sd from each subject's mean
sklar.sem <- ddply(sklar.sem, .(SubjNo), function(d){
include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt > include[1] & rt < include[2])
})
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)
sklar.sem <- ddply(sklar.sem, .(Condition), function(d){
by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt < by_subj_include[2])
})
# Rsemve RTs < 200ms & > 10
sklar.sem <- subset(sklar.sem, rt > -1.6)
sklar.sem <- subset(sklar.sem, rt <= 2.3)
# Lin Reg (sklar style)
sklar.sem$log.rt <- log(sklar.sem$rt)
sklar.sem.sum <- summaryBy(rt + log.rt~ SubjNo + Condition, data = sklar.sem, keep.names = T)
t.test(rt ~ Condition, data = sklar.sem.sum, paired = T)
t.test(log.rt ~ Condition, data = sklar.sem.sum, paired = T)
# lmer (Rabag style)
sklar.sem.sum.raw <- summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|Stim_column), data = sklar.sem))
print(sklar.sem.sum.raw)
print(paste("p value = ", 2*pnorm(-abs(coef(sklar.sem.sum.raw)[,3]))))
sklar.sem.sum.log <- summary(lmer(rt ~ Condition +Length+ (1+Condition|SubjNo)+ (1|Stim_column), data = sklar.sem))
print(sklar.sem.sum.log)
print(paste("p value = ", 2*pnorm(-abs(coef(sklar.sem.sum.log)[,3]))))
##########
# Edin Sem
edin.sem <- Test_Import("./data","edin_sem")
edin.sem$rt <- log(edin.sem$rt)
edin.sem$Length <- nchar(as.character(edin.sem$Stim_column))
edin.sem$Length <- (edin.sem$Length - mean(edin.sem$Length))/sd(edin.sem$Length)
edin.sem$Condition = "Violation"
edin.sem[edin.sem$Type_column == "Sensible",]$Condition <- "Control"
#Mark timeouts as incorrect, for exclusion
edin.sem[edin.sem$Acc < 0,]$Acc <- 0
# Rsemve outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we rsemve extra-fast participants)
Acc <- summaryBy(Acc + rt ~ SubjNo, data = edin.sem, keep.names = T)
edin.sem <- edin.sem[edin.sem$SubjNo %in% Acc[Acc$Acc > 0.9,]$SubjNo,]
edin.sem <- edin.sem[edin.sem$SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo,]
#Rsemve incorrect trials
edin.sem <- edin.sem[edin.sem$Acc ==1,]
# Rsemve RTs +/-3sd from each subject's mean
edin.sem <- ddply(edin.sem, .(SubjNo), function(d){
include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt > include[1] & rt < include[2])
})
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)
edin.sem <- ddply(edin.sem, .(Condition), function(d){
by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt < by_subj_include[2])
})
# Rsemve RTs < 200ms & > 10
edin.sem <- subset(edin.sem, rt > -1.6)
edin.sem <- subset(edin.sem, rt <= 2.3)
# Lin Reg (sklar style)
edin.sem$log.rt <- log(edin.sem$rt)
edin.sem.sum <- summaryBy(rt + log.rt ~ SubjNo + Condition, data = edin.sem, keep.names = T)
t.test(rt ~ Condition, data = edin.sem.sum, paired = T)
t.test(log.rt ~ Condition, data = edin.sem.sum, paired = T)
# lmer (Rabag style)
edin.sem.sum.raw <- summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|Stim_column), data = edin.sem))
print(edin.sem.sum.raw)
print(paste("p value = ", 2*pnorm(-abs(coef(edin.sem.sum.raw)[,3]))))
edin.sem.sum.log <- summary(lmer(rt ~ Condition+ Length + (1+Condition|SubjNo)+ (1|Stim_column), data = edin.sem))
print(edin.sem.sum.log)
print(paste("p value = ", 2*pnorm(-abs(coef(edin.sem.sum.log)[,3]))))
###########
# Sem Graphs
sense.sklar.graph <- summaryBy(rt ~ Condition + SubjNo, data = sklar.sem, keep.names = T)
sense.sklar.graph <- summaryBy(rt ~ Condition, data = sense.sklar.graph, FUN = c(mean,sd))
sense.sklar.graph$SE <- sense.sklar.graph$rt.sd/sqrt(length(unique(sklar.sem$SubjNo)))
sense.sklar.graph$Experiment <- "Experiment 1a"
sense.new.graph <- summaryBy(rt ~ Condition + SubjNo, data = edin.sem, keep.names = T)
sense.new.graph <- summaryBy(rt ~ Condition, data = sense.new.graph, FUN = c(mean,sd))
sense.new.graph$SE <- sense.new.graph$rt.sd/sqrt(length(unique(edin.sem$SubjNo)))
sense.new.graph$Experiment <- "Experiment 1b"
sense.graph <- rbind(sense.sklar.graph,sense.new.graph)
sense.graph$Cond_Graph <- "Violation"
sense.graph[sense.graph$Condition %in% c("Control"),]$Cond_Graph <- "Control"
sense.graph$Cond_Graph <- ordered(sense.graph$Cond_Graph, levels = c("Violation", "Control"))
sense.graph$rt.mean <- sense.graph$rt.mean * 1000
sense.graph$SE <- sense.graph$SE * 1000
dodge <- position_dodge(width=0.9)
ggplot(sense.graph, aes(Experiment,rt.mean, fill = Cond_Graph)) +
geom_bar(stat = "identity",  position = dodge) +
geom_errorbar(aes(ymax = sense.graph$rt.mean +
sense.graph$SE, ymin = sense.graph$rt.mean - sense.graph$SE), width=0.25, position = dodge) +
labs(fill = "Sentence Type") +
theme(axis.text.x = element_text(colour = "black", size = 12)) +
ylab("Reaction Time (ms)") +
xlab("") +
ylim(c(0,2500))
hist(edin.emo$rt)
hist(edin.sem$rt)
summary(lm(rt ~ MeanAffectivity_column, data = sklar.emo.sum))
a <- lm(rt ~ MeanAffectivity_column, data = sklar.emo.sum)
summary(a)
hist(a$residuals)
library(lme4)
library(ez)
library(jsonlite)
library(ggplot2)
library(gridExtra)
library(plyr)
library(dplyr)
library(doBy)
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R
library(bootstrap)
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}
ci.low <- function(x,na.rm=T) {
quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -
ci.high <- function(x,na.rm=T) {
quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}
Test_Import = function(path_name,expt){
library(jsonlite)
list.files(path = path_name,full.names = T, pattern = expt) -> file_list
comp = c()
for (x in file_list){
file_name = x
d <- read.csv(file_name, header = T)
comp = rbind(comp,d)
print(x)
}
comp$rt <- comp$ReactionTime_column
comp$Acc <- comp$Valid_column
comp$SubjNo <- comp$subject_column
return(comp)
}
##########
# Sklar Emo
sklar.emo <- Test_Import("./data","sklar_emo")
sklar.emo$Length <- nchar(as.character(sklar.emo$Stim_column))
#Mark timeouts as incorrect, for exclusion
sklar.emo[sklar.emo$Acc < 0,]$Acc <- 0
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be > (!!, see by trial exclusion below) 3sd from group mean [ie we remove extra-fast participants)
Acc <- summaryBy(Acc + rt ~ SubjNo, data = sklar.emo, keep.names = T)
sklar.emo <- sklar.emo[sklar.emo$SubjNo %in% Acc[Acc$Acc > 0.9,]$SubjNo,]
sklar.emo <- sklar.emo[sklar.emo$SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo,]
#Remove incorrect trials
sklar.emo <- sklar.emo[sklar.emo$Acc ==1,]
# Remove RTs +/-3sd from each subject's mean
sklar.emo <- ddply(sklar.emo, .(SubjNo), function(d){
include = mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt > include[1] & rt < include[2])
})
# Remove RTs < 200ms & > 1000ms
sklar.emo <- subset(sklar.emo, rt > 0.2)
sklar.emo <- subset(sklar.emo, rt <= 10)
# First standardize the MeanAffectivity score
sklar.emo$MeanAffectivity_column <- (sklar.emo$MeanAffectivity_column - mean(sklar.emo$MeanAffectivity_column, na.rm = T))/sd(sklar.emo$MeanAffectivity_column, na.rm = T)
a <- lm(rt ~ MeanAffectivity_column, data = sklar.emo.sum)
hist(a$residuals)
sklar.emo$log.rt <- log(sklar.emo$rt)
sklar.emo.sum <- summaryBy(rt + log.rt~ Stim_column + MeanAffectivity_column, data = sklar.emo, keep.names = T)
summary(lm(rt ~ MeanAffectivity_column, data = sklar.emo.sum))
a <- lm(rt ~ MeanAffectivity_column, data = sklar.emo.sum)
hist(a$residuals)
qqnorm(a$residuals)
setwd("~/GitHub/CFS_Compositionality/Expt1_Sensicality Study")
library(plyr)
library(lme4)
library(doBy)
library(ggplot2)
read_data <- function(path_name){
list.files(path = path_name,full.names = T, pattern = ".csv") -> file_list
comp = c()
for (x in file_list){
data <- read.csv(x,header = T)
if ("perceptual.rating.reactiontime" %in% colnames(data)){
data <- subset(data, select = -perceptual.rating.reactiontime)
}
if ("X" %in% colnames(data)){
data <- subset(data, select = -X)
data$rt <- as.character(data$rt)
}
comp <- rbind(comp, data)
}
return(comp)
}
sense.pop <- read_data("./data/")
# Make RTs numeric [need to remove timeout "none" responses to 8s]
sense.pop <- subset(sense.pop, rt != "None")
sense.pop$rt <- as.numeric(sense.pop$rt)
sense.pop$Length <- nchar(as.character(sense.pop$prime),allowNA = T)
sense.pop$Condition <- as.character(sense.pop$prime_semantics)
sense.pop[sense.pop$prime_semantics %in% c("Sklar_control_A","Sklar_control_B"),]$Condition <- "Sklar_control"
sense.pop$Condition <- as.factor(sense.pop$Condition)
# Note that this analysis includes all of the inclusion criteria discussed by Sklar et al.
##########################################################################################################################
#
# Let's first analyze for the Sklar trials
sense.pop.sklar <- subset(sense.pop, Condition %in% c("Sklar_control", "Sklar_violation"))
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < 3sd above group mean)
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.sklar), keep.names = T)
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)
# Remove incorrect trials
sense.pop.sklar <- subset(sense.pop.sklar, match. == 1)
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)
sense.pop.sklar <- ddply(sense.pop.sklar, .(SubjNo), function(d){
by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt < by_subj_include[2])
})
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)
sense.pop.sklar <- ddply(sense.pop.sklar, .(Condition), function(d){
by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt < by_subj_include[2])
})
# Remove RTs < 200ms
sense.pop.sklar <- subset(sense.pop.sklar, rt > 0.2)
# T test (Sklar style)
sense.pop.sklar$log.rt <- log(sense.pop.sklar$rt)
sense.pop.sklar.summary <- summaryBy(rt + log.rt~ SubjNo + Condition, data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")), keep.names = T)
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)
t.test(log.rt ~ Condition, data = sense.pop.sklar.summary, paired = T)
# Bayes factor -- minimum effect of 0.01, maximum of 0.06, our effect = -0.03519684 and our SE = -0.03/-1.7874=  0.01678416
# lmer (Rabag style)
sense.pop.sklar.raw <- summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control"))))
print(sense.pop.sklar.raw)
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.sklar.raw)[,3]))))
sense.pop.sklar.log <- summary(lmer(log.rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control"))))
print(sense.pop.sklar.log)
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.sklar.log)[,3]))))
sense.pop.sklar$Length <- (sense.pop.sklar$Length - mean(sense.pop.sklar$Length))/sd(sense.pop.sklar$Length)
sense.pop.sklar$Cond <- "Violation"
sense.pop.sklar[sense.pop.sklar$Condition == "Sklar_control",]$Cond <- "Control"
sense.pop.sklar$Cond <- as.factor(sense.pop.sklar$Cond)
contrasts(sense.pop.sklar$Cond)[1] <- -1
summary(glmer(rt ~ Cond+Length + (1+Cond|SubjNo)+ (1|prime), data = sense.pop.sklar, family = "inverse.gaussian"(link="log")))
##########################################################################################################################
#
# Let's now analyze for the new trials
sense.pop.new <- subset(sense.pop, Condition %in% c("Sensible", "Non-sensible"))
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < (!!, see by trial exclusion below) 3sd from group mean)
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.new), keep.names = T)
sense.pop.new <- subset(sense.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)
sense.pop.new <- subset(sense.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)
# Remove incorrect trials
sense.pop.new <- subset(sense.pop.new, match. == 1)
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)
sense.pop.new <- ddply(sense.pop.new, .(SubjNo), function(d){
by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt < by_subj_include[2])
})
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)
sense.pop.new <- ddply(sense.pop.new, .(Condition), function(d){
by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt < by_subj_include[2])
})
# Remove RTs < 200ms
sense.pop.new <- subset(sense.pop.new, rt > 0.2)
# T test (Sklar style)
sense.pop.new$log.rt <- log(sense.pop.new$rt)
sense.pop.new.summary <- summaryBy(rt + log.rt~ SubjNo + Condition, data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible")), keep.names = T)
t.test(rt ~ Condition, data = sense.pop.new.summary, paired = T)
t.test(log.rt ~ Condition, data = sense.pop.new.summary, paired = T)
# Bayes factor -- minimum effect of 0.01, maximum of 0.06, our effect = -0.0002327283 and our SE = -0.03/-0.02217=  0.01049744
# lmer (rabag style)
sense.pop.new.raw <-summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible"))))
print(sense.pop.new.raw)
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.new.raw)[,3]))))
sense.pop.new.log <- summary(lmer(log.rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible"))))
library(plyr)
library(lme4)
library(doBy)
library(ggplot2)
read_data <- function(path_name){
list.files(path = path_name,full.names = T, pattern = ".csv") -> file_list
comp = c()
for (x in file_list){
data <- read.csv(x,header = T)
if ("perceptual.rating.reactiontime" %in% colnames(data)){
data <- subset(data, select = -perceptual.rating.reactiontime)
}
if ("X" %in% colnames(data)){
data <- subset(data, select = -X)
data$rt <- as.character(data$rt)
}
comp <- rbind(comp, data)
}
return(comp)
}
sense.pop <- read_data("./data/")
# Make RTs numeric [need to remove timeout "none" responses to 8s]
sense.pop <- subset(sense.pop, rt != "None")
sense.pop$rt <- as.numeric(sense.pop$rt)
sense.pop$Length <- nchar(as.character(sense.pop$prime),allowNA = T)
sense.pop$Condition <- as.character(sense.pop$prime_semantics)
sense.pop[sense.pop$prime_semantics %in% c("Sklar_control_A","Sklar_control_B"),]$Condition <- "Sklar_control"
sense.pop$Condition <- as.factor(sense.pop$Condition)
# Note that this analysis includes all of the inclusion criteria discussed by Sklar et al.
##########################################################################################################################
#
# Let's first analyze for the Sklar trials
sense.pop.sklar <- subset(sense.pop, Condition %in% c("Sklar_control", "Sklar_violation"))
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < 3sd above group mean)
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.sklar), keep.names = T)
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)
# Remove incorrect trials
sense.pop.sklar <- subset(sense.pop.sklar, match. == 1)
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)
sense.pop.sklar <- ddply(sense.pop.sklar, .(SubjNo), function(d){
by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt < by_subj_include[2])
})
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)
sense.pop.sklar <- ddply(sense.pop.sklar, .(Condition), function(d){
by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt < by_subj_include[2])
})
# Remove RTs < 200ms
sense.pop.sklar <- subset(sense.pop.sklar, rt > 0.2)
# T test (Sklar style)
sense.pop.sklar$log.rt <- log(sense.pop.sklar$rt)
sense.pop.sklar.summary <- summaryBy(rt + log.rt~ SubjNo + Condition, data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")), keep.names = T)
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)
t.test(log.rt ~ Condition, data = sense.pop.sklar.summary, paired = T)
# Bayes factor -- minimum effect of 0.01, maximum of 0.06, our effect = -0.03519684 and our SE = -0.03/-1.7874=  0.01678416
# lmer (Rabag style)
sense.pop.sklar.raw <- summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control"))))
print(sense.pop.sklar.raw)
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.sklar.raw)[,3]))))
sense.pop.sklar.log <- summary(lmer(log.rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control"))))
print(sense.pop.sklar.log)
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.sklar.log)[,3]))))
sense.pop.sklar$Length <- (sense.pop.sklar$Length - mean(sense.pop.sklar$Length))/sd(sense.pop.sklar$Length)
sense.pop.sklar$Cond <- "Violation"
sense.pop.sklar[sense.pop.sklar$Condition == "Sklar_control",]$Cond <- "Control"
sense.pop.sklar$Cond <- as.factor(sense.pop.sklar$Cond)
contrasts(sense.pop.sklar$Cond)[1] <- -1
summary(glmer(rt ~ Cond+Length + (1+Cond|SubjNo)+ (1|prime), data = sense.pop.sklar, family = "inverse.gaussian"(link="log")))
##########################################################################################################################
#
# Let's now analyze for the new trials
sense.pop.new <- subset(sense.pop, Condition %in% c("Sensible", "Non-sensible"))
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < (!!, see by trial exclusion below) 3sd from group mean)
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.new), keep.names = T)
sense.pop.new <- subset(sense.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)
sense.pop.new <- subset(sense.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)
# Remove incorrect trials
sense.pop.new <- subset(sense.pop.new, match. == 1)
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)
sense.pop.new <- ddply(sense.pop.new, .(SubjNo), function(d){
by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt < by_subj_include[2])
})
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)
sense.pop.new <- ddply(sense.pop.new, .(Condition), function(d){
by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt < by_subj_include[2])
})
# Remove RTs < 200ms
sense.pop.new <- subset(sense.pop.new, rt > 0.2)
# T test (Sklar style)
sense.pop.new$log.rt <- log(sense.pop.new$rt)
sense.pop.new.summary <- summaryBy(rt + log.rt~ SubjNo + Condition, data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible")), keep.names = T)
t.test(rt ~ Condition, data = sense.pop.new.summary, paired = T)
t.test(log.rt ~ Condition, data = sense.pop.new.summary, paired = T)
# Bayes factor -- minimum effect of 0.01, maximum of 0.06, our effect = -0.0002327283 and our SE = -0.03/-0.02217=  0.01049744
# lmer (rabag style)
sense.pop.new.raw <-summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible"))))
print(sense.pop.new.raw)
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.new.raw)[,3]))))
sense.pop.new.log <- summary(lmer(log.rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible"))))
print(sense.pop.new.log)
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.new.log)[,3]))))
sense.pop.new$Length <- (sense.pop.new$Length - mean(sense.pop.new$Length))/sd(sense.pop.new$Length)
sense.pop.new$Cond <- "Violation"
sense.pop.new[sense.pop.new$Condition == "Sensible",]$Cond <- "Control"
sense.pop.new$Cond <- as.factor(sense.pop.new$Cond)
contrasts(sense.pop.new$Cond)[1] <- -1
summary(glmer(rt ~ Cond+Length + (1+Cond|SubjNo)+ (1|prime), data = sense.pop.new, family = "inverse.gaussian"(link="log")))
###########################################################################################################################
#
# Finally -- a quick test if longer stims are perceived faster than shorter,
sense.pop.length <- sense.pop
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < (!!, see by trial exclusion below) 3sd from group mean)
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.length), keep.names = T)
sense.pop.length <- subset(sense.pop.length, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)
sense.pop.length <- subset(sense.pop.length, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)
# Remove incorrect trials
sense.pop.length <- subset(sense.pop.length, match. == 1)
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)
sense.pop.length <- ddply(sense.pop.length, .(SubjNo), function(d){
by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)
d = subset(d, rt < by_subj_include[2])
})
# Remove RTs < 200ms
sense.pop.length <- subset(sense.pop.length, rt > 0.2)
# Standardize lenth
sense.pop.length$Length <- (sense.pop.length$Length - mean(sense.pop.length$Length, na.rm = T))/sd(sense.pop.length$Length, na.rm = T)
sense.pop.length.raw <- summary(lmer(rt ~ Length + (1+Length|SubjNo), data = sense.pop.length))
print(sense.pop.length.raw)
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.length.raw)[,3]))))
###########################################################################################################################
#
# Graphs
sense.sklar.graph <- summaryBy(rt ~ Condition + SubjNo, data = sense.pop.sklar, keep.names = T)
sense.sklar.graph <- summaryBy(rt ~ Condition, data = sense.sklar.graph, FUN = c(mean,sd))
sense.sklar.graph$SE <- sense.sklar.graph$rt.sd/sqrt(length(unique(sense.pop.sklar$SubjNo)))
sense.sklar.graph$Experiment <- "Experiment 1a"
sense.new.graph <- summaryBy(rt ~ Condition + SubjNo, data = sense.pop.new, keep.names = T)
sense.new.graph <- summaryBy(rt ~ Condition, data = sense.new.graph, FUN = c(mean,sd))
sense.new.graph$SE <- sense.new.graph$rt.sd/sqrt(length(unique(sense.pop.new$SubjNo)))
sense.new.graph$Experiment <- "Experiment 1b"
sense.graph <- rbind(sense.sklar.graph,sense.new.graph)
sense.graph$Cond_Graph <- "Violation"
sense.graph[sense.graph$Condition %in% c("Sklar_control", "Sensible"),]$Cond_Graph <- "Control"
sense.graph$Cond_Graph <- ordered(sense.graph$Cond_Graph, levels = c("Violation", "Control"))
sense.graph$rt.mean <- sense.graph$rt.mean * 1000
sense.graph$SE <- sense.graph$SE * 1000
dodge <- position_dodge(width=0.9)
ggplot(sense.graph, aes(Experiment,rt.mean, fill = Cond_Graph)) +
geom_bar(stat = "identity",  position = dodge) +
geom_errorbar(aes(ymax = sense.graph$rt.mean +
sense.graph$SE, ymin = sense.graph$rt.mean - sense.graph$SE), width=0.25, position = dodge) +
labs(fill = "Sentence Type") +
theme(axis.text.x = element_text(colour = "black", size = 12)) +
ylab("Reaction Time (ms)") +
xlab("") +
ylim(c(0,2000))
summary(sense.pop.new)
summaryBy(perceptual.rating ~ Condition, data =sense.pop.new)
summaryBy(perceptual.rating ~ Condition, data =sense.pop.sklar)
summary(lm(perceptual.rating ~ Condition, data =sense.pop.sklar))
summary(lmer(perceptual.rating ~ Condition+(1|SubjNo), data =sense.pop.sklar))
summary(lmer(perceptual.rating ~ Condition+(1+Condition|SubjNo), data =sense.pop.sklar))
summary(lmer(perceptual.rating ~ Condition+(1+Condition|SubjNo), data =sense.pop.new))
